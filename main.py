# Instala a biblioteca do Google AI de forma silenciosa


# -*- coding: utf-8 -*-
"""Untitled58.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dxsGlagLWkjT0YmQFxLg94GmLr-2jC5B
"""

# -*- coding: utf-8 -*-
"""
Sistema Cortical com Plasticidade Sin√°ptica Aprimorada

üîπ Novas melhorias no n√≠vel das conex√µes:
1. **Aprendizado Hebbiano com satura√ß√£o**: Pesos n√£o crescem infinitamente
2. **Sinapses vol√°teis**: Conex√µes novas come√ßam fr√°geis e se consolidam com uso
3. **Depress√£o a longo prazo (LTD)**: Conex√µes pouco usadas enfraquecem mais
4. **Metaplasticidade**: Hist√≥rico de ativa√ß√£o influencia taxa de aprendizado
"""

import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
import networkx as nx
from networkx.algorithms import community
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path
from collections import defaultdict, deque
import faiss
from sentence_transformers import SentenceTransformer
import seaborn as sns
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy.spatial.distance import pdist
import warnings
import json
import re
import os
from datetime import datetime
# Integra√ß√£o opcional com Neo4j
try:
    from neo4j import GraphDatabase
except ImportError:
    GraphDatabase = None
    print("AVISO: Pacote 'neo4j' n√£o encontrado. Integra√ß√£o com Neo4j ficar√° desativada.")
# Tente importar a biblioteca do Google, mas n√£o quebre se n√£o estiver instalada ainda
try:
    import google.generativeai as genai
except ImportError:
    print("AVISO: A biblioteca google-generativeai n√£o foi encontrada. O GenerativeCorticalSystem n√£o funcionar√°.")
    print("Por favor, instale com: pip install google-generativeai")
    genai = None

warnings.filterwarnings('ignore')

# Cliente LLM desacoplado (prefer√≠vel ao uso direto do SDK)
try:
    from llm_client import LLMClient, LLMClientError
except Exception:
    LLMClient = None  # type: ignore
    class LLMClientError(Exception):  # type: ignore
        pass
try:
    from synaptic_state_adapter import SynapticStateAdapter
except Exception:
    SynapticStateAdapter = None  # type: ignore


class GenerativeCorticalSystem:
    """
    Orquestra a intera√ß√£o entre a mem√≥ria cortical e um modelo de linguagem generativo (LLM),
    criando um ciclo de aprendizagem simbi√≥tico.
    """
    def __init__(self, cortical_system, llm_client=None, gemini_model_name="gemini-2.5-flash"):
        self.cortical_memory = cortical_system
        self.max_new_knowledge_per_write = 3

        # Preferir cliente desacoplado
        self.llm = None
        if llm_client is not None:
            self.llm = llm_client
        elif 'LLMClient' in globals() and LLMClient is not None:
            try:
                self.llm = LLMClient(model_name=gemini_model_name)
            except Exception as e:
                print("\n" + "="*80)
                print("ERRO: Falha ao inicializar o cliente LLM.")
                print(f"Detalhe do erro: {e}")
                print("="*80)
                self.llm = None

        # Adaptador de estado sin√°ptico (LLM como c√≥rtex)
        if 'SynapticStateAdapter' in globals() and SynapticStateAdapter is not None:
            try:
                self.state_adapter = SynapticStateAdapter(self.cortical_memory)
            except Exception:
                self.state_adapter = None
        else:
            self.state_adapter = None
        # Mem√≥ria de sess√£o leve (perfil curto)
        self.short_term_profile = {}

    def _parse_gemini_response(self, response_text):
        """Extrai a resposta em linguagem natural e o JSON de an√°lise do texto do LLM."""
        try:
            # Tenta encontrar o in√≠cio de um bloco de c√≥digo JSON
            json_match = re.search(r'```json\s*({.*})\s*```', response_text, re.DOTALL)
            if json_match:
                json_part = json_match.group(1).strip()
                # A parte em linguagem natural √© tudo o que vem ANTES do bloco de c√≥digo
                natural_language_part = response_text[:json_match.start()].strip()
                parsed_json = self._try_parse_json_with_sanitization(json_part)
                return natural_language_part, parsed_json

            # Fallback para a l√≥gica original se n√£o encontrar o bloco de c√≥digo formatado
            json_match = re.search(r'{\s*("resumo_essencial"|\'resumo_essencial\')', response_text, re.DOTALL)
            if not json_match:
                return response_text, {}

            json_start_index = json_match.start()
            natural_language_part = response_text[:json_start_index].strip()

            # Encontra o final do JSON contando os colchetes
            brace_count = 0
            json_end_index = -1
            in_string = False

            for i in range(json_start_index, len(response_text)):
                char = response_text[i]

                if char == '"':
                    # Verifica se a aspa n√£o est√° escapada
                    if i > 0 and response_text[i-1] != '\\':
                        in_string = not in_string

                if not in_string:
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1

                if brace_count == 0 and i > json_start_index:
                    json_end_index = i + 1
                    break

            if json_end_index == -1:
                # N√£o encontrou um final correspondente, retorna o texto completo
                return response_text, {}

            json_part = response_text[json_start_index:json_end_index].strip()
            parsed_json = self._try_parse_json_with_sanitization(json_part)
            return natural_language_part, parsed_json

        except (json.JSONDecodeError, AttributeError) as e:
            print(f"AVISO: Falha ao analisar a resposta do Gemini. Retornando resposta completa. Erro: {e}")
            return response_text, {}

    def _try_parse_json_with_sanitization(self, json_text):
        """Tenta parsear JSON aplicando corre√ß√µes comuns: aspas simples e v√≠rgulas de cauda."""
        try:
            return json.loads(json_text)
        except Exception:
            pass
        # Remove v√≠rgulas de cauda antes de } ou ]
        sanitized = re.sub(r",\s*([}\]])", r"\1", json_text)
        # Converte aspas simples para duplas quando h√° chaves/valores simples (heur√≠stica leve)
        # Aten√ß√£o: evitamos tocar em n√∫meros/booleanos
        sanitized = re.sub(r"'", '"', sanitized)
        try:
            return json.loads(sanitized)
        except Exception:
            # √öltimo recurso: retorna dict vazio para n√£o travar write-back
            return {}

    def _update_short_term_profile(self, user_text):
        """Extrai e armazena dados simples de perfil (ex.: nome do usu√°rio)."""
        try:
            m = re.search(r"\bmeu\s+nome\s+e\s*[:=]?\s*([A-Za-z√Ä-√ø'\-\s]+)", user_text, re.IGNORECASE)
            if m:
                name = m.group(1).strip().strip('.!?,;:"')
                # normaliza capitaliza√ß√£o b√°sica
                name_norm = name[0].upper() + name[1:] if name else name
                self.short_term_profile['user_name'] = name_norm
                # escreve conhecimento vol√°til no c√≥rtex
                try:
                    self.cortical_memory.add_new_knowledge(
                        [f"O nome do usu√°rio √© {name_norm}."],
                        metadata_list=[{"volatile": True, "kind": "profile"}],
                        max_to_add=1
                    )
                except Exception:
                    pass
        except Exception:
            pass

    def _write_back_to_memory(self, analysis_json, provenance_meta=None, write_mode="auto"):
        """Traduz a an√°lise metacognitiva do LLM em a√ß√µes REAIS na mem√≥ria cortical."""
        print("\n--- PASSO 4: Escrevendo de volta na Mem√≥ria Cortical (Reflex√£o) ---")

        if not analysis_json:
            print("   >> Nenhuma an√°lise para escrever de volta.")
            return

        # Pol√≠tica de escrita
        if str(write_mode).lower() == "off":
            print("   >> Write-back desativado por pol√≠tica (write_mode=off).")
            return

        # 1. Adicionar novo conhecimento (resumos e infer√™ncias)
        new_knowledge = []
        if analysis_json.get("resumo_essencial"):
            new_knowledge.append(analysis_json["resumo_essencial"])
        if analysis_json.get("novas_inferencias"):
            new_knowledge.extend(analysis_json["novas_inferencias"])
        # Suporte a "ensino" expl√≠cito: lista de fatos/novas mem√≥rias
        # Formatos aceitos:
        # - analysis_json["facts"]: [ {"kind":"fact","content":{"text":"..."},"confidence":0.9}, ... ]
        # - analysis_json["new_memory"]: ["texto 1", "texto 2"]
        taught_texts = []
        if isinstance(analysis_json.get("facts"), list):
            for item in analysis_json["facts"]:
                try:
                    txt = item.get("content", {}).get("text")
                    if isinstance(txt, str) and txt.strip():
                        taught_texts.append(txt.strip())
                except Exception:
                    continue
        if isinstance(analysis_json.get("new_memory"), list):
            for txt in analysis_json["new_memory"]:
                if isinstance(txt, str) and txt.strip():
                    taught_texts.append(txt.strip())
        if taught_texts:
            new_knowledge.extend(taught_texts)

        if new_knowledge:
            # Limitar por ciclo
            capped = new_knowledge[:self.max_new_knowledge_per_write]
            # Constr√≥i metadados por item, incluindo modo "volatile" quando solicitado
            meta_list = []
            for _ in capped:
                md = dict(provenance_meta or {})
                if str(write_mode).lower() == "volatile":
                    md["volatile"] = True
                meta_list.append(md)
            self.cortical_memory.add_new_knowledge(capped, metadata_list=meta_list, max_to_add=self.max_new_knowledge_per_write)

        # 2. Refor√ßar conceitos-chave
        reinforced_concepts = analysis_json.get("conceitos_chave_reforcados", [])
        if reinforced_concepts:
            self.cortical_memory.reinforce_concepts(reinforced_concepts)

        # 3. Modula√ß√£o de consolida√ß√£o/decodifica√ß√£o baseada em confian√ßa/homeostase
        try:
            conf = float(analysis_json.get("confidence", 0.7) or 0.7)
        except Exception:
            conf = 0.7

        # Gate: baixa confian√ßa mant√©m sinapses vol√°teis (quando write-back adicionar novo conhecimento)
        if conf < 0.6:
            write_mode = "volatile"

        # Perfil global de decodifica√ß√£o (temperatura/top_p) suavemente ajustado
        if hasattr(self, "decoding_profile") is False:
            self.decoding_profile = {"temperature": 0.7, "top_p": 0.9}

        # --- Filtro Bayesiano de confian√ßa (Beta conjugada) + curva de aprendizado ---
        if not hasattr(self, "_conf_alpha"):
            self._conf_alpha = 1.0
            self._conf_beta = 1.0
            self._conf_updates = 0
            # hiperpar√¢metros da curva de aprendizado para modula√ß√£o do passo de ajuste
            self._eta0 = 1.0
            self._eta_lambda = 0.05

        # atualiza√ß√£o suave com "pseudo-contagens" proporcionais √† confian√ßa
        self._conf_alpha += float(conf)
        self._conf_beta += float(max(0.0, 1.0 - conf))
        self._conf_updates += 1

        # confiabilidade posterior m√©dia
        posterior_mean = float(self._conf_alpha / max(1e-9, (self._conf_alpha + self._conf_beta)))
        # exp√µe para o adapter ler (meta-acoplamento)
        try:
            self.cortical_memory.posterior_mean = float(posterior_mean)
        except Exception:
            pass
        # taxa de aprendizagem decrescente ao longo do tempo (e reduzida quando confian√ßa m√©dia sobe)
        eta_t = float(self._eta0 / (1.0 + self._eta_lambda * self._conf_updates))
        eta_t *= float(1.0 - 0.5 * max(0.0, posterior_mean - 0.5))

        delta_t = -0.05 if conf < 0.6 else (0.03 if conf > 0.8 else 0.0)
        delta_t *= eta_t
        self.decoding_profile["temperature"] = float(
            min(max(self.decoding_profile["temperature"] + delta_t, 0.3), 0.95)
        )
        # top_p acompanha inversamente de forma leve
        self.decoding_profile["top_p"] = float(
            min(max(0.85 + (0.95 - self.decoding_profile["temperature"]) * 0.1, 0.85), 0.97)
        )

        print("   >> Ciclo de reflex√£o conclu√≠do. A mem√≥ria foi enriquecida e refor√ßada.")
        # Benchmarks: computa ap√≥s cada ciclo de reflex√£o
        try:
            if hasattr(self.cortical_memory, 'compute_and_log_benchmarks'):
                self.cortical_memory.compute_and_log_benchmarks()
        except Exception as _:
            pass


    def process_complex_query(self, user_query, pre_queries=None, write_mode="auto"):
        """
        LLM como C√≥rtex: usa estado sin√°ptico para prefixo + dicas de decodifica√ß√£o,
        chama o LLM diretamente e mant√©m write-back metacognitivo.
        """
        if not getattr(self, 'llm', None) or not self.llm.health_check():
            return "LLM indispon√≠vel. Verifique instala√ß√£o/vari√°veis.", {}

        # Atualiza perfil de sess√£o (ex.: "meu nome √© X")
        self._update_short_term_profile(str(user_query))

        # Pergunta direta sobre nome do usu√°rio
        if re.search(r"\bqual\s+meu\s+nome\b", str(user_query), re.IGNORECASE):
            uname = self.short_term_profile.get('user_name')
            if uname:
                answer = f"Seu nome √© {uname}."
                analysis_json = {
                    "resumo_essencial": f"Usu√°rio chamado {uname} (perfil conhecido).",
                    "novas_inferencias": [],
                    "conceitos_chave_reforcados": ["perfil_do_usuario"],
                    "confidence": 0.95
                }
                provenance = {
                    'source': 'profile',
                    'timestamp': datetime.utcnow().isoformat() + 'Z',
                    'query': user_query
                }
                self._write_back_to_memory(analysis_json, provenance_meta=provenance, write_mode=write_mode)
                return answer, analysis_json
            # caso n√£o haja nome salvo, LLM responde normalmente

        # 1) Estado sin√°ptico ‚Üí prefixo + hints (se adaptador existir)
        if getattr(self, 'state_adapter', None) is not None:
            base_prof = getattr(self, 'decoding_profile', None)
            state = self.state_adapter.to_prompt_state(user_query, base_profile=base_prof)
            prefix = state.get("prefix", "")
            hints = state.get("hints", None)
        else:
            prefix = "<<CORTEX-STATE v1>>\nGUIDE: responda com objetividade e inclua JSON metacognitivo.\n"
            hints = None

        # 2) Prompt unificado: LLM age como c√≥rtex
        prompt = (
            f"{prefix}\n\n"
            "TAREFA: responda de forma clara e objetiva. Em seguida, forne√ßa um bloco JSON curto e V√ÅLIDO (sem v√≠rgulas finais, com aspas duplas) com:\n"
            "```json\n"
            "{\n"
            "  \"resumo_essencial\": \"...\",\n"
            "  \"novas_inferencias\": [\"...\"],\n"
            "  \"conceitos_chave_reforcados\": [\"...\"],\n"
            "  \"confidence\": 0.0\n"
            "}\n"
            "```\n"
            "Responda primeiro em linguagem natural, depois forne√ßa SOMENTE um bloco ```json.\n"
        )

        # 3) Chamada LLM com hints mesclados √† decoding_profile (quando dispon√≠vel)
        try:
            profile = getattr(self, 'decoding_profile', None)
            if hints is not None or profile is not None:
                # valores base
                t_adapter = getattr(hints, 'temperature', None) if hints is not None else None
                p_adapter = getattr(hints, 'top_p', None) if hints is not None else None
                t_prof = float(profile.get('temperature')) if isinstance(profile, dict) and 'temperature' in profile else None
                p_prof = float(profile.get('top_p')) if isinstance(profile, dict) and 'top_p' in profile else None

                # blend ponderado (prioriza leve vi√©s sin√°ptico, preserva estabilidade do perfil)
                def _blend(a, b, wa=0.6, wb=0.4, lo=0.0, hi=1.0):
                    if a is None and b is None:
                        return None
                    if a is None:
                        val = b
                    elif b is None:
                        val = a
                    else:
                        val = wa * float(a) + wb * float(b)
                    if lo is not None and hi is not None:
                        return float(min(max(val, lo), hi))
                    return float(val)

                eff_temp = _blend(t_adapter, t_prof, wa=0.6, wb=0.4, lo=0.3, hi=0.95)
                eff_top_p = _blend(p_adapter, p_prof, wa=0.5, wb=0.5, lo=0.85, hi=0.97)

                full_response = self.llm.generate_text(
                    prompt,
                    temperature=eff_temp,
                    top_p=eff_top_p,
                    presence_penalty=getattr(hints, 'presence_penalty', None) if hints is not None else None,
                    frequency_penalty=getattr(hints, 'frequency_penalty', None) if hints is not None else None,
                )
            else:
                full_response = self.llm.generate_text(prompt)
            natural_language_response, analysis_json = self._parse_gemini_response(full_response)
        except Exception as e:
            print(f"ERRO ao comunicar com a API de LLM: {e}")
            return f"Ocorreu um erro ao tentar gerar a resposta: {e}", {}

        # 4) Write-back: refor√ßo sin√°ptico + homeostase
        provenance = {
            'source': 'LLM',
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'query': user_query
        }
        self._write_back_to_memory(analysis_json, provenance_meta=provenance, write_mode=write_mode)

        return natural_language_response, analysis_json


class EnhancedSynapticCorticalSystem:
    """
    Sistema Cortical com Plasticidade Sin√°ptica Biol√≥gicamente Inspirada

    Novas funcionalidades sin√°pticas:
    - Satura√ß√£o de pesos sin√°pticos (limite biol√≥gico)
    - Sinapses vol√°teis que se consolidam com uso
    - Depress√£o a longo prazo (LTD) para conex√µes fracas
    - Metaplasticidade baseada em hist√≥rico de ativa√ß√£o
    """

    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2',
                 n_subspaces=4, max_hierarchy_levels=3):
        self.rng = np.random.default_rng(12345)
        self.model = SentenceTransformer(model_name)
        self.dim = 384

        # Par√¢metros otimizados
        self.n_subspaces = n_subspaces
        self.max_hierarchy_levels = max_hierarchy_levels

        # Estruturas principais
        self.fragments = {}
        self.items_index = {}
        self.faiss_indices = {}
        self.faiss_id_maps = {}
        self.subspace_projections = {}

        # Grafos hier√°rquicos
        self.fragment_graph = nx.Graph()
        self.concept_graph = nx.Graph()
        self.meta_graph = nx.Graph()
        self.subspace_labels = {}

        # Assinaturas esparsas (Top-K) por subespa√ßo
        self.use_sparse_signatures = True
        self.pattern_k = 24
        self.enable_pattern_sign = True
        # √çndice invertido: subspace -> feature_index -> {fragment_ids}
        self.signature_index = defaultdict(lambda: defaultdict(set))
        # Contadores por padr√£o para plasticidade de padr√£o
        self.pattern_counters = defaultdict(lambda: defaultdict(float))
        self.pattern_counter_decay = 0.95

        # Par√¢metros de plasticidade local
        self.vector_plasticity_rate = 0.08
        self.usage_decay_rate = 0.985
        self.specialization_threshold = 0.5      # Reduzido de 0.7 para acelerar
        self.fusion_threshold = 0.92             # Reduzido de 0.95 para acelerar
        self.context_memory_size = 10

        # NOVOS: Par√¢metros de plasticidade sin√°ptica
        self.max_synaptic_weight = 1.0           # Satura√ß√£o sin√°ptica
        self.initial_volatile_weight = 0.05      # Peso inicial de sinapses vol√°teis
        self.consolidation_threshold = 3         # Uso m√≠nimo para consolida√ß√£o
        self.volatile_decay_rate = 0.7           # Decay de sinapses vol√°teis
        self.ltd_threshold = 0.1                 # Limiar para depress√£o a longo prazo
        self.metaplasticity_window = 5           # Janela para metaplasticidade
        self.node_activities = defaultdict(lambda: deque(maxlen=10))  # Hist√≥rico de ativa√ß√µes por n√≥
        self.edge_usage = defaultdict(int)  # Uso recente das sinapses

        # NOVO: Par√¢metros de Controle de Crescimento e Homeostase
        self.max_synapses_per_fragment = 25      # Limite de conex√µes por fragmento
        self.deconsolidation_inactivity_threshold = 15 # Passos para reverter sinapse consolidada
        self.max_total_synaptic_weight = 3.0     # Homeostase: Soma m√°xima dos pesos sin√°pticos de sa√≠da

        # Par√¢metros Hebbianos ajustados
        self.hebb_eta = 0.15                     # Taxa base aumentada
        self.hebb_lambda = 0.01                  # Aumentado de 0.008 para mais poda
        self.ltd_eta = 0.05                      # Taxa de depress√£o a longo prazo
        self.hebb_eta_contrastive = 0.05         # Taxa para aprendizado contrastivo
        self.hebb_eta_temporal = 0.1             # Taxa para aprendizado temporal

        # NOVO: Hiperpar√¢metros de refinamento da atualiza√ß√£o de pesos
        self.kappa_adaptive_lr = 0.01            # Taxa de decaimento da aprendizagem ao longo do tempo
        self.gamma_error_correction = 0.05       # Ganho do termo de corre√ß√£o de erro
        self.beta_weight_smoothing = 0.02        # Regulariza√ß√£o de varia√ß√£o de peso (suaviza√ß√£o)
        self.default_activation_fn = 'sigmoid'   # Fun√ß√£o para sigma'(a)

        # Outros par√¢metros
        self.novelty_threshold = 0.85
        self.community_threshold = 0.08
        self.meta_activation_threshold = 0.3

        self.next_frag_id = 0
        self.query_history = []
        self.activation_history = []
        self.last_activated_fragments = []

        # Consolidation tracking
        self.interaction_count = 0
        self.consolidation_interval = 5

        # Tracking estat√≠sticas expandido
        self.plasticity_stats = {
            'vectors_adapted': 0,
            'fragments_specialized': 0,
            'fragments_fused': 0,
            'decay_cycles': 0,
            'synapses_created': 0,
            'synapses_consolidated': 0,
            'synapses_pruned': 0,
            'volatile_synapses_decayed': 0,
            'ltd_applications': 0,
            'synapses_deconsolidated': 0
        }

        # NOVO: Par√¢metros para Propaga√ß√£o de Ativa√ß√£o
        self.propagation_steps = 2
        self.propagation_damping_factor = 0.5

        # NOVO: Hist√≥rico de conex√µes para metaplasticidade
        self.connection_history = defaultdict(lambda: deque(maxlen=self.metaplasticity_window))

        # NOVO: Par√¢metros para links por subespa√ßo e sobreposi√ß√£o parcial
        self.enable_subspace_linking = True
        self.subspace_explanations = True
        self.use_partial_weights_in_propagation = True
        self.enable_partial_deduplication = True
        self.partial_overlap_threshold = 0.5  # Limiar mais alto para evitar criar novos fragmentos com sobreposi√ß√£o parcial

        # NOVO: Par√¢metros STDP (regra dependente do tempo)
        self.enable_stdp = True
        self.stdp_A_plus = 0.01
        self.stdp_A_minus = 0.012
        self.stdp_tau_plus = 20.0
        self.stdp_tau_minus = 20.0
        self.stdp_dt_window = 100  # janela m√°xima em unidades de intera√ß√£o

        # NOVO: Par√¢metros BCM (limiar deslizante)
        self.enable_bcm = True
        self.bcm_eta = 0.01
        self.bcm_theta_init = 0.3
        self.bcm_tau_inv = 0.05  # taxa de atualiza√ß√£o do limiar
        self.bcm_theta = defaultdict(lambda: self.bcm_theta_init)

        # NOVO: PID locais por subespa√ßo (meta-PID)
        self.local_pid_Kp = 0.20
        self.local_pid_Ki = 0.02
        self.local_pid_Kd = 0.05
        self.local_pid_target_vol_frac = 0.30
        self.local_pid_target_entropy = 0.60
        self.local_pid_w_vol = 0.6
        self.local_pid_w_ent = 0.4
        self.local_pid_integral = defaultdict(float)
        self.local_pid_prev_error = defaultdict(float)
        # Escala local para teto de peso por n√≥ (atua sobre W_max_total ou normaliza√ß√£o)
        self.local_weight_scale = defaultdict(lambda: 1.0)

        # NOVO: Metaplasticidade de par√¢metros por n√≥
        self.node_eta_scale = defaultdict(lambda: 1.0)
        self.node_stdp_tau_scale = defaultdict(lambda: 1.0)
        self.node_bcm_theta_scale = defaultdict(lambda: 1.0)
        self._last_meta_update = -1

        # NOVO: Par√¢metros de Plasticidade de Curto Prazo (Tsodyks‚ÄìMarkram)
        self.enable_stp = True
        self.stp_U = 0.2
        self.stp_tau_f = 3.0
        self.stp_tau_d = 5.0

        # NOVO: Aprendizado de 3-fatores (tra√ßos de elegibilidade + recompensa)
        self.enable_three_factor = True
        self.eligibility_decay = 0.9
        self.reward_eta = 0.01
        self.max_reward_delta = 0.05
        self.reward_normalizer = 5.0
        self.tag_capture_threshold = 0.02

        # Estado auxiliar para recompensa
        self._last_top_score = 0.0

        # --- Aproxima√ß√µes COLD/WARM/HOT e Orquestra√ß√£o ---
        self.fp_budget_per_interaction = 50000  # or√ßamento de opera√ß√µes FP aproximado
        self.max_hot_nodes = 16
        self.warm_residual_dim = 8  # r componentes para residual WARM
        self.quant_block_size = 16  # blocos para int8
        self.delta_vec_epsilon = 0.015  # ||Œîv|| limiar para re-PQ
        self.alpha_local_bias = 0.25  # vi√©s local para score via Œîv
        self.frontier_k_cold = 6
        self.frontier_k_warm = 12
        self.frontier_k_hot = 24
        self.energy_cutoff = 1e-3  # corte de energia Œª^d aproximado

        # Estados de computa√ß√£o por fragmento: 'COLD'|'WARM'|'HOT'
        self._default_compute_state = 'COLD'

        # Tra√ßos de elegibilidade (int16) e escalas
        self.batch_apply_eligibility = True
        self.eligibility_scale = 1e-3  # escala global
        self.eligibility_i16 = defaultdict(int)  # chave (u,v) -> int acumulado saturado
        self.eligibility_lambda = 0.95  # decaimento no commit

        # Quantiza√ß√£o de pesos por aresta (int8 + escala)
        # Guardado por aresta para reuso em propaga√ß√£o COLD/WARM
        # Campos: edge_data['w_q'] (int), edge_data['w_scale'] (float)

        # Agendamento de centralidade/comunidades
        self.edge_change_threshold = 0.15  # Œî|E| relativo
        self.community_interval = 6       # K intera√ß√µes
        self._last_edge_count = 0
        self._last_community_interaction = 0

        # Limites para atualiza√ß√£o Hebbiana local
        self.hebb_top_m = 24
        self.hebb_top_k = 32

        # Subgrafo sujo (tocado desde o √∫ltimo commit)
        self._dirty_nodes = set()
        self._dirty_edges = set()  # usa tupla (min(u,v), max(u,v))

        # Limiar de sujeira
        self.dirty_edge_tau = 0.02
        self.faiss_reindex_interval = 6

        # --- Fast paths (atalhos feedforward) e coativa√ß√£o ---
        self.fast_paths = defaultdict(set)  # u -> {v}
        self.edge_coact = defaultdict(float)  # (min(u,v), max(u,v)) -> score deca√≠do
        self.fast_coact_threshold = 3.0  # T m√≠nimo de uso nas √∫ltimas janelas (aprox. com decaimento)
        self.fast_weight_threshold = 0.35  # œÑ m√≠nimo de peso m√©dio
        self.fast_decay = 0.7  # decaimento por janela (commit)
        self.fast_demote_threshold = 0.8  # se cair abaixo disso ap√≥s decaimento ‚Üí despromove

        # --- Seeds por erro preditivo (baseline m√≥vel) ---
        self.baseline_act = defaultdict(float)
        self.baseline_beta = 0.1

        # --- Benchmarks: reten√ß√£o, transfer√™ncia, curr√≠culo ---
        self.bench = {
            'retention_scores': [],       # [(t, score)]
            'transfer_scores': [],        # [(t, score)]
            'curriculum_depth': [],       # [(t, depth)]
            'trajectory_entropy': [],     # [(t, H)]
            'coactivation_entropy': [],   # [(t, H)]
        }

        # --- Event-driven e WTA local ---
        self.delta_a_epsilon = 0.05
        self.hebb_prepost_epsilon = 0.05
        self.wta_nu = 0.05

        # --- Gating por fase ---
        self.phase_k = 5  # janelas
        self.phase = 0
        self.stdp_tau_fast = 8.0

        # --- Par√¢metros de higiene sugeridos ---
        self.novelty_min_for_dynamic = 0.60
        self.min_content_tokens = 3
        self.max_dynamic_per_interaction_lowinfo = 1
        self.ctx_score_gap = 0.50  # œÑ_gap
        self.consolidation_threshold_lowinfo = 4
        self.volatile_decay_rate_lowinfo = 0.8

        # Estado por-intera√ß√£o
        self._current_lowinfo = False
        self._current_query_tokens = 0
        self._last_interaction_was_lowinfo = False
        # Runtime somente Neo4j (sem depender do NetworkX como fonte de dados)
        self.use_neo4j_runtime = os.getenv("NEO4J_RUNTIME_ONLY", "off").lower() in ("on", "true", "1")

    # ==========================
    # Quantiza√ß√£o e Aproxima√ß√µes
    # ==========================
    def _blockwise_quantize(self, vector):
        """Quantiza vetor FP32 em blocos de tamanho quant_block_size para int8 + escalas por bloco."""
        bs = self.quant_block_size
        length = vector.shape[0]
        num_blocks = (length + bs - 1) // bs
        q_int8 = np.zeros(length, dtype=np.int8)
        scales = np.zeros(num_blocks, dtype=np.float32)
        for b in range(num_blocks):
            start = b * bs
            end = min(start + bs, length)
            block = vector[start:end]
            max_abs = float(np.max(np.abs(block))) if end > start else 0.0
            scale = max(max_abs / 127.0, 1e-8)
            scales[b] = scale
            if max_abs > 0:
                q_int8[start:end] = np.clip(np.round(block / scale), -127, 127).astype(np.int8)
            else:
                q_int8[start:end] = 0
        return q_int8, scales

    def _ensure_fragment_quant(self, fid):
        """Garante que o fragmento tenha representa√ß√£o quantizada e residual se necess√°rio."""
        frag = self.fragments[fid]
        # No modo de assinaturas esparsas, n√£o quantizamos o vetor do fragmento.
        # Mantemos placeholders para compatibilidade com caminhos COLD/WARM/HOT.
        vec = np.zeros(self.dim, dtype=np.float32)
        last_q = frag.get("last_quant_fp32")
        need_reencode = True
        if last_q is not None:
            delta = np.linalg.norm(vec - last_q)
            if delta < self.delta_vec_epsilon:
                need_reencode = False
                frag["last_delta"] = vec - last_q
        if need_reencode:
            q_int8, q_scales = self._blockwise_quantize(vec)
            # Residual para WARM: top-r componentes por magnitude do erro
            dequant = self._dequantize_vector(q_int8, q_scales)
            residual = vec - dequant
            if residual.shape[0] > 0:
                idx_sorted = np.argsort(-np.abs(residual))[:self.warm_residual_dim]
                frag["residual_idx"] = idx_sorted.astype(np.int32)
                frag["residual_vals"] = residual[idx_sorted].astype(np.float32)
            else:
                frag["residual_idx"] = np.array([], dtype=np.int32)
                frag["residual_vals"] = np.array([], dtype=np.float32)
            frag["q_int8"] = q_int8
            frag["q_scales"] = q_scales
            frag["last_quant_fp32"] = vec.copy()
            frag["last_delta"] = np.zeros_like(vec)

    def _dequantize_vector(self, q_int8, q_scales):
        """Dequantiza int8 + escalas por bloco para FP32 (apenas para c√°lculos auxiliares)."""
        bs = self.quant_block_size
        length = q_int8.shape[0]
        num_blocks = (length + bs - 1) // bs
        out = np.zeros(length, dtype=np.float32)
        for b in range(num_blocks):
            start = b * bs
            end = min(start + bs, length)
            out[start:end] = q_int8[start:end].astype(np.float32) * q_scales[b]
        return out

    def _approximate_dot(self, fid, query_proj, mode):
        """Computa dot aproximado de acordo com COLD/WARM/HOT, com vi√©s local por Œîv."""
        frag = self.fragments[fid]
        if mode == 'HOT':
            # Baseia-se em overlap de padr√µes (assinaturas esparsas)
            patt = frag.get('pattern', set())
            q_pattern, _ = self._vector_to_pattern(query_proj, k=self.pattern_k, with_sign=self.enable_pattern_sign)
            base = float(self._pattern_overlap_score(q_pattern, patt))
        else:
            self._ensure_fragment_quant(fid)
            q_int8 = frag.get("q_int8")
            q_scales = frag.get("q_scales")
            # COLD: dot aproximado por bloco
            bs = self.quant_block_size
            length = q_int8.shape[0]
            num_blocks = (length + bs - 1) // bs
            approx = 0.0
            for b in range(num_blocks):
                start = b * bs
                end = min(start + bs, length)
                # (q_int8 * scale) ¬∑ query
                approx += float(np.dot(q_int8[start:end].astype(np.float32), query_proj[start:end])) * float(q_scales[b])
            base = approx
            if mode == 'WARM':
                # Corre√ß√£o residual
                idx = frag.get("residual_idx")
                vals = frag.get("residual_vals")
                if idx is not None and vals is not None and len(idx) > 0:
                    base += float(np.dot(vals, query_proj[idx]))
        # Vi√©s local se sem re-encode
        delta = frag.get("last_delta")
        if delta is not None and np.any(delta):
            base += float(self.alpha_local_bias * np.dot(delta, query_proj))
        return base

    def _orchestrate_compute_states(self, fragment_acts):
        """Seleciona conjuntos HOT/WARM/COLD com or√ßamento de FP, usando activity, centralidade e novidade."""
        if not fragment_acts:
            return set(), set(), set()
        scored = []
        for fid, act in fragment_acts.items():
            frag = self.fragments.get(fid, {})
            central = float(frag.get('centrality', 0.0))
            novelty = float(frag.get('novelty_score', 0.0)) if 'novelty_score' in frag else 0.0
            score = 1.0 * act + 0.5 * central + 0.3 * novelty
            scored.append((fid, score))
        scored.sort(key=lambda x: -x[1])
        hot = set([fid for fid, _ in scored[:self.max_hot_nodes]])
        # Estima FP custo: cada HOT ~ 2*dim, WARM ~ dim, COLD ~ ~0
        fp_budget = self.fp_budget_per_interaction
        fp_cost = len(hot) * 2 * self.dim
        warm = set()
        idx = self.max_hot_nodes
        while idx < len(scored) and fp_cost + self.dim <= fp_budget:
            warm.add(scored[idx][0])
            fp_cost += self.dim
            idx += 1
        cold = set(fid for fid, _ in scored[idx:])
        # Grava estado no fragmento
        for fid in hot:
            if fid in self.fragments:
                self.fragments[fid]['compute_state'] = 'HOT'
        for fid in warm:
            if fid in self.fragments:
                self.fragments[fid]['compute_state'] = 'WARM'
        for fid in cold:
            if fid in self.fragments:
                self.fragments[fid]['compute_state'] = 'COLD'
        return hot, warm, cold

    # ==============================
    # Tra√ßo de Elegibilidade (int16)
    # ==============================
    def _stdp_kernel(self, dt):
        """Kernel STDP simples: triangular/exponencial discretizado."""
        if dt == 0:
            return 0.0
        tau = 20.0
        if dt > 0:
            return np.exp(-dt / tau)
        else:
            return -np.exp(dt / tau)

    def _update_edge_eligibility(self, i, j, a_i, a_j, dt):
        """Atualiza tra√ßo e_ij em int16 com escala global e decaimento lazy."""
        f = (a_i * a_j) * self._stdp_kernel(dt)
        inc = int(np.clip(np.round(f / max(self.eligibility_scale, 1e-9)), -32767, 32767))
        key = (int(i), int(j))
        curr = self.eligibility_i16[key]
        new_val = int(np.clip(curr + inc, -32767, 32767))
        self.eligibility_i16[key] = new_val

    def _commit_eligibilities(self, reward=0.0):
        """Aplica em lote w_ij += eta * (e_ij * recompensa) e decai o tra√ßo."""
        if not self.eligibility_i16:
            return 0
        applied = 0
        eta = self.reward_eta
        for (i, j), e16 in list(self.eligibility_i16.items()):
            if i in self.fragment_graph and j in self.fragment_graph[i]:
                e_float = float(e16) * self.eligibility_scale
                delta = eta * e_float * float(reward)
                w_prev = float(self.fragment_graph[i][j].get('w', 0.0))
                self.fragment_graph[i][j]['w'] = float(np.clip(w_prev + delta, 0.0, self.max_synaptic_weight))
                applied += 1
                # decaimento do tra√ßo
                self.eligibility_i16[(i, j)] = int(self.eligibility_lambda * e16)
                # Atualiza quantiza√ß√£o da aresta on-demand
                self._quantize_edge_weight(i, j)
                # coativa√ß√£o para fast path (acumula e decai no commit)
                key = (min(i, j), max(i, j))
                self.edge_coact[key] = self.edge_coact.get(key, 0.0) + abs(e_float)
            else:
                # remove tra√ßo √≥rf√£o
                self.eligibility_i16.pop((i, j), None)
        return applied

    # =======================
    # Pesos quantizados edge
    # =======================
    def _quantize_edge_weight(self, u, v):
        """Mant√©m peso quantizado int8 + escala na aresta para uso COLD/WARM."""
        if u not in self.fragment_graph or v not in self.fragment_graph[u]:
            return
        d = self.fragment_graph[u][v]
        w_prev = float(d.get('w', 0.0))
        old_eff = float(d.get('w_q', 0) * d.get('w_scale', 0.0)) if ('w_q' in d and 'w_scale' in d) else w_prev
        scale = max(abs(w_prev) / 127.0, 1e-8)
        d['w_scale'] = scale
        d['w_q'] = int(np.clip(np.round(w_prev / scale), -127, 127))
        new_eff = float(d['w_q'] * d['w_scale'])
        if abs(new_eff - old_eff) > self.dirty_edge_tau:
            self._dirty_edges.add((min(u, v), max(u, v)))
            self._dirty_nodes.add(u); self._dirty_nodes.add(v)

    def _edge_weight_effective(self, u, v, hot=False):
        """Retorna peso efetivo da aresta considerando modo de computa√ß√£o.
        Se NEO4J_RUNTIME_ONLY=on, busca diretamente no Neo4j.
        """
        if self.use_neo4j_runtime and GraphDatabase is not None:
            try:
                uri = os.getenv("NEO4J_URI"); user = os.getenv("NEO4J_USER"); password = os.getenv("NEO4J_PASSWORD")
                if not (uri and user and password):
                    return 0.0
                driver = GraphDatabase.driver(uri, auth=(user, password))
                with driver.session() as session:
                    rec = session.run(
                        """
                        MATCH (a:Fragment {id:$u})-[r:SYNAPSE]->(b:Fragment {id:$v})
                        RETURN coalesce(r.w,0.0) AS w
                        """,
                        u=int(u), v=int(v)
                    ).single()
                    w = float(rec["w"]) if rec else 0.0
                    return w
            except Exception:
                return 0.0
            finally:
                try:
                    driver.close()
                except Exception:
                    pass
        # Caminho padr√£o com grafo em mem√≥ria
        d = self.fragment_graph[u][v]
        if hot:
            return float(d.get('w', 0.0))
        if 'w_q' not in d or 'w_scale' not in d:
            self._quantize_edge_weight(u, v)
        return float(d.get('w_q', 0) * d.get('w_scale', 0.0))

    def initialize_semantic_subspaces(self, embedding_matrix, labels=None):
        """Initialize subspace projections with better component distribution"""
        print("--- Inicializando subespa√ßos sem√¢nticos ---")

        comps_per_subspace = 10
        total_comps = min(self.n_subspaces * comps_per_subspace,
                          embedding_matrix.shape[0]-1, self.dim)

        pca = PCA(n_components=total_comps, random_state=0)
        pca.fit(embedding_matrix)
        components = pca.components_.T

        per_sub = total_comps // self.n_subspaces
        extras = total_comps % self.n_subspaces
        idx = 0

        for s in range(self.n_subspaces):
            take = per_sub + (1 if s < extras else 0)
            self.subspace_projections[s] = components[:, idx:idx+take]
            label = labels[s] if labels and s < len(labels) else f"semantico_{s}"
            self.subspace_labels[s] = label
            print(f"Subespa√ßo {s} ({label}): {take} componentes (vari√¢ncia explicada: {pca.explained_variance_ratio_[idx:idx+take].sum():.3f})")
            idx += take

    def project_to_subspace(self, vector, subspace_id):
        """Project with better normalization"""
        P = self.subspace_projections[subspace_id]
        z = vector @ P
        norm = np.linalg.norm(z)
        return z / norm if norm > 1e-9 else z

    # ==========================
    # Assinaturas Esparsas (Top-K)
    # ==========================
    def _vector_to_pattern(self, projected_vec, k=None, with_sign=True):
        """Converte vetor projetado em assinatura esparsa Top-K por |valor|.

        Retorna (pattern_set, sign_dict). sign_dict pode ser vazio se with_sign=False.
        """
        try:
            arr = np.asarray(projected_vec).reshape(-1)
        except Exception:
            arr = projected_vec
        if arr is None or arr.shape[0] == 0:
            return set(), {}
        k_eff = int(min(self.pattern_k if k is None else k, arr.shape[0]))
        # √≠ndices ordenados por magnitude
        idx = np.argsort(-np.abs(arr))[:k_eff]
        pattern = set(int(i) for i in idx)
        if with_sign:
            sign = {int(i): (1 if arr[int(i)] >= 0 else -1) for i in idx}
        else:
            sign = {}
        return pattern, sign

    def _register_fragment_pattern(self, fid):
        """Registra o padr√£o do fragmento no √≠ndice invertido."""
        frag = self.fragments.get(fid)
        if not frag:
            return
        s = frag.get('subspace', 0)
        patt = frag.get('pattern', set())
        for feat in list(patt):
            self.signature_index[s][int(feat)].add(int(fid))

    def _unregister_fragment_pattern(self, fid, old_pattern=None):
        """Remove o padr√£o anterior do √≠ndice invertido (se existir)."""
        frag = self.fragments.get(fid)
        if not frag and old_pattern is None:
            return
        s = (frag.get('subspace', 0) if frag else 0)
        patt = (old_pattern if old_pattern is not None else frag.get('pattern', set()))
        for feat in list(patt):
            try:
                self.signature_index[s][int(feat)].discard(int(fid))
            except Exception:
                pass

    def _pattern_overlap_score(self, A, B, method='jaccard'):
        """Calcula overlap entre dois padr√µes (sets de √≠ndices)."""
        if not A or not B:
            return 0.0
        inter = len(A & B)
        if method == 'jaccard':
            uni = len(A | B)
            return float(inter / max(1, uni))
        # padr√£o: fra√ß√£o de interse√ß√£o sobre K m√©dio
        denom = float((len(A) + len(B)) / 2.0)
        return float(inter / max(1.0, denom))

    def _pattern_overlap_similarity(self, fi, fj, method='jaccard'):
        """Similaridade entre fragmentos por overlap de padr√µes."""
        A = self.fragments.get(fi, {}).get('pattern', set())
        B = self.fragments.get(fj, {}).get('pattern', set())
        return self._pattern_overlap_score(A, B, method=method)

    def extract_initial_fragments_hierarchical(self, items):
        """Extract fragments using hierarchical clustering for better organization"""
        print("--- Extraindo fragmentos com clustering hier√°rquico ---")

        for s in range(self.n_subspaces):
            projected = np.array([
                self.project_to_subspace(item["vec"], s)
                for item in items
            ]).astype('float32')

            if len(projected) < 4:
                n_samples = len(projected)
                if n_samples == 0:
                    continue
                n_clusters = 1 if n_samples == 1 else min(2, n_samples)
                if n_clusters == 1:
                    clusters = np.ones(n_samples, dtype=int)
                else:
                    mid = n_samples // 2
                    clusters = np.array([1] * mid + [2] * (n_samples - mid), dtype=int)
            else:
                distances = pdist(projected)
                linkage_matrix = linkage(distances, method='ward')
                n_clusters = min(8, max(3, len(projected) // 6))
                clusters = fcluster(linkage_matrix, n_clusters, criterion='maxclust')

            for cluster_id in range(1, n_clusters + 1):
                cluster_mask = clusters == cluster_id
                cluster_vectors = projected[cluster_mask]

                if len(cluster_vectors) == 0:
                    continue

                centroid = np.mean(cluster_vectors, axis=0)
                centroid = centroid / (np.linalg.norm(centroid) + 1e-9)
                # Converte centroid em assinatura esparsa
                pattern, sign = (set(), {})
                if self.use_sparse_signatures:
                    pattern, sign = self._vector_to_pattern(centroid, k=self.pattern_k, with_sign=self.enable_pattern_sign)

                fid = self.next_frag_id
                self.fragments[fid] = {
                    "subspace": s,
                    # representa√ß√£o esparsa
                    "pattern": pattern,
                    "sign": sign,
                    "usage": 0.0,
                    "level": 0,
                    "cluster_size": len(cluster_vectors),
                    "label": f"frag_s{s}_{fid}_c{cluster_id}",

                    # Plasticidade vetorial
                    "activation_contexts": deque(maxlen=self.context_memory_size),
                    "last_activation": 0,
                    "adaptation_count": 0,
                    # mant√©m um vetor refer√™ncia m√≠nimo apenas se necess√°rio (desativado por padr√£o)
                    "original_vector": centroid.copy(),
                    "specialization_score": 0.0,

                    # NOVO: Metaplasticidade
                    "activation_frequency": 0.0,
                    "recent_activations": deque(maxlen=self.metaplasticity_window),
                    "plasticity_state": "normal"  # normal, potentiated, depressed
                }

                self.fragment_graph.add_node(fid, **self.fragments[fid])
                self.next_frag_id += 1
                # registra padr√£o no √≠ndice invertido
                if self.use_sparse_signatures:
                    self._register_fragment_pattern(fid)
                # Neo4j runtime: upsert n√≥
                try:
                    self._neo4j_upsert_fragment(fid)
                except Exception:
                    pass

            print(f"Subespa√ßo {s}: {n_clusters} fragmentos hier√°rquicos criados")

    def build_faiss_indices(self):
        """Constr√≥i √≠ndice invertido de assinaturas (substitui FAISS)."""
        print("--- Construindo √≠ndice invertido de assinaturas ---")
        self.signature_index = defaultdict(lambda: defaultdict(set))
        for fid, f in self.fragments.items():
            patt = f.get('pattern', set())
            s = f.get('subspace', 0)
            for feat in list(patt):
                self.signature_index[s][int(feat)].add(int(fid))
        print("√çndice invertido constru√≠do para", len(self.fragments), "fragmentos")

    # NOVO: Fun√ß√£o para calcular metaplasticidade
    def _calculate_metaplasticity_factor(self, fid):
        """
        Calcula fator de metaplasticidade baseado no hist√≥rico de ativa√ß√µes

        Returns:
            float: Fator multiplicativo para taxa de aprendizado (0.5 - 2.0)
        """
        fragment = self.fragments[fid]
        recent_acts = list(fragment["recent_activations"])

        if len(recent_acts) < 2:
            return 1.0  # Fator neutro para fragmentos novos

        # Calcular frequ√™ncia recente de ativa√ß√£o
        recent_freq = np.mean(recent_acts)

        # Calcular variabilidade das ativa√ß√µes
        act_variance = np.var(recent_acts) if len(recent_acts) > 1 else 0.0

        # Estados metapl√°sticos (recalibrados para melhor equil√≠brio)
        if recent_freq > 0.7 and act_variance < 0.1: # Mais rigoroso para deprimir
            # Muita ativa√ß√£o consistente ‚Üí Depress√£o metapl√°stica
            fragment["plasticity_state"] = "depressed"
            return 0.6  # Reduz plasticidade
        elif (0.3 < recent_freq < 0.65) and act_variance > 0.15: # Mais permissivo para potenciar
            # Ativa√ß√£o moderada e vari√°vel ‚Üí Potencia√ß√£o metapl√°stica
            fragment["plasticity_state"] = "potentiated"
            return 1.5  # Aumenta plasticidade
        else:
            fragment["plasticity_state"] = "normal"
            return 1.0

    def _calculate_semantic_component(self, fi, fj, ai, aj, adaptive_eta, plasticity_boost):
        """Componente sem√¢ntico via overlap de padr√µes (assinaturas esparsas)."""
        semantic_similarity = self._pattern_overlap_similarity(fi, fj, method='jaccard')
        return adaptive_eta * ai * aj * float(semantic_similarity) * plasticity_boost

    def _calculate_contrastive_component(self, fi, fj, ai, aj):
        """Componente contrastivo com base em 1 - overlap."""
        semantic_similarity = self._pattern_overlap_similarity(fi, fj, method='jaccard')
        return self.hebb_eta_contrastive * ai * aj * float(1.0 - semantic_similarity)

    def _calculate_temporal_component(self, fi, fj, ai, aj):
        """Calcula o componente temporal para criar sequ√™ncias."""
        dw_temporal = 0
        fi_was_recent = fi in self.last_activated_fragments
        fj_was_recent = fj in self.last_activated_fragments

        # Refor√ßa a conex√£o A -> B se A foi ativado no passo anterior e B √© novo
        if fi_was_recent and not fj_was_recent:
            dw_temporal = self.hebb_eta_temporal * ai * aj
        elif fj_was_recent and not fi_was_recent:
            dw_temporal = self.hebb_eta_temporal * ai * aj
        return dw_temporal

    def _calculate_hierarchical_feedback_component(self, fi, fj):
        """
        Calcula um b√¥nus de aprendizado se os conceitos-pai dos fragmentos
        estiverem conectados na rede de conceitos (feedback top-down).
        """
        concept_i = self.fragment_graph.nodes[fi].get('concept_id')
        concept_j = self.fragment_graph.nodes[fj].get('concept_id')

        # Se ambos os fragmentos pertencem a conceitos e os conceitos s√£o diferentes
        if concept_i and concept_j and concept_i != concept_j:
            # Verificar se h√° uma conex√£o entre esses conceitos no grafo de alto n√≠vel
            if self.concept_graph.has_edge(concept_i, concept_j):
                # O b√¥nus √© proporcional √† for√ßa da conex√£o conceitual
                conceptual_link_strength = self.concept_graph[concept_i][concept_j].get('w', 0.0)
                # Retorna um b√¥nus multiplicativo
                return 1.0 + (conceptual_link_strength * 1.5) # O fator 1.5 amplifica o efeito

        # Se n√£o houver conex√£o conceitual, n√£o h√° b√¥nus
        return 1.0

    def _calculate_global_attention_component(self, fi, fj):
        """Calcula o fator de aten√ß√£o global baseado na centralidade dos fragmentos."""
        centrality_i = self.fragments[fi].get('centrality', 0.1)
        centrality_j = self.fragments[fj].get('centrality', 0.1)
        # O fator de refor√ßo √© maior se ambos os fragmentos s√£o centrais
        return 1.0 + (centrality_i * centrality_j * 2.0)

    # NOVO: Aprendizado Hebbiano com satura√ß√£o e metaplasticidade
    def _enhanced_hebbian_update_with_saturation(self, activated, query_vec):
        """
        Aprendizado Hebbiano aprimorado com:
        - Satura√ß√£o sin√°ptica biol√≥gica
        - Sinapses vol√°teis que se consolidam
        - Depress√£o a longo prazo (LTD)
        - Metaplasticidade
        - Aprendizado Contraste-Sem√¢ntico
        - Mem√≥ria Relacional Temporal
        """
        # Limitar aos top_m por ativa√ß√£o para reduzir custo
        fids = [fid for fid, _ in sorted(activated.items(), key=lambda x: -x[1])[:self.hebb_top_m]]

        # Atualizar hist√≥rico de ativa√ß√µes para metaplasticidade
        for fid in fids:
            activation = activated[fid]
            self.fragments[fid]["recent_activations"].append(activation)
            self.fragments[fid]["activation_frequency"] = np.mean(
                list(self.fragments[fid]["recent_activations"])
            )

        # Potencia√ß√£o Hebbian para pares co-ativados
        synapses_updated = 0
        new_synapses = 0

        for i in range(len(fids)):
            fi = fids[i]
            # Seleciona top_k co-ativados para fi
            co_list = [(fids[j], activated[fids[j]]) for j in range(i + 1, len(fids))]
            co_list.sort(key=lambda x: -x[1])
            allowed_fj = set(fid for fid, _ in co_list[:self.hebb_top_k])
            for j in range(i + 1, len(fids)):
                fj = fids[j]
                if fj not in allowed_fj:
                    continue
                ai, aj = activated[fi], activated[fj]
                # Event-driven Hebb: s√≥ considera se pre*post > epsilon
                if ai * aj < self.hebb_prepost_epsilon:
                    continue

                # L√≥gica para sinapses cross-subspace
                subspace_i = self.fragments[fi]["subspace"]
                subspace_j = self.fragments[fj]["subspace"]

                if subspace_i != subspace_j:
                    # Similaridade cross-subspace aproximada: overlap do padr√£o do query em cada subespa√ßo
                    qi_pattern, _ = self._vector_to_pattern(self.project_to_subspace(query_vec, subspace_i), k=self.pattern_k, with_sign=self.enable_pattern_sign)
                    qj_pattern, _ = self._vector_to_pattern(self.project_to_subspace(query_vec, subspace_j), k=self.pattern_k, with_sign=self.enable_pattern_sign)
                    patt_i = self.fragments[fi].get('pattern', set())
                    patt_j = self.fragments[fj].get('pattern', set())
                    sim_i = self._pattern_overlap_score(qi_pattern, patt_i)
                    sim_j = self._pattern_overlap_score(qj_pattern, patt_j)
                    cross_subspace_similarity = float(sim_i * sim_j)

                    if cross_subspace_similarity > 0.3: # Threshold para criar conex√£o cross-subspace
                        dw_cross = self.hebb_eta * ai * aj * 0.2 # Peso reduzido para cross-subspace

                        if self.fragment_graph.has_edge(fi, fj):
                            edge_data = self.fragment_graph[fi][fj]
                            if "weights" not in edge_data:
                                edge_data["weights"] = defaultdict(float)

                            # Adiciona um peso simb√≥lico para a conex√£o cross-subspace
                            edge_data["weights"][-1] = max(edge_data["weights"][-1], dw_cross) # -1 indica cross-subspace
                        else:
                            initial_weights = defaultdict(float)
                            initial_weights[-1] = dw_cross
                            self.fragment_graph.add_edge(fi, fj,
                                                       w=dw_cross,
                                                       weights=initial_weights,
                                                       usage_count=1,
                                                       volatile=True,
                                                       created_at=self.interaction_count,
                                                       last_update=self.interaction_count)
                        # Explicabilidade para cross-subspace
                        activation_path = getattr(self, '_current_activation_path', None)
                        if activation_path is not None:
                            activation_path[fi]['sources'].append({'cross_subspace': True, 'similarity': float(cross_subspace_similarity)})
                            activation_path[fj]['sources'].append({'cross_subspace': True, 'similarity': float(cross_subspace_similarity)})
                        continue # Pula a l√≥gica de atualiza√ß√£o intra-subspace

                # Calcular fatores de metaplasticidade
                meta_factor_i = self._calculate_metaplasticity_factor(fi)
                meta_factor_j = self._calculate_metaplasticity_factor(fj)
                meta_factor = (meta_factor_i + meta_factor_j) / 2.0

                # Taxa de aprendizado adaptativa
                if ai > 0.7 and aj > 0.7:
                    base_eta = self.hebb_eta * 1.5
                else:
                    base_eta = self.hebb_eta

                # Aplicar metaplasticidade
                adaptive_eta = base_eta * meta_factor

                # Boost para fragmentos com alta plasticidade vetorial
                plasticity_boost = 1.0
                if (self.fragments[fi].get("adaptation_count", 0) > 0 or
                    self.fragments[fj].get("adaptation_count", 0) > 0):
                    plasticity_boost = 1.2

                # 1. Calcular componentes via padr√µes esparsos (sem vetores densos)
                dw_semantic = self._calculate_semantic_component(fi, fj, ai, aj, adaptive_eta, plasticity_boost)
                dw_contrastive = self._calculate_contrastive_component(fi, fj, ai, aj)
                dw_temporal = self._calculate_temporal_component(fi, fj, ai, aj)
                global_attention_factor = self._calculate_global_attention_component(fi, fj)
                hierarchical_feedback = self._calculate_hierarchical_feedback_component(fi, fj)

                # 2. Combinar os componentes na f√≥rmula h√≠brida final, aplicando todos os fatores
                base_dw = dw_semantic + dw_contrastive + dw_temporal
                dw = base_dw * global_attention_factor * hierarchical_feedback

                edge_key = (fi, fj)

                # Atualizar ou criar conex√£o
                if self.fragment_graph.has_edge(fi, fj):
                    # Conex√£o existente
                    edge_data = self.fragment_graph[fi][fj]
                    # MODIFICADO: L√≥gica para sinapses parciais
                    if "weights" not in edge_data:
                        edge_data["weights"] = defaultdict(float)

                    # Apenas atualiza o peso para o subespa√ßo compartilhado, se houver
                    subspace_i = self.fragments[fi]["subspace"]
                    subspace_j = self.fragments[fj]["subspace"]

                    # Atualiza√ß√£o de sinapse intra-subespa√ßo
                    if subspace_i == subspace_j:
                        current_w = edge_data["weights"][subspace_i]
                        new_w = current_w + dw - self.hebb_lambda * current_w
                        edge_data["weights"][subspace_i] = min(self.max_synaptic_weight, max(0.0, new_w))

                    # Atualiza o peso total como a m√©dia ou soma dos pesos parciais
                    total_weight = sum(edge_data["weights"].values())

                    usage_count = edge_data.get("usage_count", 0) + 1
                    is_volatile = edge_data.get("volatile", False)

                    threshold_use = self.consolidation_threshold
                    if self._current_lowinfo:
                        threshold_use = max(threshold_use, self.consolidation_threshold_lowinfo)
                    if is_volatile and usage_count >= threshold_use:
                        is_volatile = False
                        self.plasticity_stats['synapses_consolidated'] += 1
                        print(f"       >> Sinapse {fi}-{fj} consolidada (uso: {usage_count})")

                    # Atualiza atributos b√°sicos
                    self.fragment_graph[fi][fj].update({
                        "w": total_weight, # Peso agregado para visualiza√ß√£o/an√°lise
                        "weights": edge_data["weights"],
                        "usage_count": usage_count,
                        "volatile": is_volatile,
                        "last_update": self.interaction_count
                    })
                    # Atualiza peso quantizado para modos COLD/WARM
                    self._quantize_edge_weight(fi, fj)
                    # Marca aresta e n√≥s como sujos
                    self._dirty_edges.add((min(fi, fj), max(fi, fj)))
                    self._dirty_nodes.add(fi); self._dirty_nodes.add(fj)
                    # Neo4j runtime: upsert aresta
                    try:
                        self._neo4j_upsert_edge(fi, fj)
                    except Exception:
                        pass

                    # 3-fatores: atualiza tra√ßo de elegibilidade (decai e acumula com coativa√ß√£o)
                    if self.enable_three_factor:
                        e_prev = edge_data.get('eligibility', 0.0) * self.eligibility_decay
                        edge_data['eligibility'] = e_prev + (ai * aj)
                        # tagging se forte LTP
                        if dw > self.tag_capture_threshold:
                            edge_data['tagged'] = True

                    synapses_updated += 1

                    # Registrar para metaplasticidade
                    self.connection_history[edge_key].append(dw)

                else:
                    # Nova conex√£o
                    if dw > 0.02:  # Threshold para criar nova sinapse
                        # MODIFICADO: L√≥gica para sinapses parciais
                        subspace_i = self.fragments[fi]["subspace"]
                        subspace_j = self.fragments[fj]["subspace"]

                        initial_weights = defaultdict(float)
                        if subspace_i == subspace_j:
                            initial_weights[subspace_i] = max(self.initial_volatile_weight, dw * 0.5)

                        total_weight = sum(initial_weights.values())

                        self.fragment_graph.add_edge(fi, fj,
                                                   w=total_weight,
                                                   weights=initial_weights,
                                                   usage_count=1,
                                                   volatile=True,
                                                   created_at=self.interaction_count,
                                                   last_update=self.interaction_count)
                        # Inicializa quantiza√ß√£o do peso
                        self._quantize_edge_weight(fi, fj)

                        # 3-fatores: inicializa elegibilidade
                        if self.enable_three_factor:
                            self.fragment_graph[fi][fj]['eligibility'] = ai * aj
                            self.fragment_graph[fi][fj]['tagged'] = (dw > self.tag_capture_threshold)

                        new_synapses += 1
                        self.plasticity_stats['synapses_created'] += 1

                        # Registrar para metaplasticidade
                        self.connection_history[edge_key].append(dw)
                        # Marca aresta e n√≥s como sujos
                        self._dirty_edges.add((min(fi, fj), max(fi, fj)))
                        self._dirty_nodes.add(fi); self._dirty_nodes.add(fj)
                        # Neo4j runtime: upsert aresta
                        try:
                            self._neo4j_upsert_edge(fi, fj)
                        except Exception:
                            pass

        
        # NOVO: Aplicar Homeostase Sin√°ptica aos fragmentos modificados
        # Coleta os n√≥s √∫nicos que foram atualizados
        updated_fids = set()
        for i in range(len(fids)):
            for j in range(i + 1, len(fids)):
                updated_fids.add(fids[i])
                updated_fids.add(fids[j])

        if updated_fids:
            self._apply_synaptic_homeostasis(list(updated_fids))

        # Atualizar a lista de fragmentos ativados recentemente
        self.last_activated_fragments = fids

        # Decays/LTD movidos para janela de commit (consolidation_interval)

        if synapses_updated > 0 or new_synapses > 0:
            print(f"       >> Sinapses: {synapses_updated} atualizadas, {new_synapses} criadas")

    # NOVO: Decay sin√°ptico e depress√£o a longo prazo
    def _apply_synaptic_decay_and_ltd(self):
        """
        Aplica decay diferenciado e depress√£o a longo prazo
        """
        edges_to_remove = []
        volatile_decayed = 0
        ltd_applied = 0

        # Se subgrafo sujo estiver dispon√≠vel, restringe a ele
        if self._dirty_edges:
            candidate_edges = []
            for (u0, v0) in list(self._dirty_edges):
                if u0 in self.fragment_graph and v0 in self.fragment_graph[u0]:
                    candidate_edges.append((u0, v0, self.fragment_graph[u0][v0]))
        else:
            candidate_edges = list(self.fragment_graph.edges(data=True))

        for u, v, edge_data in candidate_edges:
            current_w = edge_data.get("w", 0.0)
            is_volatile = edge_data.get("volatile", False)
            usage_count = edge_data.get("usage_count", 0)
            last_update = edge_data.get("last_update", 0)

            # Calcular inatividade
            inactivity = self.interaction_count - last_update

            if is_volatile:
                # NOVO: Decay agressivo para sinapses vol√°teis n√£o consolidadas
                if inactivity > 2:  # 2 intera√ß√µes sem uso
                    base = self.volatile_decay_rate
                    if self._last_interaction_was_lowinfo:
                        base = max(base, self.volatile_decay_rate_lowinfo)
                    decay_factor = base ** inactivity
                    new_w = current_w * decay_factor

                    if new_w < 0.01:  # Remove sinapses vol√°teis muito fracas
                        edges_to_remove.append((u, v))
                        volatile_decayed += 1
                    else:
                        edge_data["w"] = new_w

            else:
                # Decay normal para sinapses consolidadas
                decay = self.hebb_lambda * 0.5  # Decay mais lento para consolidadas
                new_w = current_w * (1 - decay)

                # NOVO: Depress√£o a longo prazo para conex√µes pouco usadas
                if (usage_count < 2 and inactivity > 5 and
                    current_w < self.ltd_threshold):

                    # Aplicar LTD
                    ltd_decay = self.ltd_eta * (inactivity / 10.0)
                    new_w -= ltd_decay
                    ltd_applied += 1

                # NOVO: Mecanismo de Desconsolida√ß√£o
                if (inactivity > self.deconsolidation_inactivity_threshold and new_w < 0.2):
                    edge_data["volatile"] = True
                    self.plasticity_stats['synapses_deconsolidated'] += 1
                    print(f"       >> Sinapse {u}-{v} desconsolidada por desuso prolongado.")


                if new_w < 1e-5:
                    edges_to_remove.append((u, v))
                else:
                    edge_data["w"] = new_w

        # Remover sinapses fracas
        for u, v in edges_to_remove:
            if u in self.fragment_graph and v in self.fragment_graph[u]:
                self.fragment_graph.remove_edge(u, v)
                self._dirty_edges.discard((min(u, v), max(u, v)))
                # Neo4j runtime: remover aresta
                try:
                    self._neo4j_delete_edge(u, v)
                except Exception:
                    pass

        if volatile_decayed > 0:
            self.plasticity_stats['volatile_synapses_decayed'] += volatile_decayed

        if ltd_applied > 0:
            self.plasticity_stats['ltd_applications'] += ltd_applied

        self.plasticity_stats['synapses_pruned'] += len(edges_to_remove)

        if edges_to_remove:
            print(f"       >> Poda sin√°ptica: {len(edges_to_remove)} removidas "
                  f"({volatile_decayed} vol√°teis, {ltd_applied} LTD)")

    def _apply_vector_plasticity(self, fid, query_vec, activation_strength):
        """Plasticidade de padr√£o: atualiza assinatura via contadores e voto (sem FP32)."""
        fragment = self.fragments[fid]
        subspace = fragment["subspace"]
        # Fator de metaplasticidade
        meta_factor = self._calculate_metaplasticity_factor(fid)
        base_rate = self.vector_plasticity_rate * activation_strength
        adaptive_rate = base_rate * meta_factor

        # Gera padr√£o do query neste subespa√ßo
        q_proj = self.project_to_subspace(query_vec, subspace)
        q_pattern, _q_sign = self._vector_to_pattern(q_proj, k=self.pattern_k, with_sign=self.enable_pattern_sign)

        # Decai contadores antigos
        counters = self.pattern_counters[fid]
        if counters:
            for idx in list(counters.keys()):
                counters[idx] *= self.pattern_counter_decay

        # Refor√ßa contadores para √≠ndices presentes no padr√£o da consulta
        for idx in q_pattern:
            counters[int(idx)] += float(adaptive_rate)

        # Atualiza padr√£o por voto: escolhe Top-K pelos maiores contadores
        old_pattern = set(fragment.get('pattern', set()))
        if counters:
            sorted_idx = sorted(counters.items(), key=lambda x: -x[1])
            new_pattern = set(int(i) for i, _v in sorted_idx[:self.pattern_k])
        else:
            new_pattern = set(old_pattern)

        if new_pattern != old_pattern:
            # Atualiza √≠ndice invertido
            self._unregister_fragment_pattern(fid, old_pattern=old_pattern)
            fragment['pattern'] = new_pattern
            self._register_fragment_pattern(fid)
            fragment['adaptation_count'] += 1
            fragment['last_adaptation'] = self.interaction_count
            # contexto (proxy de similaridade: overlap)
            overlap = self._pattern_overlap_score(q_pattern, new_pattern)
        context_info = {
                'query_similarity': float(overlap),
                'adaptation_strength': float(adaptive_rate),
                'metaplasticity_factor': float(meta_factor),
            'plasticity_state': fragment["plasticity_state"],
            'interaction': self.interaction_count
        }
        fragment["activation_contexts"].append(context_info)
        self.plasticity_stats['vectors_adapted'] += 1
            # marca n√≥ como sujo
            self._dirty_nodes.add(fid)

    def _apply_usage_decay(self):
        """Aplica decay temporal no uso dos fragmentos"""
        decayed_fragments = 0

        for fid, fragment in self.fragments.items():
            old_usage = fragment["usage"]
            fragment["usage"] *= self.usage_decay_rate

            if fragment["usage"] < 0.01 and old_usage > 0.01:
                fragment["low_usage_marked"] = self.interaction_count
                decayed_fragments += 1

        if decayed_fragments > 0:
            print(f"       >> Decay aplicado em {decayed_fragments} fragmentos")

        self.plasticity_stats['decay_cycles'] += 1

    def _analyze_specialization_needs(self):
        """Analisa fragmentos para especializa√ß√£o ou fus√£o"""
        specializations = []
        fusions = []

        for fid, fragment in self.fragments.items():
            if len(fragment["activation_contexts"]) >= 3:
                similarities = [ctx['query_similarity'] for ctx in fragment["activation_contexts"]]
                diversity = np.std(similarities) if len(similarities) > 1 else 0.0
                fragment["specialization_score"] = diversity

                if (diversity > self.specialization_threshold and
                    fragment["usage"] > 5.0 and
                    fragment.get("adaptation_count", 0) > 3):
                    specializations.append((fid, diversity))

        # An√°lise para fus√£o
        fragment_pairs = []
        frag_ids = list(self.fragments.keys())

        for i in range(len(frag_ids)):
            for j in range(i + 1, len(frag_ids)):
                fid1, fid2 = frag_ids[i], frag_ids[j]
                frag1, frag2 = self.fragments[fid1], self.fragments[fid2]

                if frag1["subspace"] != frag2["subspace"]:
                    continue

                # Similaridade por overlap de padr√µes
                similarity = float(self._pattern_overlap_score(frag1.get('pattern', set()), frag2.get('pattern', set())))

                if (similarity > self.fusion_threshold and
                    frag1["usage"] < 2.0 and frag2["usage"] < 2.0 and
                    not frag1.get("recently_created", False) and
                    not frag2.get("recently_created", False)):
                    fusions.append((fid1, fid2, similarity))

        return specializations, fusions

    def _execute_specialization(self, fid, diversity_score):
        """Divide um fragmento em dois especializados"""
        parent = self.fragments[fid]
        subspace = parent["subspace"]

        contexts = list(parent["activation_contexts"])
        if len(contexts) < 4:
            return False

        similarities = np.array([ctx['query_similarity'] for ctx in contexts])
        median_sim = np.median(similarities)
        group1_mask = similarities <= median_sim
        group2_mask = similarities > median_sim

        if np.sum(group1_mask) < 2 or np.sum(group2_mask) < 2:
            return False

        # Mantemos refer√™ncia apenas para compatibilidade, n√£o utilizada no modo esparso
        original_vec = np.zeros_like(parent.get("original_vector", np.zeros(self.dim, dtype=np.float32)))

        # Criar fragmentos especializados
        child1_id = self.next_frag_id
        child1_vec = original_vec * 0.8 + self.rng.normal(0, 0.1, size=original_vec.shape) * 0.2
        child1_vec = child1_vec / (np.linalg.norm(child1_vec) + 1e-9)

        self.fragments[child1_id] = {
            "subspace": subspace,
            "vector": child1_vec,
            "usage": parent["usage"] * 0.6,
            "level": parent["level"],
            "label": f"spec_{fid}_1",
            "activation_contexts": deque(maxlen=self.context_memory_size),
            "parent_fragment": fid,
            "specialization_type": "low_similarity",
            "recently_created": True,
            "adaptation_count": 0,
            "original_vector": child1_vec.copy(),
            "specialization_score": 0.0,
            "activation_frequency": 0.0,
            "recent_activations": deque(maxlen=self.metaplasticity_window),
            "plasticity_state": "normal"
        }
        self.next_frag_id += 1

        child2_id = self.next_frag_id
        child2_vec = original_vec * 0.8 + self.rng.normal(0, 0.1, size=original_vec.shape) * 0.2
        child2_vec = child2_vec / (np.linalg.norm(child2_vec) + 1e-9)

        self.fragments[child2_id] = {
            "subspace": subspace,
            "vector": child2_vec,
            "usage": parent["usage"] * 0.6,
            "level": parent["level"],
            "label": f"spec_{fid}_2",
            "activation_contexts": deque(maxlen=self.context_memory_size),
            "parent_fragment": fid,
            "specialization_type": "high_similarity",
            "recently_created": True,
            "adaptation_count": 0,
            "original_vector": child2_vec.copy(),
            "specialization_score": 0.0,
            "activation_frequency": 0.0,
            "recent_activations": deque(maxlen=self.metaplasticity_window),
            "plasticity_state": "normal"
        }
        self.next_frag_id += 1

        self.fragment_graph.add_node(child1_id, **self.fragments[child1_id])
        self.fragment_graph.add_node(child2_id, **self.fragments[child2_id])

        # Transferir conex√µes do pai (preservando propriedades sin√°pticas)
        if fid in self.fragment_graph:
            neighbors = list(self.fragment_graph.neighbors(fid))
            for neighbor in neighbors:
                edge_data = self.fragment_graph[fid][neighbor]

                # Dividir peso entre filhos
                weight = edge_data.get('w', 0.0) * 0.7
                usage_count = max(1, edge_data.get('usage_count', 1))
                is_volatile = True  # Novas conex√µes come√ßam vol√°teis

                if self.rng.random() < 0.5:
                    self.fragment_graph.add_edge(child1_id, neighbor,
                                               w=weight, usage_count=usage_count,
                                               volatile=is_volatile,
                                               created_at=self.interaction_count,
                                               last_update=self.interaction_count)
                else:
                    self.fragment_graph.add_edge(child2_id, neighbor,
                                               w=weight, usage_count=usage_count,
                                               volatile=is_volatile,
                                               created_at=self.interaction_count,
                                               last_update=self.interaction_count)

        # Remover fragmento pai
        if fid in self.fragment_graph:
            self.fragment_graph.remove_node(fid)
        del self.fragments[fid]

        print(f"       >> Fragmento {fid} especializado ‚Üí {child1_id}, {child2_id}")
        self.plasticity_stats['fragments_specialized'] += 1
        return True

    def _execute_fusion(self, fid1, fid2, similarity):
        """Funde dois fragmentos similares"""
        frag1, frag2 = self.fragments[fid1], self.fragments[fid2]

        fused_id = self.next_frag_id

        # Vetor m√©dio ponderado pelo uso
        total_usage = frag1["usage"] + frag2["usage"]
        if total_usage > 0:
            w1 = frag1["usage"] / total_usage
            w2 = frag2["usage"] / total_usage
        else:
            w1 = w2 = 0.5

        # Fus√£o de padr√µes: uni√£o ponderada por uso ‚Üí top-K pelos contadores locais
        patt1 = frag1.get('pattern', set())
        patt2 = frag2.get('pattern', set())
        union = set(patt1) | set(patt2)
        scores = {}
        for idx in union:
            scores[int(idx)] = (w1 if idx in patt1 else 0.0) + (w2 if idx in patt2 else 0.0)
        fused_sorted = sorted(scores.items(), key=lambda x: -x[1])
        fused_pattern = set(int(i) for i, _ in fused_sorted[:self.pattern_k])

        self.fragments[fused_id] = {
            "subspace": frag1["subspace"],
            "pattern": fused_pattern,
            "usage": total_usage * 1.1,
            "level": max(frag1["level"], frag2["level"]),
            "label": f"fused_{fid1}_{fid2}",
            "activation_contexts": deque(maxlen=self.context_memory_size),
            "fused_from": [fid1, fid2],
            "fusion_similarity": similarity,
            "recently_created": True,
            "adaptation_count": frag1.get("adaptation_count", 0) + frag2.get("adaptation_count", 0),
            "original_vector": np.zeros(self.dim, dtype=np.float32),
            "specialization_score": 0.0,
            "activation_frequency": (frag1.get("activation_frequency", 0.0) +
                                    frag2.get("activation_frequency", 0.0)) / 2.0,
            "recent_activations": deque(maxlen=self.metaplasticity_window),
            "plasticity_state": "normal"
        }
        self.next_frag_id += 1

        self.fragment_graph.add_node(fused_id, **self.fragments[fused_id])

        # Transferir todas as conex√µes dos fragmentos originais
        neighbors1 = list(self.fragment_graph.neighbors(fid1)) if fid1 in self.fragment_graph else []
        neighbors2 = list(self.fragment_graph.neighbors(fid2)) if fid2 in self.fragment_graph else []

        all_neighbors = set(neighbors1 + neighbors2)
        all_neighbors.discard(fid1)
        all_neighbors.discard(fid2)

        for neighbor in all_neighbors:
            # Combinar propriedades sin√°pticas
            edge1_data = self.fragment_graph[fid1][neighbor] if (fid1 in self.fragment_graph and
                        self.fragment_graph.has_edge(fid1, neighbor)) else {}
            edge2_data = self.fragment_graph[fid2][neighbor] if (fid2 in self.fragment_graph and
                        self.fragment_graph.has_edge(fid2, neighbor)) else {}

            weight1 = edge1_data.get('w', 0.0)
            weight2 = edge2_data.get('w', 0.0)
            usage1 = edge1_data.get('usage_count', 0)
            usage2 = edge2_data.get('usage_count', 0)

            # Combinar pesos (manter o maior com b√¥nus)
            combined_weight = max(weight1, weight2) * 1.1
            combined_usage = usage1 + usage2

            # Nova conex√£o √© vol√°til se ambas eram ou se peso √© baixo
            is_volatile = (edge1_data.get('volatile', True) or
                          edge2_data.get('volatile', True) or
                          combined_weight < self.initial_volatile_weight * 3)

            self.fragment_graph.add_edge(fused_id, neighbor,
                                       w=combined_weight,
                                       usage_count=combined_usage,
                                       volatile=is_volatile,
                                       created_at=self.interaction_count,
                                       last_update=self.interaction_count)

        # Remover fragmentos originais
        for old_fid in [fid1, fid2]:
            if old_fid in self.fragment_graph:
                self.fragment_graph.remove_node(old_fid)
            if old_fid in self.fragments:
                del self.fragments[old_fid]

        print(f"       >> Fragmentos {fid1}, {fid2} fundidos ‚Üí {fused_id} (sim: {similarity:.3f})")
        self.plasticity_stats['fragments_fused'] += 1

        return fused_id

    def progressive_community_detection(self):
        """Progressive community detection with adaptive thresholds"""
        print("--- Detec√ß√£o progressiva de comunidades ---")

        communities_found = {}
        fragment_edges = [(u, v, d['w']) for u, v, d in self.fragment_graph.edges(data=True)
                          if d.get('w', 0) > 0]

        if len(fragment_edges) < 2:
            print("Insuficientes conex√µes para detec√ß√£o de comunidades")
            return communities_found

        sorted_edges = sorted(fragment_edges, key=lambda x: -x[2])
        thresholds = [0.15, 0.10, 0.05, 0.02]

        for threshold in thresholds:
            strong_edges = [(u, v) for u, v, w in sorted_edges if w > threshold]

            if len(strong_edges) < 2:
                continue

            try:
                subgraph = self.fragment_graph.edge_subgraph(strong_edges)
                communities = list(community.greedy_modularity_communities(subgraph))

                if len(communities) > 1:
                    print(f"Threshold {threshold}: {len(communities)} comunidades detectadas")

                    for i, comm in enumerate(communities):
                        if len(comm) >= 2:
                            concept_id = f"concept_L0_{i}_t{int(threshold*100)}"
                            communities_found[concept_id] = {
                                'members': list(comm),
                                'threshold': threshold,
                                'strength': len(comm),
                                'cohesion': self._calculate_community_cohesion(list(comm))
                            }
                    break

            except Exception as e:
                print(f"Erro na detec√ß√£o com threshold {threshold}: {e}")
                continue

        if communities_found:
            self._create_concept_relationships(communities_found)

        return communities_found

    def _calculate_community_cohesion(self, members):
        """Calculate internal cohesion of a community"""
        if len(members) < 2:
            return 0.0

        total_weight = 0.0
        connections = 0

        for i in range(len(members)):
            for j in range(i + 1, len(members)):
                if self.fragment_graph.has_edge(members[i], members[j]):
                    total_weight += self.fragment_graph[members[i]][members[j]].get('w', 0.0)
                    connections += 1

        max_connections = len(members) * (len(members) - 1) // 2
        return (total_weight / connections if connections > 0 else 0.0) * (connections / max_connections)

    def _create_concept_relationships(self, communities):
        """Create relationships between detected communities and link fragments to them."""
        print(f"--- Criando rela√ß√µes conceituais entre {len(communities)} comunidades ---")

        for concept_id, concept_data in communities.items():
            self.concept_graph.add_node(concept_id, **concept_data)
            # NOVO: Associar cada fragmento membro ao seu conceito-pai
            for fid in concept_data['members']:
                if fid in self.fragment_graph.nodes:
                    self.fragment_graph.nodes[fid]['concept_id'] = concept_id

        concept_ids = list(communities.keys())
        for i in range(len(concept_ids)):
            for j in range(i + 1, len(concept_ids)):
                c1, c2 = concept_ids[i], concept_ids[j]

                interaction = self._calculate_inter_community_interaction(
                    communities[c1]['members'],
                    communities[c2]['members']
                )

                if interaction > 0.05:
                    self.concept_graph.add_edge(c1, c2, w=interaction)
                    print(f"Conex√£o conceitual: {c1} <-> {c2} (for√ßa: {interaction:.3f})")

    def _calculate_inter_community_interaction(self, members1, members2):
        """Calculate interaction strength between communities"""
        total_weight = 0.0
        connections = 0

        for m1 in members1:
            for m2 in members2:
                if self.fragment_graph.has_edge(m1, m2):
                    total_weight += self.fragment_graph[m1][m2].get('w', 0.0)
                    connections += 1

        if connections == 0:
            return 0.0

        max_connections = len(members1) * len(members2)
        return (total_weight / connections) * (connections / max_connections)

    def compute_activations(self, query):
        # This is a hypothetical function. 
        # You may need to replace this with your actual activation logic.
        return {node: np.random.rand() for node in self.fragment_graph.nodes()}
        
    def enhanced_query_processing(self, query_text):
        """Enhanced query processing with synaptic plasticity mechanisms"""
        print(f"\n>>> CONSULTA COM PLASTICIDADE SIN√ÅPTICA: '{query_text}'")

        query_vec = self.model.encode([query_text])
        query_vec = normalize(query_vec, axis=1, norm='l2')[0]

        # NOVO: Estrutura para Explicabilidade Nativa
        activation_path = defaultdict(lambda: {'type': 'unknown', 'sources': [], 'score': 0.0})
        # Marcar low-info por tokens
        self._current_query_tokens = len(str(query_text).split())
        self._current_lowinfo = self._current_query_tokens < self.min_content_tokens

        # Fragment activation with plasticity, now tracking path
        fragment_acts = self._activate_fragments_with_plasticity(query_vec, activation_path)

        # NOVO: Propaga√ß√£o de Ativa√ß√£o Multin√≠vel para racioc√≠nio associativo
        fragment_acts = self._propagate_activation(fragment_acts, activation_path)

        # Aplicar plasticidade vetorial nos fragmentos ativados
        for fid, activation in fragment_acts.items():
            if activation > 0.3:
                self._apply_vector_plasticity(fid, query_vec, activation)

        # NOVO: Update connections with enhanced synaptic learning
        self._enhanced_hebbian_update_with_saturation(fragment_acts, query_vec)

        # Substitui atualiza√ß√£o direta por tra√ßo de elegibilidade e aplica√ß√£o seletiva
        # Usa fragment_acts (j√° com propaga√ß√£o) como ativa√ß√£o p√≥s-propaga√ß√£o
        for i in list(self.fragment_graph.nodes()):
            a_i = float(fragment_acts.get(i, 0.0))
            if a_i <= 0.0:
                continue
            state_i = self.fragments.get(i, {}).get('compute_state', self._default_compute_state)
            # Sele√ß√£o de vizinhos: top-k por peso + amostragem p‚àùw no WARM
            neighbors = []
            for j in self.fragment_graph.neighbors(i):
                if j == i:
                    continue
                w_eff = self._edge_weight_effective(i, j, hot=(state_i in ['HOT', 'WARM']))
                neighbors.append((j, w_eff))
            if not neighbors:
                continue
            neighbors.sort(key=lambda x: -x[1])
            if state_i == 'HOT':
                selected = [n for n, _ in neighbors]  # atualiza todos
            elif state_i == 'WARM':
                k = min(self.frontier_k_warm, len(neighbors))
                topk = [n for n, _ in neighbors[:k]]
                # amostragem adicional proporcional ao peso
                rest = neighbors[k:]
                sampled = []
                if rest:
                    weights = np.array([max(w, 1e-9) for _, w in rest], dtype=float)
                    probs = weights / np.sum(weights)
                    m = min(4, len(rest))
                    idx = np.random.choice(len(rest), size=m, replace=False, p=probs)
                    sampled = [rest[ii][0] for ii in idx]
                selected = topk + sampled
            else:  # COLD
                k = min(self.frontier_k_cold, len(neighbors))
                selected = [n for n, _ in neighbors[:k]]

            for j in selected:
                a_j = float(fragment_acts.get(j, 0.0))
                # Atualiza tra√ßo (sempre)
                self._update_edge_eligibility(i, j, a_i, a_j, dt=1)
                # Aplica√ß√£o imediata conforme estado
                if state_i == 'HOT' or state_i == 'WARM':
                    self.update_synaptic_weight(i, j, a_i, a_j, delta_t=1)
                    self._quantize_edge_weight(i, j)

        # Score items and build explanations
        item_scores, item_explanations = self._score_items_and_explain(fragment_acts, activation_path)
        ranked_items = sorted(item_scores.items(), key=lambda x: -x[1])[:12]
        # Reranking consciente do grafo (centralidade e coes√£o de comunidade)
        ranked_items = self._graph_aware_rerank(ranked_items)

        # Store for analysis
        self.query_history.append(query_text)
        self.activation_history.append(fragment_acts)
        self.interaction_count += 1

        # Aplicar decay a cada intera√ß√£o
        if self.interaction_count % 2 == 0:
            self._apply_usage_decay()

        # 3-fatores: computa recompensa simples (melhora de score top-1)
        if self.enable_three_factor:
            current_top = ranked_items[0][1] if ranked_items else 0.0
            reward_raw = max(0.0, current_top - self._last_top_score)
            self._last_top_score = max(self._last_top_score, current_top)
            reward = min(self.max_reward_delta, reward_raw / max(1e-6, self.reward_normalizer))
            # Se low-info (cumprimento etc.), base_reward=0
            if self._current_lowinfo:
                reward = 0.0
            if reward > 0:
                self._apply_reward_to_eligibilities(reward)
        # Commit em lote de tra√ßos de elegibilidade ‚Üí pesos
        if self.batch_apply_eligibility:
            applied = self._commit_eligibilities(reward if self.enable_three_factor else 0.0)
            if applied > 0:
                print(f"       >> Commit elegibilidade: {applied} arestas atualizadas")

        # Reindexa√ß√£o substitu√≠da por manuten√ß√£o do √≠ndice invertido (no-op aqui)

        # Periodic consolidation and plasticity analysis
        if self.interaction_count % self.consolidation_interval == 0:
            print(f"\n=== CONSOLIDA√á√ÉO E PLASTICIDADE SIN√ÅPTICA (intera√ß√£o {self.interaction_count}) ===")

            # Detectar comunidades
            communities = self.progressive_community_detection()

            # An√°lise de especializa√ß√£o/fus√£o
            specializations, fusions = self._analyze_specialization_needs()

            # Executar especializa√ß√µes (limitadas por ciclo)
            max_specializations = 2
            for i, (fid, diversity) in enumerate(specializations[:max_specializations]):
                success = self._execute_specialization(fid, diversity)
                if success:
                    self._rebuild_faiss_indices()

            # Executar fus√µes (limitadas por ciclo)
            max_fusions = 3
            for i, (fid1, fid2, sim) in enumerate(fusions[:max_fusions]):
                if fid1 in self.fragments and fid2 in self.fragments:
                    fused_id = self._execute_fusion(fid1, fid2, sim)
                    self._rebuild_faiss_indices()

            # Consolida√ß√£o tradicional
            self._consolidate_fragments()

            # NOVO: Aplicar controle de crescimento para homeostase da rede (preferir subgrafo sujo)
            if self._dirty_nodes:
                self._enforce_synaptic_limit_subset(self._dirty_nodes)
            else:
                self._enforce_synaptic_limit()

            # NOVO: Atualizar a centralidade (Personalized PageRank incremental)
            self._update_fragment_centrality_incremental(seeds=list(fragment_acts.keys()))

            # NOVO: Agendamento de comunidades por Œî|E| e intervalo
            curr_edges = len(self.fragment_graph.edges())
            prev_edges = max(1, self._last_edge_count)
            delta_e = abs(curr_edges - prev_edges) / prev_edges
            should_run_communities = (
                delta_e > self.edge_change_threshold or
                (self.interaction_count - self._last_community_interaction) >= self.community_interval
            )
            if should_run_communities:
                communities = self.progressive_community_detection()
                if communities:
                    self._last_community_interaction = self.interaction_count
                self._last_edge_count = curr_edges

            # Decay/LTD apenas na janela de commit e preferindo subgrafo sujo
            self._apply_synaptic_decay_and_ltd()

            # Atualiza/poda fast paths com base em coativa√ß√£o e peso
            self._update_fast_paths()

            # Push incremental para Neo4j (opcional)
            if os.getenv("NEO4J_AUTO_PUSH", "on").lower() in ("on", "auto") and GraphDatabase is not None:
                try:
                    self.push_incremental_to_neo4j()
                except Exception as e:
                    print(f"Neo4j: erro no push incremental: {e}")

            # Limpa marca√ß√µes sujas ap√≥s commit
            self._dirty_nodes.clear()
            self._dirty_edges.clear()

            # Avan√ßa fase de gating
            self.phase = (self.phase + 1) % self.phase_k

        # Limpar flags de cria√ß√£o recente
        for frag in self.fragments.values():
            frag.pop('recently_created', None)

        # Marca se esta intera√ß√£o foi low-info (para decay/poda posterior)
        self._last_interaction_was_lowinfo = bool(self._current_lowinfo)

        # Log temporal no Neo4j (opcional)
        if os.getenv("NEO4J_LOG_INTERACTIONS", "off").lower() == "on" and GraphDatabase is not None:
            try:
                self._log_interaction_to_neo4j(query_text, fragment_acts)
            except Exception as e:
                print(f"AVISO: Falha ao logar intera√ß√£o no Neo4j: {e}")

        # NOVO: Estat√≠sticas sin√°pticas expandidas
        synaptic_stats = self._get_synaptic_network_stats()

        # Build final results with explanations
        top_items_with_explanations = []
        for iid, score in ranked_items:
            item_info = (iid, self.items_index[iid]["text"], score, item_explanations.get(iid, "Nenhuma explica√ß√£o gerada."))
            top_items_with_explanations.append(item_info)

        results = {
            'query': query_text,
            'fragment_activations': fragment_acts,
            'top_items': top_items_with_explanations,
            'active_fragments': len(fragment_acts),
            'plasticity_applied': sum(1 for act in fragment_acts.values() if act > 0.3),
            'system_changes': {
                'vectors_adapted': self.plasticity_stats['vectors_adapted'],
                'fragments_specialized': self.plasticity_stats['fragments_specialized'],
                'fragments_fused': self.plasticity_stats['fragments_fused'],
                'synapses_created': self.plasticity_stats['synapses_created'],
                'synapses_consolidated': self.plasticity_stats['synapses_consolidated'],
                'synapses_pruned': self.plasticity_stats['synapses_pruned']
            },
            'synaptic_network': synaptic_stats
        }

        return results

    def _log_interaction_to_neo4j(self, query_text, fragment_acts):
        uri = os.getenv("NEO4J_URI")
        user = os.getenv("NEO4J_USER")
        password = os.getenv("NEO4J_PASSWORD")
        if not (uri and user and password):
            return False
        driver = GraphDatabase.driver(uri, auth=(user, password))
        try:
            with driver.session() as session:
                ts = datetime.utcnow().isoformat() + 'Z'
                interaction_id = f"int_{self.interaction_count+1}"
                session.run(
                    """
                    MERGE (i:Interaction {id: $iid})
                    SET i.query = $query, i.timestamp = $ts
                    """,
                    iid=interaction_id, query=query_text, ts=ts
                )
                # relaciona com fragmentos ativados
                rows = []
                for fid, act in fragment_acts.items():
                    rows.append({'iid': interaction_id, 'fid': int(fid), 'a': float(act)})
                def chunks(lst, n):
                    for i in range(0, len(lst), n):
                        yield lst[i:i+n]
                for batch in chunks(rows, 500):
                    session.run(
                        """
                        UNWIND $rows AS row
                        MATCH (f:Fragment {id: row.fid})
                        MATCH (i:Interaction {id: row.iid})
                        MERGE (i)-[r:ACTIVATED_AT]->(f)
                        SET r.activation = row.a
                        """,
                        rows=batch
                    )
        finally:
            driver.close()
        return True

    def _get_synaptic_network_stats(self):
        """Coleta estat√≠sticas da rede sin√°ptica"""
        if not self.fragment_graph.edges():
            return {'total_synapses': 0}

        edge_data = list(self.fragment_graph.edges(data=True))
        weights = [d.get('w', 0.0) for u, v, d in edge_data]
        volatile_count = sum(1 for u, v, d in edge_data if d.get('volatile', False))
        consolidated_count = len(edge_data) - volatile_count
        usage_counts = [d.get('usage_count', 0) for u, v, d in edge_data]

        return {
            'total_synapses': len(edge_data),
            'volatile_synapses': volatile_count,
            'consolidated_synapses': consolidated_count,
            'avg_weight': np.mean(weights),
            'max_weight': np.max(weights),
            'avg_usage': np.mean(usage_counts),
            'strong_synapses': sum(1 for w in weights if w > 0.5),
            'weak_synapses': sum(1 for w in weights if w < 0.1),
            'consolidation_rate': consolidated_count / len(edge_data) if edge_data else 0.0
        }

    def _activate_fragments_with_plasticity(self, query_vec, activation_path):
        """Ativa√ß√£o baseada em assinaturas esparsas (overlap Top-K) com plasticidade e explicabilidade."""
        fragment_acts = {}
        dynamic_created = 0

        for s in range(self.n_subspaces):
            # Projeta e gera padr√£o esparso para o query
            z_proj = self.project_to_subspace(query_vec, s)
            q_pattern, _q_sign = self._vector_to_pattern(z_proj, k=self.pattern_k, with_sign=self.enable_pattern_sign)
            if not q_pattern:
                continue

            # Candidatos via √≠ndice invertido (uni√£o de listas por features do padr√£o)
            candidate_ids = set()
            inv = self.signature_index[s]
            for feat in q_pattern:
                if feat in inv:
                    candidate_ids |= set(inv[int(feat)])

            # Se n√£o houver candidatos, considerar cria√ß√£o din√¢mica inicial
            best_score = 0.0
            scored = []
            for fid in candidate_ids:
                patt = self.fragments.get(fid, {}).get('pattern', set())
                if not patt:
                    continue
                ov = self._pattern_overlap_score(q_pattern, patt, method='jaccard')
                if ov <= 0.0:
                    continue
                # Boost de contexto
                        context_boost = self._get_enhanced_context_boost(fid)
                final_activation = float(ov) * (1.0 + context_boost)
                scored.append((fid, final_activation))
                if final_activation > best_score:
                    best_score = final_activation

            # Ranking top-K vizinhos por overlap
            scored.sort(key=lambda x: -x[1])
            for fid, final_activation in scored[:12]:
                             activation_path[fid]['type'] = 'direta'
                             activation_path[fid]['score'] = final_activation
                activation_path[fid]['sources'] = [{'subspace': s, 'overlap': float(final_activation)}]
                fragment_acts[fid] = max(fragment_acts.get(fid, 0.0), float(final_activation))
                        self.fragments[fid]["usage"] += 1.0
                        self.fragments[fid]["last_activation"] = self.interaction_count
                        self._dirty_nodes.add(fid)

            # Pol√≠tica de cria√ß√£o din√¢mica baseada em novidade (baixo overlap m√°ximo)
            should_create_dynamic = (best_score < (1.0 - self.novelty_threshold)) and (dynamic_created < 2) and (self.interaction_count < 20)
            token_like_len = int(len(str(query_vec).split())) if isinstance(query_vec, str) else 4
            create_guard = (token_like_len >= 3)
            if should_create_dynamic and create_guard:
                new_fid = self._add_dynamic_fragment_pattern(s, q_pattern)
                fragment_acts[new_fid] = 1.0
                activation_path[new_fid]['type'] = 'dinamico'
                activation_path[new_fid]['score'] = 1.0
                activation_path[new_fid]['sources'].append({'subspace': s, 'novelty': 1.0 - best_score})
                dynamic_created += 1

        print(f"     Fragmentos ativados: {len(fragment_acts)}, Din√¢micos criados: {dynamic_created}")

        # Orquestra estados COLD/WARM/HOT (reutiliza a mesma l√≥gica)
        hot, warm, cold = self._orchestrate_compute_states(fragment_acts)
        # Mant√©m pontua√ß√µes j√° calculadas (sem dot), apenas escreve modo para explicabilidade
        for fid in list(fragment_acts.keys()):
            if fid in hot:
                mode = 'HOT'
            elif fid in warm:
                mode = 'WARM'
            else:
                mode = 'COLD'
            if activation_path[fid]['type'] == 'direta' and activation_path[fid]['sources']:
                activation_path[fid]['sources'][0]['mode'] = mode
        return fragment_acts

    def _get_enhanced_context_boost(self, fid):
        """Calculate enhanced context boost including plasticity factors"""
        fragment = self.fragments[fid]
        boost = 0.0

        # Boost tradicional por ativa√ß√µes recentes
        if not self.activation_history:
            return boost

        recent_activations = 0
        for past_activation in self.activation_history[-3:]:
            if fid in past_activation:
                recent_activations += 1

        boost += recent_activations * 0.2

        # Boost por adapta√ß√£o recente
        if fragment.get("adaptation_count", 0) > 0:
            boost += min(0.15, fragment["adaptation_count"] * 0.03)

        # NOVO: Boost por estado de metaplasticidade
        plasticity_state = fragment.get("plasticity_state", "normal")
        if plasticity_state == "potentiated":
            boost += 0.1
        elif plasticity_state == "depressed":
            boost -= 0.05

        # Penalidade por baixo uso (decay)
        if fragment["usage"] < 1.0:
            boost *= fragment["usage"]

        return boost

    def _add_dynamic_fragment(self, subspace, vector, similarity):
        """Mantido por compatibilidade: delega para assinatura esparsa a partir de vetor."""
        pattern, sign = self._vector_to_pattern(vector, k=self.pattern_k, with_sign=self.enable_pattern_sign)
        return self._add_dynamic_fragment_pattern(subspace, pattern, sign)

    def _add_dynamic_fragment_pattern(self, subspace, pattern, sign=None):
        """Cria fragmento din√¢mico a partir de um padr√£o esparso."""
        fid = self.next_frag_id
        self.fragments[fid] = {
            "subspace": subspace,
            "pattern": set(pattern) if pattern else set(),
            "sign": dict(sign) if (sign is not None) else {},
            "usage": 1.0,
            "level": 0,
            "label": f"frag_s{subspace}_{fid}_dyn",

            # Plasticidade features
            "activation_contexts": deque(maxlen=self.context_memory_size),
            "last_activation": self.interaction_count,
            "adaptation_count": 0,
            "original_vector": np.zeros(self.dim, dtype=np.float32),
            "specialization_score": 0.0,
            "recently_created": True,
            "novelty_score": 1.0,

            # NOVO: Metaplasticidade
            "activation_frequency": 0.0,
            "recent_activations": deque(maxlen=self.metaplasticity_window),
            "plasticity_state": "normal"
        }

        self.fragment_graph.add_node(fid, **self.fragments[fid])
        # registra no √≠ndice invertido
        self._register_fragment_pattern(fid)
        self.next_frag_id += 1
        print(f"       >> Fragmento din√¢mico {fid} criado (pattern |A|={len(pattern)})")
        return fid

    def _score_items_and_explain(self, fragment_acts, activation_path):
        """Score items with activation spreading and generate explanations."""
        scores = {}
        explanations = {}
        for iid, record in self.items_index.items():
            score = 0.0
            direct_hits = []
            propagated_hits = []

            for link in record["links"]:
                # Suporta formatos (fid, peso) e (fid, peso, subspace)
                if isinstance(link, (list, tuple)) and len(link) == 3:
                    fid, weight, link_subspace = link
                else:
                    fid, weight = link[0], link[1]
                    link_subspace = self.fragments[fid]['subspace'] if fid in self.fragments else None
                activation = fragment_acts.get(fid, 0.0)
                if activation > 0:
                    score += activation * weight
                    path_info = activation_path.get(fid)
                    if path_info:
                        if path_info['type'] in ['direta', 'dinamico']:
                            subspace_label = link_subspace if link_subspace is not None else (self.fragments[fid]['subspace'] if fid in self.fragments else '?')
                            direct_hits.append(f"frag_{fid} (subespa√ßo {subspace_label}, sim: {path_info['score']:.2f})")
                        else: # Propagated
                            propagated_hits.append(f"frag_{fid} (ativado por associa√ß√£o)")

            if score > 0:
                scores[iid] = score
                explanation = f"Pontua√ß√£o: {score:.2f}. "
                if direct_hits:
                    explanation += f"Relev√¢ncia direta via {', '.join(direct_hits)}. "
                if propagated_hits:
                    explanation += f"Relev√¢ncia associativa via {', '.join(propagated_hits)}."
                explanations[iid] = explanation

        return scores, explanations

    def _graph_aware_rerank(self, ranked_items, alpha=0.15, beta=0.10, final_k=5):
        """Ajusta o ranking usando centralidade m√©dia e coes√£o de comunidade dos fragmentos linkados ao item."""
        if not ranked_items:
            return ranked_items
        item_boosts = {}
        for iid, base in ranked_items:
            links = self.items_index.get(iid, {}).get('links', [])
            if not links:
                item_boosts[iid] = 1.0
                continue
            centralities = []
            communities = []
            for link in links:
                fid = link[0]
                frag = self.fragments.get(fid)
                if not frag:
                    continue
                centralities.append(float(frag.get('centrality', 0.0)))
                communities.append(frag.get('concept_id'))
            avg_centrality = float(np.mean(centralities)) if centralities else 0.0
            filtered = [c for c in communities if c]
            cohesion = 0.0
            if filtered:
                most = max(filtered, key=filtered.count)
                freq = filtered.count(most)
                cohesion = freq / max(1, len(filtered))
            boost = 1.0 + alpha * avg_centrality + beta * cohesion
            item_boosts[iid] = boost
        rescored = [(iid, score * item_boosts.get(iid, 1.0)) for iid, score in ranked_items]
        rescored.sort(key=lambda x: -x[1])
        return rescored[:final_k]

    def _enforce_synaptic_limit(self):
        """Poda as sinapses mais fracas de fragmentos que excedem o limite de conex√µes."""
        pruned_count = 0
        for fid in list(self.fragment_graph.nodes()):
            if fid not in self.fragment_graph:
                continue

            degree = self.fragment_graph.degree(fid)

            if degree > self.max_synapses_per_fragment:
                edges = list(self.fragment_graph.edges(fid, data=True))

                # Ordena as arestas pela sua for√ßa (peso 'w'), das mais fracas para as mais fortes
                edges.sort(key=lambda x: x[2].get('w', 0.0))

                num_to_prune = degree - self.max_synapses_per_fragment

                for i in range(num_to_prune):
                    u, v, _ = edges[i]
                    self.fragment_graph.remove_edge(u, v)
                    pruned_count += 1

        if pruned_count > 0:
            print(f"       >> Controle de Crescimento: {pruned_count} sinapses podadas para manter homeostase.")
            self.plasticity_stats['synapses_pruned'] += pruned_count

    def _enforce_synaptic_limit_subset(self, node_subset):
        """Vers√£o subset: aplica limite apenas nos n√≥s do conjunto fornecido."""
        pruned_count = 0
        for fid in list(node_subset):
            if fid not in self.fragment_graph:
                continue
            degree = self.fragment_graph.degree(fid)
            if degree > self.max_synapses_per_fragment:
                edges = list(self.fragment_graph.edges(fid, data=True))
                edges.sort(key=lambda x: x[2].get('w', 0.0))
                num_to_prune = degree - self.max_synapses_per_fragment
                for i in range(num_to_prune):
                    u, v, _ = edges[i]
                    if u in self.fragment_graph and v in self.fragment_graph[u]:
                        self.fragment_graph.remove_edge(u, v)
                        pruned_count += 1
                        self._dirty_edges.discard((min(u, v), max(u, v)))
        if pruned_count > 0:
            print(f"       >> Controle de Crescimento (subset): {pruned_count} sinapses podadas.")
            self.plasticity_stats['synapses_pruned'] += pruned_count

    def _propagate_activation(self, initial_activations, activation_path):
        """
        Propaga a ativa√ß√£o atrav√©s da rede sin√°ptica e rastreia o caminho para explicabilidade.
        """
        if not self.fragment_graph.edges():
            return initial_activations

        print("       >> Propagando ativa√ß√£o pela rede...")

        # Seeds por erro preditivo (se dispon√≠vel)
        seeds = {}
        for fid, a in initial_activations.items():
            baseline = self.baseline_act[fid]
            seeds[fid] = max(0.0, float(a) - float(baseline))
        all_time_activations = seeds.copy()
        current_activations = seeds.copy()

        for step in range(self.propagation_steps):
            propagated_activations = defaultdict(float)

            # Event-driven: s√≥ espalha se Œîa > epsilon
            nodes_to_spread = []
            for fid, act in current_activations.items():
                prev = activation_path.get(fid, {}).get('score', 0.0)
                if abs(act - float(prev)) > self.delta_a_epsilon and act > 0.1:
                    nodes_to_spread.append(fid)

            for fid in nodes_to_spread:
                if fid not in self.fragment_graph:
                    continue

                activation_to_spread = current_activations[fid]

                # Sele√ß√£o top-k de vizinhos (frontier) por estado do n√≥ atual
                compute_state = self.fragments.get(fid, {}).get('compute_state', self._default_compute_state)
                if compute_state == 'HOT':
                    k_frontier = self.frontier_k_hot
                elif compute_state == 'WARM':
                    k_frontier = self.frontier_k_warm
                else:
                    k_frontier = self.frontier_k_cold

                # Coleta vizinhos e pesos efetivos
                neighbors = []

                # 1) Fast paths primeiro
                for neighbor in list(self.fast_paths.get(fid, [])):
                    if neighbor == fid or all_time_activations.get(neighbor, 0.0) > 0.9:
                        continue
                    if neighbor in self.fragment_graph[fid]:
                        w_eff = self._edge_weight_effective(fid, neighbor, hot=True)
                        neighbors.append((neighbor, w_eff * 1.2))  # pequeno boost para fast path

                # 2) Demais arestas
                for neighbor in self.fragment_graph.neighbors(fid):
                    if neighbor == fid or all_time_activations.get(neighbor, 0.0) > 0.9:
                        continue
                    hot_effective = (compute_state in ['HOT', 'WARM'])
                    w_eff = self._edge_weight_effective(fid, neighbor, hot=hot_effective)
                    neighbors.append((neighbor, w_eff))

                # Ordena por hubs: hub_score* w, depois pega top-k e aplica WTA
                if neighbors:
                    # hub_score: usa centralidade atual como proxy
                    scored = []
                    for v, w in neighbors:
                        hub = float(self.fragments.get(v, {}).get('centrality', 0.0))
                        scored.append((v, w * (1.0 + hub)))
                    scored.sort(key=lambda x: -x[1])
                    selected = scored[:k_frontier]
                else:
                    selected = []

                for rank, (neighbor, base_weight) in enumerate(selected, start=1):
                    edge_data = self.fragment_graph[fid][neighbor]
                    # WTA local: inibi√ß√£o por rank
                    inhib = max(0.0, 1.0 - self.wta_nu * (rank - 1))
                    weight = base_weight * inhib

                    # STP ainda pode modular o peso efetivo
                    if self.enable_stp:
                        if 'stp_u' not in edge_data:
                            edge_data['stp_u'] = self.stp_U
                        if 'stp_x' not in edge_data:
                            edge_data['stp_x'] = 1.0
                        u = edge_data['stp_u']
                        x = edge_data['stp_x']
                        u = u + (self.stp_U - u) / max(1.0, self.stp_tau_f)
                        u = min(1.0, max(0.0, u))
                        x = x + (1.0 - x) / max(1.0, self.stp_tau_d)
                        x = x * (1.0 - u)
                        edge_data['stp_u'] = u
                        edge_data['stp_x'] = x
                        weight = base_weight * u * x

                    if weight > 0.0 and not edge_data.get('volatile', True):
                        propagated_energy = activation_to_spread * weight * self.propagation_damping_factor
                        if propagated_energy >= self.energy_cutoff:
                            propagated_activations[neighbor] += propagated_energy

            if not propagated_activations:
                break

            newly_activated_count = 0
            for fid, energy in propagated_activations.items():
                current_level = all_time_activations.get(fid, 0.0)
                if current_level < 1.0: # Only update if not saturated
                    # Explanation update
                    if activation_path[fid]['type'] == 'unknown':
                        activation_path[fid]['type'] = 'propagada'
                        newly_activated_count += 1

                    activation_path[fid]['score'] = min(1.0, current_level + energy)
                    # Note: source tracking for propagation can get complex, skipping for now to keep it clean.

                    all_time_activations[fid] = min(1.0, current_level + energy)

            current_activations = propagated_activations

            if newly_activated_count > 0:
                print(f"       ... Passo {step+1}: {newly_activated_count} novos fragmentos ativados por associa√ß√£o.")

        # Atualiza baseline (m√©dia m√≥vel) ap√≥s propaga√ß√£o
        for fid, a in all_time_activations.items():
            prev = self.baseline_act[fid]
            self.baseline_act[fid] = (1 - self.baseline_beta) * prev + self.baseline_beta * float(a)
        return all_time_activations

    def _consolidate_fragments(self):
        """Consolidate fragments with plasticity awareness"""
        removed = []
        for fid, fragment in list(self.fragments.items()):
            # Remove fragments with very low usage, considering plasticity
            should_remove = (
                fragment["usage"] < 0.5 and
                self.interaction_count > 10 and
                'dyn' in fragment.get('label', '') and
                fragment.get("adaptation_count", 0) < 2 and
                not fragment.get("recently_created", False) and
                fragment.get("plasticity_state", "normal") != "potentiated"
            )

            if should_remove:
                removed.append(fid)
                del self.fragments[fid]
                if fid in self.fragment_graph:
                    self.fragment_graph.remove_node(fid)

        if removed:
            print(f"     Consolida√ß√£o removeu {len(removed)} fragmentos pouco usados")
            # Neo4j runtime: remover n√≥s
            for fid in removed:
                try:
                    self._neo4j_delete_fragment(fid)
                except Exception:
                    pass

    def _update_fragment_centrality(self):
        """Calculates PageRank centrality for all fragments and stores it."""
        if len(self.fragment_graph) > 1:
            print("       >> Calculando centralidade do grafo (PageRank)...")
            try:
                # Use 'w' as the weight for PageRank calculation
                pagerank_scores = nx.pagerank(self.fragment_graph, alpha=0.85, weight='w')

                # Normalize scores for easier use (0 to 1)
                max_centrality = max(pagerank_scores.values()) if pagerank_scores else 1.0
                if max_centrality == 0:
                    max_centrality = 1.0 # Avoid division by zero

                # Update centrality for each fragment
                for fid, fragment in self.fragments.items():
                    normalized_score = pagerank_scores.get(fid, 0.0) / max_centrality
                    fragment['centrality'] = normalized_score

                print(f"       >> Centralidade atualizada para {len(pagerank_scores)} fragmentos.")

            except Exception as e:
                print(f"       >> AVISO: Falha ao calcular PageRank: {e}")

    def _update_fragment_centrality_incremental(self, seeds, alpha=0.85, tol=1e-4, max_push=5000, max_hops=2):
        """Personalized PageRank incremental (push-based) seedado nos fragmentos ativados.
        Atualiza centralidade apenas no subgrafo tocado para reduzir custo.
        """
        if not seeds:
            return
        # Coleta subgrafo at√© max_hops
        visited = set(seeds)
        frontier = list(seeds)
        for _ in range(max_hops):
            next_frontier = []
            for u in list(frontier):
                if u not in self.fragment_graph:
                    continue
                for v in self.fragment_graph.neighbors(u):
                    if v not in visited:
                        visited.add(v)
                        next_frontier.append(v)
            frontier = next_frontier
            if not frontier:
                break
        if not visited:
            return
        # Prepara estruturas de pesos de sa√≠da
        out_strength = {}
        neighbors = {}
        for u in visited:
            if u not in self.fragment_graph:
                continue
            neighs = []
            strength = 0.0
            for v in self.fragment_graph.neighbors(u):
                if v in visited:
                    w = float(self.fragment_graph[u][v].get('w', 0.0))
                    if w > 0:
                        neighs.append((v, w))
                        strength += w
            neighbors[u] = neighs
            out_strength[u] = strength
        # PPR push-based
        p = defaultdict(float)
        r = defaultdict(float)
        seed_mass = 1.0 / max(1, len(seeds))
        for s in seeds:
            r[s] = seed_mass
        pushes = 0
        queue = deque([s for s in seeds])
        while queue and pushes < max_push:
            u = queue.popleft()
            ru = r[u]
            if ru <= 0:
                continue
            deg = out_strength.get(u, 0.0)
            if deg == 0.0 and ru < tol:
                continue
            # push
            p[u] += alpha * ru
            residual = (1.0 - alpha) * ru
            r[u] = 0.0
            if deg > 0.0:
                for v, w in neighbors.get(u, []):
                    add = residual * (w / deg)
                    old = r[v]
                    r[v] = old + add
                    if r[v] > tol:
                        queue.append(v)
            pushes += 1
        # Normaliza p localmente e grava em 'centrality'
        if p:
            max_p = max(p.values())
            if max_p <= 0:
                max_p = 1.0
            for u, val in p.items():
                if u in self.fragments:
                    self.fragments[u]['centrality'] = float(val / max_p)

    def _update_fast_paths(self):
        """Promove/demove fast paths a partir de coativa√ß√£o acumulada e peso m√©dio das arestas."""
        # Decai coativa√ß√£o e aplica promo√ß√£o/despromo√ß√£o
        promote = []
        demote = []
        for (u, v), score in list(self.edge_coact.items()):
            new_score = score * self.fast_decay
            self.edge_coact[(u, v)] = new_score
            # Peso m√©dio atual
            if u in self.fragment_graph and v in self.fragment_graph[u]:
                w = float(self.fragment_graph[u][v].get('w', 0.0))
            else:
                w = 0.0
            if new_score >= self.fast_coact_threshold and w >= self.fast_weight_threshold:
                promote.append((u, v))
            elif new_score < self.fast_demote_threshold:
                demote.append((u, v))
        # Aplica mudan√ßas
        for u, v in promote:
            self.fast_paths[u].add(v)
        for u, v in demote:
            if v in self.fast_paths.get(u, set()):
                self.fast_paths[u].discard(v)

    # =====================
    # Benchmarks e M√©tricas
    # =====================
    def compute_and_log_benchmarks(self):
        """Calcula m√©tricas de reten√ß√£o, transfer√™ncia e curr√≠culo emergente.

        Heur√≠sticas r√°pidas:
        - reten√ß√£o: estabilidade dos pesos fortes ao longo do tempo (1 - var normalizada)
        - transfer√™ncia: propor√ß√£o de ativa√ß√µes que cruzam subespa√ßos mantendo alta coativa√ß√£o
        - curr√≠culo: profundidade m√©dia de caminhos usados recentemente (via fast_paths e trajet√≥rias)
        - entropias auxiliares: de trajet√≥rias (fast_paths) e de coativa√ß√£o
        """
        t = int(self.interaction_count)
        # Reten√ß√£o: mede estabilidade dos top-K pesos
        weights = []
        for _, _, d in self.fragment_graph.edges(data=True):
            w = float(d.get('w', 0.0))
            if w > 0:
                weights.append(w)
        retention_score = 0.0
        if weights:
            W = np.asarray(weights, dtype=float)
            k = min(64, len(W))
            top = np.sort(W)[-k:]
            # aproxima√ß√£o: menor vari√¢ncia relativa ‚áí maior reten√ß√£o
            var_rel = float(np.var(top) / max(1e-9, np.mean(top)**2)) if np.mean(top) > 0 else 1.0
            retention_score = float(np.clip(1.0 - var_rel, 0.0, 1.0))
        self.bench['retention_scores'].append((t, retention_score))

        # Transfer√™ncia: ativa√ß√µes cross-subspace com coativa√ß√£o alta
        transfer_hits = 0
        total_hits = 0
        window = self.activation_history[-10:] if self.activation_history else []
        for acts in window:
            active_ids = [fid for fid, a in acts.items() if a >= self.meta_activation_threshold]
            for i in range(len(active_ids)):
                for j in range(i+1, len(active_ids)):
                    fi, fj = active_ids[i], active_ids[j]
                    si = self.fragments.get(fi, {}).get('subspace', -1)
                    sj = self.fragments.get(fj, {}).get('subspace', -1)
                    if si == -1 or sj == -1 or fi == fj:
                        continue
                    total_hits += 1
                    if si != sj:
                        # coativa√ß√£o recente como proxy de transfer√™ncia
                        key = (min(fi, fj), max(fi, fj))
                        if self.edge_coact.get(key, 0.0) >= self.fast_coact_threshold:
                            transfer_hits += 1
        transfer_score = float(transfer_hits / max(1, total_hits)) if total_hits > 0 else 0.0
        self.bench['transfer_scores'].append((t, transfer_score))

        # Curr√≠culo: profundidade m√©dia de caminhos recentes em fast_paths
        depth_sum = 0
        depth_cnt = 0
        for u, vs in self.fast_paths.items():
            for v in vs:
                depth_sum += 1
                depth_cnt += 1
        curriculum_depth = float(depth_sum / max(1, depth_cnt))
        self.bench['curriculum_depth'].append((t, curriculum_depth))

        # Entropia das trajet√≥rias (grau de distribui√ß√£o dos destinos nos fast_paths)
        out_counts = []
        for u, vs in self.fast_paths.items():
            out_counts.append(len(vs))
        if out_counts:
            C = np.asarray(out_counts, dtype=float)
            p = C / float(np.sum(C) + 1e-9)
            H_traj = -float(np.sum(p * np.log(p + 1e-12)))
            H_traj_norm = float(H_traj / max(1e-9, np.log(len(C))))
        else:
            H_traj_norm = 0.0
        self.bench['trajectory_entropy'].append((t, H_traj_norm))

        # Entropia de coativa√ß√£o na janela
        co_vals = np.asarray(list(self.edge_coact.values()), dtype=float) if self.edge_coact else np.asarray([])
        if co_vals.size > 0 and float(np.sum(co_vals)) > 0:
            p = co_vals / float(np.sum(co_vals))
            H_co = -float(np.sum(p * np.log(p + 1e-12)))
            H_co_norm = float(H_co / max(1e-9, np.log(co_vals.size)))
        else:
            H_co_norm = 0.0
        self.bench['coactivation_entropy'].append((t, H_co_norm))

    def _apply_synaptic_homeostasis(self, fids_to_check):
        """
        Aplica homeostase sin√°ptica, normalizando os pesos de sa√≠da de fragmentos
        cuja soma total de pesos excede um limite.
        """
        normalized_count = 0

        # Meta-PID local por subespa√ßo: atualiza escalas de normaliza√ß√£o conforme
        # fra√ß√£o de arestas vol√°teis e entropia local de pesos.
        by_sub = defaultdict(list)
        for fid in fids_to_check:
            if fid in self.fragments:
                s = self.fragments[fid].get('subspace', 0)
                by_sub[s].append(fid)

        for s, fids in by_sub.items():
            weights = []
            total_edges = 0
            volatile_count = 0
            for u in fids:
                for _, v, d in self.fragment_graph.edges(u, data=True):
                    if v not in fids:
                        continue
                    total_edges += 1
                    w = float(d.get('w', 0.0))
                    if w > 0:
                        weights.append(w)
                    if d.get('volatile', False):
                        volatile_count += 1
            vol_frac = float(volatile_count / max(1, total_edges)) if total_edges > 0 else 0.0
            if weights:
                W = np.asarray(weights, dtype=float)
                p = W / float(np.sum(W))
                H = -float(np.sum(p * np.log(p + 1e-12)))
                H_norm = float(H / max(1e-9, np.log(len(W))))
            else:
                H_norm = 0.0

            err_vol = (vol_frac - self.local_pid_target_vol_frac)
            err_ent = (H_norm - self.local_pid_target_entropy)
            error = self.local_pid_w_vol * err_vol + self.local_pid_w_ent * err_ent

            self.local_pid_integral[s] += error
            d_err = error - self.local_pid_prev_error[s]
            self.local_pid_prev_error[s] = error
            u_pid = -(self.local_pid_Kp * error + self.local_pid_Ki * self.local_pid_integral[s] + self.local_pid_Kd * d_err)

            new_scale = float(min(max(self.local_weight_scale[s] + u_pid, 0.5), 1.5))
            self.local_weight_scale[s] = new_scale
        for fid in fids_to_check:
            if fid not in self.fragment_graph:
                continue

            # Obter todas as arestas de sa√≠da e seus pesos
            outgoing_edges = list(self.fragment_graph.edges(fid, data=True))
            if not outgoing_edges:
                continue

            # Calcular a soma total dos pesos de sa√≠da com teto local escalado
            s = self.fragments.get(fid, {}).get('subspace', 0)
            local_cap = float(self.max_total_synaptic_weight) * float(self.local_weight_scale[s])
            total_weight = sum(edge[2].get('w', 0.0) for edge in outgoing_edges)

            # Se a soma exceder o limite, aplicar normaliza√ß√£o
            if total_weight > local_cap:
                normalized_count += 1
                # Fator de normaliza√ß√£o para reescalar os pesos
                normalization_factor = local_cap / total_weight

                for u, v, edge_data in outgoing_edges:
                    # Reduzir o peso de cada sinapse proporcionalmente
                    edge_data['w'] *= normalization_factor

        if normalized_count > 0:
            print(f"       >> Homeostase Sin√°ptica: {normalized_count} fragmentos normalizados.")

    def _apply_reward_to_eligibilities(self, reward):
        """Aplica recompensa aos tra√ßos de elegibilidade (aprendizado 3-fatores).

        Extens√£o: adiciona componente preditiva baseada em ganho de informa√ß√£o (IG)
        e energia livre aproximada (blend energia/entropia do grafo), funcionando
        como feedforward reflexivo antes do feedback externo.
        """
        if not self.enable_three_factor:
            return

        hot_phase = (self.phase % self.phase_k) in (0, 1)
        # Recompensa externa escalada
        scaled_reward = (self.reward_eta * float(reward)) if hot_phase else (self.reward_eta * 0.25 * float(reward))

        # --- Componente preditiva: IG e -F ---
        try:
            # Aproxima IG por redu√ß√£o esperada de entropia local dos pesos
            # e/ou incerteza de arestas (vari√¢ncia impl√≠cita pelos estados vol√°teis)
            def _local_entropy():
                weights = []
                for _, _, d in self.fragment_graph.edges(data=True):
                    w = float(d.get('w', 0.0))
                    if w > 0:
                        weights.append(w)
                if not weights:
                    return 0.0
                W = np.asarray(weights, dtype=float)
                p = W / float(np.sum(W))
                H = -float(np.sum(p * np.log(p + 1e-12)))
                H_max = float(np.log(len(W))) if len(W) > 0 else 1.0
                return float(H / max(H_max, 1e-9))

            H_norm_before = _local_entropy()
            # estimativa simples do IG: maior quando h√° muitas arestas vol√°teis/incertas
            num_volatile = 0
            total_e = 0
            for _, _, d in self.fragment_graph.edges(data=True):
                total_e += 1
                if d.get('volatile', False):
                    num_volatile += 1
            uncertainty_ratio = float(num_volatile / max(1, total_e))
            IG_hat = 0.5 * H_norm_before + 0.5 * uncertainty_ratio

            # Proxy de energia livre F ~ blend(energia, -entropia)
            # Reutiliza l√≥gica do adapter: energia m√©dia dos pesos e entropia normalizada
            weights_energy = []
            for _, _, d in self.fragment_graph.edges(data=True):
                w = float(d.get('w', 0.0))
                if w > 0:
                    weights_energy.append(w)
            if weights_energy:
                W_arr = np.asarray(weights_energy, dtype=float)
                E_norm = float(np.mean(W_arr))
                p_w = W_arr / float(np.sum(W_arr))
                H_graph = -float(np.sum(p_w * np.log(p_w + 1e-12)))
                H_norm = float(H_graph / max(1e-9, np.log(len(W_arr))))
                # F mais baixo quando energia baixa e entropia alta; aqui usamos uma proxy simples
                F_proxy = max(0.0, E_norm - H_norm)
            else:
                F_proxy = 0.0

            lambda1 = 0.4  # peso curiosidade/IG
            lambda2 = 0.3  # peso -F (redu√ß√£o de energia livre)
            predictive_bonus = (lambda1 * IG_hat) + (lambda2 * (0.0 - F_proxy))
            scaled_reward += self.reward_eta * predictive_bonus
        except Exception:
            pass
        updated = 0
        # Aplica em lote apenas nas arestas sujas; decai tra√ßos dos demais sem atualizar peso
        target_edges = []
        if self._dirty_edges:
            for (u0, v0) in list(self._dirty_edges):
                if u0 in self.fragment_graph and v0 in self.fragment_graph[u0]:
                    target_edges.append((u0, v0, self.fragment_graph[u0][v0]))
        else:
            target_edges = list(self.fragment_graph.edges(data=True))
        dirty_set = set(self._dirty_edges)
        for u, v, edge_data in target_edges:
            e = edge_data.get('eligibility', 0.0)
            if e <= 0:
                continue
            # Aplica atualiza√ß√£o limitada por max_reward_delta implicitamente pelo reward recebido
            delta = scaled_reward * e
            new_w = min(self.max_synaptic_weight, max(0.0, edge_data.get('w', 0.0) + delta))
            edge_data['w'] = new_w
            # Refor√ßa consolida√ß√£o se estava tagged
            if edge_data.get('tagged') and edge_data.get('volatile', True):
                edge_data['volatile'] = False
                self.plasticity_stats['synapses_consolidated'] += 1
            # Decai elegibilidade p√≥s uso
            edge_data['eligibility'] = e * self.eligibility_decay
            updated += 1

        if updated > 0:
            print(f"       >> Recompensa aplicada a {updated} sinapses (3-fatores).")

        # Decai silenciosamente elegibilidade de arestas fora do conjunto sujo
        if self._dirty_edges:
            for u, v, edge_data in list(self.fragment_graph.edges(data=True)):
                if (min(u, v), max(u, v)) in dirty_set:
                    continue
                e = edge_data.get('eligibility', 0.0)
                if e > 0:
                    edge_data['eligibility'] = e * self.eligibility_decay

        # Atualiza metaplasticidade por n√≥/aresta em commit de recompensa (estado dos par√¢metros)
        try:
            if self.interaction_count != self._last_meta_update:
                self._last_meta_update = self.interaction_count
                # Heur√≠stica: potenciada ‚Üí aumenta eta e encurta tau; deprimida ‚Üí oposto
                for fid, f in self.fragments.items():
                    state = str(f.get('plasticity_state', 'normal')).lower()
                    if state == 'potentiated':
                        self.node_eta_scale[fid] = float(min(self.node_eta_scale[fid] * 1.5, 3.0))
                        self.node_stdp_tau_scale[fid] = float(max(self.node_stdp_tau_scale[fid] * 0.8, 0.4))
                        self.node_bcm_theta_scale[fid] = float(min(self.node_bcm_theta_scale[fid] * 1.2, 2.0))
                    elif state == 'depressed':
                        self.node_eta_scale[fid] = float(max(self.node_eta_scale[fid] * 0.6, 0.2))
                        self.node_stdp_tau_scale[fid] = float(min(self.node_stdp_tau_scale[fid] * 1.25, 3.0))
                        self.node_bcm_theta_scale[fid] = float(max(self.node_bcm_theta_scale[fid] * 0.8, 0.5))
                    else:
                        # relaxa levemente para 1.0
                        self.node_eta_scale[fid] = float(0.9 * self.node_eta_scale[fid] + 0.1 * 1.0)
                        self.node_stdp_tau_scale[fid] = float(0.9 * self.node_stdp_tau_scale[fid] + 0.1 * 1.0)
                        self.node_bcm_theta_scale[fid] = float(0.9 * self.node_bcm_theta_scale[fid] + 0.1 * 1.0)
        except Exception:
            pass


    def update_synaptic_weight(self, i, j, a_i, a_j, delta_t=1, f_target=None, activation_fn=None):
        # Taxa base de aprendizado (modulada por metaplasticidade por n√≥)
        eta_0 = 0.15
        alpha = 0.1   # Fator de adapta√ß√£o da atividade m√©dia
        k_e = 0.01    # Constante de custo energ√©tico
        lambda_ = 0.05  # Decaimento temporal
        p = 2         # Pondera√ß√£o homeost√°tica
        E_max = 1.0   # Limite m√°ximo de energia
        W_max_total = 3.0  # Limite homeost√°tico total

        # Termo 1: Atividade M√©dia Recente (Metaplasticidade)
        # Calcula a atividade m√©dia recente do neur√¥nio pr√©-sin√°ptico 'i'.
        # Isso modula a taxa de aprendizado (eta_0) com base no hist√≥rico de ativa√ß√£o.
        avg_activity = np.mean(self.node_activities[i]) if self.node_activities[i] else a_i
        self.node_activities[i].append(a_i)

        # Termo 2: Uso da Sinapse
        # Incrementa o contador de uso para a sinapse (i, j).
        usage = self.edge_usage[(i, j)] + 1
        self.edge_usage[(i, j)] = usage

        # Termo 3: Penalidade Energ√©tica
        # Modela o custo energ√©tico do disparo sin√°ptico. Sinapses mais fortes e mais usadas
        # consomem mais energia, reduzindo a plasticidade para conservar recursos.
        energy_cost = k_e * self.fragment_graph[i][j]['w'] * usage * np.exp(-lambda_ * delta_t)
        energy_penalty = 1 - (energy_cost / E_max) if energy_cost < E_max else 0

        # Termo 4: Homeostase Ponderada
        # Garante que a soma total dos pesos sin√°pticos de um neur√¥nio permane√ßa dentro de
        # um limite (W_max_total), prevenindo o crescimento descontrolado dos pesos.
        total_weight = sum(self.fragment_graph[i][k]['w']**p for k in self.fragment_graph[i])
        homeostasis = 1 - (total_weight / W_max_total**p) if total_weight < W_max_total**p else 0

        # Taxa adaptativa no tempo (eta(t) = eta_0 / (1 + kappa * t))
        t_global = max(1, self.interaction_count)
        eta_t = eta_0 / (1.0 + self.kappa_adaptive_lr * t_global)

        # Termo Hebbiano com penalidades (base existente)
        hebbian_term = eta_t * np.exp(-alpha * avg_activity) * a_i * a_j * energy_penalty * homeostasis

        # Escalas metapl√°sticas locais
        eta_scale_i = float(self.node_eta_scale[i])
        tau_scale_j = float(self.node_stdp_tau_scale[j])
        theta_scale_j = float(self.node_bcm_theta_scale[j])

        # STDP: dependente do timing pr√©/p√≥s (peso-dependente)
        stdp_term = 0.0
        if self.enable_stdp:
            try:
                t_pre = self.fragments[i].get('last_activation', self.interaction_count)
                t_post = self.fragments[j].get('last_activation', self.interaction_count)
                dt = t_post - t_pre  # dt>0: pr√© antes de p√≥s (LTP)
                if dt != 0 and abs(dt) <= self.stdp_dt_window:
                    w_curr = self.fragment_graph[i][j]['w']
                    if dt > 0:
                        # LTP diminui conforme w aproxima do m√°ximo (tau ajustado por n√≥)
                        stdp_term = self.stdp_A_plus * np.exp(-dt / max(1e-6, (self.stdp_tau_plus * tau_scale_j))) * (1.0 - w_curr)
                    else:
                        # LTD proporcional ao peso atual (tau ajustado por n√≥)
                        stdp_term = - self.stdp_A_minus * np.exp(dt / max(1e-6, (self.stdp_tau_minus * tau_scale_j))) * (w_curr)
            except Exception:
                stdp_term = 0.0

        # BCM: limiar deslizante no p√≥s-sin√°ptico
        bcm_term = 0.0
        if self.enable_bcm:
            theta_j = self.bcm_theta[j] * theta_scale_j
            bcm_term = self.bcm_eta * a_i * a_j * (a_j - theta_j)

        # Termo de corre√ß√£o de erro (aproxima√ß√£o de gradiente)
        error_term = 0.0
        if f_target is not None:
            # a_j √© considerado "sa√≠da" p√≥s-sin√°ptica; derivada da ativa√ß√£o
            act_fn = (activation_fn or self.default_activation_fn).lower()
            if act_fn == 'sigmoid':
                sigma_prime = a_j * (1.0 - a_j)
            elif act_fn == 'tanh':
                sigma_prime = 1.0 - a_j**2
            elif act_fn in ['relu', 'leaky_relu']:
                sigma_prime = 1.0 if a_j > 0 else (0.01 if act_fn == 'leaky_relu' else 0.0)
            else:
                sigma_prime = 1.0  # fallback conservador

            error_term = self.gamma_error_correction * (a_j - f_target) * sigma_prime * a_i

        # Regulariza√ß√£o de varia√ß√£o (suaviza√ß√£o temporal do peso)
        previous_w = self.fragment_graph[i][j]['w']
        if not hasattr(self, '_prev_weights'):
            self._prev_weights = {}
        prev_key = (i, j)
        previous_w_tminus1 = self._prev_weights.get(prev_key, previous_w)
        regularization = - self.beta_weight_smoothing * (previous_w - previous_w_tminus1)

        # Atualiza limiar BCM (m√©dia m√≥vel) com a_j
        if self.enable_bcm:
            theta_prev = self.bcm_theta[j]
            self.bcm_theta[j] = (1 - self.bcm_tau_inv) * theta_prev + self.bcm_tau_inv * a_j

        # Ganho Bayesiano global E[theta] (m√©dia posterior Beta) para modular plasticidade
        try:
            e_theta = None
            # preferir valor exposto pelo write-back
            e_theta = float(getattr(self, 'posterior_mean', None))
            if e_theta is None or not np.isfinite(e_theta):
                # alternativa: olhar no objeto superior (GenerativeCorticalSystem)
                if hasattr(self, 'owner'):
                    e_theta = float(getattr(self.owner, 'posterior_mean', None))
            if e_theta is None or not np.isfinite(e_theta):
                # fallback: reconstruir a partir dos par√¢metros Beta internos, se existirem
                alpha = float(getattr(self, '_conf_alpha', 0.0))
                beta = float(getattr(self, '_conf_beta', 0.0))
                if (alpha + beta) > 0:
                    e_theta = float(alpha / (alpha + beta))
            if e_theta is None or not np.isfinite(e_theta):
                e_theta = 0.7
        except Exception:
            e_theta = 0.7

        # Combina√ß√£o dos termos com modula√ß√£o por E[theta] e escala de eta por n√≥
        delta_w = (eta_scale_i * e_theta) * (hebbian_term + stdp_term + bcm_term) - error_term + regularization
        new_weight = np.clip(previous_w + delta_w, 0, self.max_synaptic_weight)

        # Aplica e armazena o peso anterior para pr√≥xima itera√ß√£o de regulariza√ß√£o
        self.fragment_graph[i][j]['w'] = new_weight
        self._prev_weights[prev_key] = new_weight

    def _rebuild_faiss_indices(self):
        """Sem FAISS: reconstr√≥i √≠ndice invertido de assinaturas."""
        print("       >> Reconstruindo √≠ndice invertido de assinaturas...")
        self.signature_index = defaultdict(lambda: defaultdict(set))
        for fid, f in self.fragments.items():
            patt = f.get('pattern', set())
            s = f.get('subspace', 0)
            for feat in list(patt):
                self.signature_index[s][int(feat)].add(int(fid))

    def _reindex_dirty_nodes_if_needed(self):
        """Mant√©m √≠ndice invertido consistente para n√≥s sujos (sem FAISS)."""
        if self.interaction_count % self.faiss_reindex_interval != 0:
            return
        if not self._dirty_nodes:
            return
        print("       >> Atualizando √≠ndice invertido (subset sujo)...")
        for fid in list(self._dirty_nodes):
            frag = self.fragments.get(fid)
            if not frag:
                continue
            # Remove entradas antigas e re-registra padr√£o atual
            self._unregister_fragment_pattern(fid)
            self._register_fragment_pattern(fid)

    # --- Neo4j Integration ---
    def load_from_neo4j(self, uri=None, user=None, password=None, database=None, limit=None):
        """Carrega fragmentos, conceitos e sinapses direto do Neo4j para a mem√≥ria local.
        √ötil para usar Neo4j como fonte de verdade e operar em mem√≥ria no loop.
        """
        if GraphDatabase is None:
            print("AVISO: Neo4j n√£o dispon√≠vel (pacote ausente).")
            return False
        uri = uri or os.getenv("NEO4J_URI")
        user = user or os.getenv("NEO4J_USER")
        password = password or os.getenv("NEO4J_PASSWORD")
        if not (uri and user and password):
            print("AVISO: Vari√°veis NEO4J_URI/NEO4J_USER/NEO4J_PASSWORD n√£o definidas.")
            return False

        # Limpa estruturas atuais
        self.fragments.clear()
        self.items_index.clear()
        self.fragment_graph.clear()
        self.concept_graph.clear()
        self.meta_graph.clear()
        self.faiss_indices.clear()
        self.faiss_id_maps.clear()

        driver = GraphDatabase.driver(uri, auth=(user, password))
        try:
            with driver.session(database=database) as session:
                # N√≥s Fragment
                q_nodes = """
                MATCH (f:Fragment)
                RETURN f.id AS id, f.subspace AS subspace, coalesce(f.usage,0.0) AS usage,
                       coalesce(f.label,'') AS label,
                       coalesce(f.plasticity_state,'normal') AS plasticity_state,
                       coalesce(f.centrality,0.0) AS centrality
                """
                if limit:
                    q_nodes += " LIMIT $limit"
                nodes_res = session.run(q_nodes, limit=limit) if limit else session.run(q_nodes)
                for r in nodes_res:
                    fid = int(r["id"]) if r["id"] is not None else None
                    if fid is None:
                        continue
                    self.fragments[fid] = {
                        "subspace": int(r["subspace"]) if r["subspace"] is not None else 0,
                        "vector": np.zeros(self.dim, dtype=np.float32),  # placeholder; vetores n√£o ficam no Neo4j
                        "usage": float(r["usage"]),
                        "level": 0,
                        "cluster_size": 1,
                        "label": str(r["label"]).strip(),
                        "activation_contexts": deque(maxlen=self.context_memory_size),
                        "last_activation": 0,
                        "adaptation_count": 0,
                        "original_vector": np.zeros(self.dim, dtype=np.float32),
                        "specialization_score": 0.0,
                        "activation_frequency": 0.0,
                        "recent_activations": deque(maxlen=self.metaplasticity_window),
                        "plasticity_state": str(r["plasticity_state"]).strip(),
                        "centrality": float(r["centrality"]),
                    }
                    self.fragment_graph.add_node(fid, **self.fragments[fid])

                # Arestas SYNAPSE
                q_edges = """
                MATCH (a:Fragment)-[e:SYNAPSE]->(b:Fragment)
                RETURN a.id AS u, b.id AS v, coalesce(e.w,0.0) AS w,
                       coalesce(e.volatile,false) AS volatile,
                       coalesce(e.usage,0) AS usage
                """
                if limit:
                    q_edges += " LIMIT $limit"
                edges_res = session.run(q_edges, limit=limit) if limit else session.run(q_edges)
                for r in edges_res:
                    u = int(r["u"]) if r["u"] is not None else None
                    v = int(r["v"]) if r["v"] is not None else None
                    if u is None or v is None:
                        continue
                    self.fragment_graph.add_edge(u, v,
                                                 w=float(r["w"]),
                                                 usage_count=int(r["usage"]),
                                                 volatile=bool(r["volatile"]),
                                                 created_at=self.interaction_count,
                                                 last_update=self.interaction_count)

                # Conceitos
                q_concepts = """
                MATCH (c:Concept)
                RETURN c.id AS id, coalesce(c.size,0) AS size
                """
                c_res = session.run(q_concepts)
                for r in c_res:
                    cid = str(r["id"]) if r["id"] is not None else None
                    if cid is None:
                        continue
                    self.concept_graph.add_node(cid, members=[], threshold=0.0, strength=int(r["size"]))

                # Membros (Fragment)-[:MEMBER_OF]->(Concept)
                q_members = """
                MATCH (f:Fragment)-[:MEMBER_OF]->(c:Concept)
                RETURN f.id AS fid, c.id AS cid
                """
                m_res = session.run(q_members)
                for r in m_res:
                    fid = int(r["fid"]) if r["fid"] is not None else None
                    cid = str(r["cid"]) if r["cid"] is not None else None
                    if fid is None or cid is None:
                        continue
                    if fid in self.fragment_graph.nodes:
                        self.fragment_graph.nodes[fid]['concept_id'] = cid
                    self.concept_graph.add_node(cid)

                # Links entre conceitos
                q_cedges = """
                MATCH (a:Concept)-[r:CONCEPT_LINK]->(b:Concept)
                RETURN a.id AS u, b.id AS v, coalesce(r.w,0.0) AS w
                """
                ce_res = session.run(q_cedges)
                for r in ce_res:
                    u = str(r["u"]) if r["u"] is not None else None
                    v = str(r["v"]) if r["v"] is not None else None
                    if not u or not v:
                        continue
                    self.concept_graph.add_edge(u, v, w=float(r["w"]))

            # Atualiza IDs auxiliares
            self.next_frag_id = (max(self.fragments.keys()) + 1) if self.fragments else 0
            # Reconstr√≥i √≠ndice invertido de assinaturas a partir dos padr√µes existentes
            if self.fragments:
                self._rebuild_faiss_indices()

            print(f"Neo4j: grafo carregado. N√≥s: {len(self.fragments)}, Arestas: {len(self.fragment_graph.edges())}")
            return True
        except Exception as e:
            print(f"Neo4j: erro ao carregar grafo: {e}")
            return False
        finally:
            driver.close()

    def push_to_neo4j(self, uri=None, user=None, password=None, batch_size=500):
        """Exporta fragmentos, conceitos e sinapses para um banco Neo4j (opcional)."""
        if GraphDatabase is None:
            print("AVISO: Neo4j n√£o dispon√≠vel (pacote ausente).")
            return False
        uri = uri or os.getenv("NEO4J_URI")
        user = user or os.getenv("NEO4J_USER")
        password = password or os.getenv("NEO4J_PASSWORD")
        if not (uri and user and password):
            print("AVISO: Vari√°veis NEO4J_URI/NEO4J_USER/NEO4J_PASSWORD n√£o definidas.")
            return False

        driver = GraphDatabase.driver(uri, auth=(user, password))
        try:
            with driver.session() as session:
                # √çndices b√°sicos
                session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (f:Fragment) REQUIRE f.id IS UNIQUE")
                session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (c:Concept) REQUIRE c.id IS UNIQUE")

                # Upsert de fragmentos em lotes
                fragments = [
                    {
                        'id': int(fid),
                        'subspace': int(f.get('subspace', -1)),
                        'usage': float(f.get('usage', 0.0)),
                        'label': f.get('label', ''),
                        'plasticity_state': f.get('plasticity_state', 'normal'),
                        'centrality': float(f.get('centrality', 0.0)) if 'centrality' in f else 0.0
                    }
                    for fid, f in self.fragments.items()
                ]

                def chunks(lst, n):
                    for i in range(0, len(lst), n):
                        yield lst[i:i+n]

                for batch in chunks(fragments, batch_size):
                    session.run(
                        """
                        UNWIND $rows AS row
                        MERGE (f:Fragment {id: row.id})
                        SET f.subspace = row.subspace,
                            f.usage = row.usage,
                            f.label = row.label,
                            f.plasticity_state = row.plasticity_state,
                            f.centrality = row.centrality
                        """,
                        rows=batch
                    )

                # Upsert de conceitos
                concept_nodes = []
                for cid, cdata in self.concept_graph.nodes(data=True):
                    concept_nodes.append({'id': str(cid), 'size': int(len(cdata.get('members', [])))})
                for batch in chunks(concept_nodes, batch_size):
                    session.run(
                        """
                        UNWIND $rows AS row
                        MERGE (c:Concept {id: row.id})
                        SET c.size = row.size
                        """,
                        rows=batch
                    )

                # Rela√ß√µes Fragment-Fragment (SINAPSES)
                edges = []
                for u, v, d in self.fragment_graph.edges(data=True):
                    edges.append({
                        'u': int(u), 'v': int(v), 'w': float(d.get('w', 0.0)),
                        'volatile': bool(d.get('volatile', False)),
                        'usage': int(d.get('usage_count', 0))
                    })
                for batch in chunks(edges, batch_size):
                    session.run(
                        """
                        UNWIND $rows AS row
                        MATCH (a:Fragment {id: row.u})
                        MATCH (b:Fragment {id: row.v})
                        MERGE (a)-[r:SYNAPSE]->(b)
                        SET r.w = row.w, r.volatile = row.volatile, r.usage = row.usage
                        """,
                        rows=batch
                    )

                # Rela√ß√µes Concept-Concept
                concept_edges = []
                for u, v, d in self.concept_graph.edges(data=True):
                    concept_edges.append({'u': str(u), 'v': str(v), 'w': float(d.get('w', 0.0))})
                for batch in chunks(concept_edges, batch_size):
                    session.run(
                        """
                        UNWIND $rows AS row
                        MATCH (a:Concept {id: row.u})
                        MATCH (b:Concept {id: row.v})
                        MERGE (a)-[r:CONCEPT_LINK]->(b)
                        SET r.w = row.w
                        """,
                        rows=batch
                    )

                # Rela√ß√µes Fragment-Concept (membresia)
                memberships = []
                for fid, fdata in self.fragment_graph.nodes(data=True):
                    cid = fdata.get('concept_id')
                    if cid:
                        memberships.append({'fid': int(fid), 'cid': str(cid)})
                for batch in chunks(memberships, batch_size):
                    session.run(
                        """
                        UNWIND $rows AS row
                        MATCH (f:Fragment {id: row.fid})
                        MATCH (c:Concept {id: row.cid})
                        MERGE (f)-[:MEMBER_OF]->(c)
                        """,
                        rows=batch
                    )

            print("Neo4j: sincroniza√ß√£o conclu√≠da.")
            return True
        except Exception as e:
            print(f"Neo4j: erro na sincroniza√ß√£o: {e}")
            return False
        finally:
            driver.close()

    # --- Neo4j runtime helpers (usados quando NEO4J_RUNTIME_ONLY=on) ---
    def _neo4j_available(self):
        return (GraphDatabase is not None and all(os.getenv(k) for k in ["NEO4J_URI", "NEO4J_USER", "NEO4J_PASSWORD"]))

    def _neo4j_upsert_fragment(self, fid):
        if not self.use_neo4j_runtime or not self._neo4j_available():
            return
        f = self.fragments.get(fid)
        if not f:
            return
        try:
            uri = os.getenv("NEO4J_URI"); user = os.getenv("NEO4J_USER"); password = os.getenv("NEO4J_PASSWORD")
            driver = GraphDatabase.driver(uri, auth=(user, password))
            with driver.session() as session:
                session.run(
                    """
                    MERGE (r:Fragment {id:$id})
                    SET r.subspace=$subspace, r.usage=$usage, r.label=$label,
                        r.plasticity_state=$plasticity_state, r.centrality=$centrality
                    """,
                    id=int(fid),
                    subspace=int(f.get('subspace', -1)),
                    usage=float(f.get('usage', 0.0)),
                    label=str(f.get('label', '')),
                    plasticity_state=str(f.get('plasticity_state', 'normal')),
                    centrality=float(f.get('centrality', 0.0))
                )
        except Exception:
            pass
        finally:
            try:
                driver.close()
            except Exception:
                pass

    def _neo4j_upsert_edge(self, u, v):
        if not self.use_neo4j_runtime or not self._neo4j_available():
            return
        if u not in self.fragment_graph or v not in self.fragment_graph[u]:
            return
        d = self.fragment_graph[u][v]
        try:
            uri = os.getenv("NEO4J_URI"); user = os.getenv("NEO4J_USER"); password = os.getenv("NEO4J_PASSWORD")
            driver = GraphDatabase.driver(uri, auth=(user, password))
            with driver.session() as session:
                session.run(
                    """
                    MATCH (a:Fragment {id:$u})
                    MATCH (b:Fragment {id:$v})
                    MERGE (a)-[r:SYNAPSE]->(b)
                    SET r.w=$w, r.volatile=$volatile, r.usage=$usage
                    """,
                    u=int(u), v=int(v),
                    w=float(d.get('w', 0.0)),
                    volatile=bool(d.get('volatile', False)),
                    usage=int(d.get('usage_count', 0))
                )
        except Exception:
            pass
        finally:
            try:
                driver.close()
            except Exception:
                pass

    def _neo4j_delete_edge(self, u, v):
        if not self.use_neo4j_runtime or not self._neo4j_available():
            return
        try:
            uri = os.getenv("NEO4J_URI"); user = os.getenv("NEO4J_USER"); password = os.getenv("NEO4J_PASSWORD")
            driver = GraphDatabase.driver(uri, auth=(user, password))
            with driver.session() as session:
                session.run(
                    """
                    MATCH (a:Fragment {id:$u})-[r:SYNAPSE]->(b:Fragment {id:$v})
                    DELETE r
                    """,
                    u=int(u), v=int(v)
                )
        except Exception:
            pass
        finally:
            try:
                driver.close()
            except Exception:
                pass

    def _neo4j_delete_fragment(self, fid):
        if not self.use_neo4j_runtime or not self._neo4j_available():
            return
        try:
            uri = os.getenv("NEO4J_URI"); user = os.getenv("NEO4J_USER"); password = os.getenv("NEO4J_PASSWORD")
            driver = GraphDatabase.driver(uri, auth=(user, password))
            with driver.session() as session:
                session.run(
                    """
                    MATCH (f:Fragment {id:$id})
                    DETACH DELETE f
                    """,
                    id=int(fid)
                )
        except Exception:
            pass
        finally:
            try:
                driver.close()
            except Exception:
                pass

    def _neo4j_set_membership(self, fid, cid):
        if not self.use_neo4j_runtime or not self._neo4j_available():
            return
        try:
            uri = os.getenv("NEO4J_URI"); user = os.getenv("NEO4J_USER"); password = os.getenv("NEO4J_PASSWORD")
            driver = GraphDatabase.driver(uri, auth=(user, password))
            with driver.session() as session:
                session.run(
                    """
                    MATCH (f:Fragment {id:$fid})
                    MERGE (c:Concept {id:$cid})
                    MERGE (f)-[:MEMBER_OF]->(c)
                    """,
                    fid=int(fid), cid=str(cid)
                )
        except Exception:
            pass
        finally:
            try:
                driver.close()
            except Exception:
                pass

    def push_incremental_to_neo4j(self, uri=None, user=None, password=None, batch_size=500):
        """Empurra apenas n√≥s e arestas marcados como sujos para o Neo4j."""
        if GraphDatabase is None:
            print("AVISO: Neo4j n√£o dispon√≠vel (pacote ausente).")
            return False
        uri = uri or os.getenv("NEO4J_URI")
        user = user or os.getenv("NEO4J_USER")
        password = password or os.getenv("NEO4J_PASSWORD")
        if not (uri and user and password):
            print("AVISO: Vari√°veis NEO4J_URI/NEO4J_USER/NEO4J_PASSWORD n√£o definidas.")
            return False

        dirty_nodes = [fid for fid in list(self._dirty_nodes) if fid in self.fragments]
        dirty_edges = []
        for (u0, v0) in list(self._dirty_edges):
            if u0 in self.fragment_graph and v0 in self.fragment_graph[u0]:
                d = self.fragment_graph[u0][v0]
                dirty_edges.append((u0, v0, d))

        if not dirty_nodes and not dirty_edges:
            return True

        driver = GraphDatabase.driver(uri, auth=(user, password))
        try:
            with driver.session() as session:
                session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (f:Fragment) REQUIRE f.id IS UNIQUE")
                session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (c:Concept) REQUIRE c.id IS UNIQUE")

                frag_rows = [
                    {
                        'id': int(fid),
                        'subspace': int(self.fragments[fid].get('subspace', -1)),
                        'usage': float(self.fragments[fid].get('usage', 0.0)),
                        'label': self.fragments[fid].get('label', ''),
                        'plasticity_state': self.fragments[fid].get('plasticity_state', 'normal'),
                        'centrality': float(self.fragments[fid].get('centrality', 0.0)) if 'centrality' in self.fragments[fid] else 0.0
                    }
                    for fid in dirty_nodes
                ]

                def _chunks(lst, n):
                    for i in range(0, len(lst), n):
                        yield lst[i:i+n]

                for batch in _chunks(frag_rows, batch_size):
                    if not batch:
                        continue
                    session.run(
                        """
                        UNWIND $rows AS row
                        MERGE (f:Fragment {id: row.id})
                        SET f.subspace = row.subspace,
                            f.usage = row.usage,
                            f.label = row.label,
                            f.plasticity_state = row.plasticity_state,
                            f.centrality = row.centrality
                        """,
                        rows=batch
                    )

                edge_rows = [
                    {
                        'u': int(u), 'v': int(v), 'w': float(d.get('w', 0.0)),
                        'volatile': bool(d.get('volatile', False)),
                        'usage': int(d.get('usage_count', 0))
                    }
                    for (u, v, d) in dirty_edges
                ]
                for batch in _chunks(edge_rows, batch_size):
                    if not batch:
                        continue
                    session.run(
                        """
                        UNWIND $rows AS row
                        MATCH (a:Fragment {id: row.u})
                        MATCH (b:Fragment {id: row.v})
                        MERGE (a)-[r:SYNAPSE]->(b)
                        SET r.w = row.w, r.volatile = row.volatile, r.usage = row.usage
                        """,
                        rows=batch
                    )

                memberships = []
                for fid in dirty_nodes:
                    fdata = self.fragments.get(fid)
                    if not fdata:
                        continue
                    cid = fdata.get('concept_id')
                    if cid:
                        memberships.append({'fid': int(fid), 'cid': str(cid)})
                for batch in _chunks(memberships, batch_size):
                    if not batch:
                        continue
                    session.run(
                        """
                        UNWIND $rows AS row
                        MATCH (f:Fragment {id: row.fid})
                        MERGE (c:Concept {id: row.cid})
                        MERGE (f)-[:MEMBER_OF]->(c)
                        """,
                        rows=batch
                    )

            return True
        except Exception as e:
            print(f"Neo4j: erro no push incremental: {e}")
            return False
        finally:
            driver.close()

    def add_new_knowledge(self, sentences, metadata_list=None, max_to_add=None, similarity_threshold=0.92):
        """Adiciona novas senten√ßas com deduplica√ß√£o por texto/similaridade e metadados."""
        if not sentences:
            return
        print(f"   >> Adicionando {len(sentences)} novas senten√ßas de conhecimento ao c√≥rtex.")

        # Cap por ciclo
        if isinstance(max_to_add, int) and max_to_add > 0:
            sentences = sentences[:max_to_add]
            if metadata_list:
                metadata_list = metadata_list[:len(sentences)]

        # Embeddings normalizados
        new_emb_matrix = self.model.encode(sentences, show_progress_bar=False)
        new_emb_matrix = normalize(new_emb_matrix, axis=1, norm='l2')

        # Preparar estruturas existentes
        existing_texts = set(v["text"] for v in self.items_index.values())
        existing_vecs = None
        if self.items_index:
            existing_vecs = np.array([v["vec"] for v in self.items_index.values()], dtype=float)

        kept = 0
        skipped_exact = 0
        skipped_sem = 0

        for i, sentence in enumerate(sentences):
            # Deduplica√ß√£o por texto exato
            if sentence in existing_texts:
                skipped_exact += 1
                continue

            # Deduplica√ß√£o por similaridade (se houver base)
            if existing_vecs is not None and len(existing_vecs) > 0:
                vec = new_emb_matrix[i]
                sims = existing_vecs @ vec
                if float(np.max(sims)) >= similarity_threshold:
                    skipped_sem += 1
                    continue

            # Indexar
            new_id = len(self.items_index)
            # Guardrails simples: utilidade m√≠nima e PII b√°sica (muito conservador)
            text_norm = sentence.strip().lower()
            pii_markers = ["cpf", "rg", "cart√£o", "cartao", "senha", "password", "ssn", "credit card"]
            if any(tok in text_norm for tok in pii_markers):
                skipped_exact += 1
                continue

            item = {"id": new_id, "text": sentence, "vec": new_emb_matrix[i]}
            meta = (metadata_list[i] if metadata_list and i < len(metadata_list) else None)
            self._index_item_optimized(item, metadata=meta)
            kept += 1

        print(f"   >> {kept} itens novos indexados ({skipped_exact} duplicados exatos, {skipped_sem} sem√¢nticos).")

    def reinforce_concepts(self, concepts):
        """Executa uma 'consulta de refor√ßo' para fortalecer representa√ß√µes existentes."""
        print(f"   >> Refor√ßando {len(concepts)} conceitos na mem√≥ria.")
        for concept in concepts:
            # Simula uma consulta interna, mas sem gerar sa√≠da, apenas para aplicar plasticidade.
            query_vec = self.model.encode([concept])
            query_vec = normalize(query_vec, axis=1, norm='l2')[0]

            # Ativa fragmentos (sem criar novos din√¢micos no refor√ßo)
            activation_path = defaultdict(lambda: {'type': 'unknown', 'sources': [], 'score': 0.0})
            fragment_acts = self._activate_fragments_with_plasticity(query_vec, activation_path)

            # Aplica aprendizagem Hebbiana para fortalecer as conex√µes co-ativadas
            self._enhanced_hebbian_update_with_saturation(fragment_acts, query_vec)

        print("   >> Refor√ßo conclu√≠do.")

    def process_corpus_optimized(self, sentences):
        """Optimized corpus processing"""
        print("=== SISTEMA CORTICAL COM PLASTICIDADE SIN√ÅPTICA ===")

        # Generate embeddings
        emb_matrix = self.model.encode(sentences, show_progress_bar=True)
        emb_matrix = normalize(emb_matrix, axis=1, norm='l2')

        # Build items
        items = []
        for i, sentence in enumerate(sentences):
            items.append({"id": i, "text": sentence, "vec": emb_matrix[i]})

        # Initialize
        self.initialize_semantic_subspaces(emb_matrix)
        self.extract_initial_fragments_hierarchical(items)
        self.build_faiss_indices()

        # Index items
        for item in items:
            self._index_item_optimized(item)

        return items

    def _index_item_optimized(self, item, metadata=None):
        """Indexa√ß√£o de item usando assinaturas esparsas e √≠ndice invertido (sem FAISS)."""
        links = []
        for s in range(self.n_subspaces):
            z_proj = self.project_to_subspace(item["vec"], s)
            q_pattern, _ = self._vector_to_pattern(z_proj, k=self.pattern_k, with_sign=self.enable_pattern_sign)
            if not q_pattern:
                continue
            # Candidatos por √≠ndice invertido
            candidate_ids = set()
            inv = self.signature_index[s]
            for feat in q_pattern:
                if feat in inv:
                    candidate_ids |= set(inv[int(feat)])
            # Score por overlap e coleta top-3
            scored = []
            for fid in candidate_ids:
                patt = self.fragments.get(fid, {}).get('pattern', set())
                if not patt:
                    continue
                ov = self._pattern_overlap_score(q_pattern, patt, method='jaccard')
                if ov > 0.0:
                    scored.append((fid, float(ov)))
            scored.sort(key=lambda x: -x[1])
            for fid, ov in scored[:3]:
                links.append((fid, float(ov), s))
                        self._dirty_nodes.add(fid)
                self.fragments[fid]['usage'] += 1.0

        self.items_index[item["id"]] = {
            "text": item["text"],
            "links": links,
            "vec": item["vec"],
            "meta": metadata or {}
        }

    def comprehensive_analysis(self):
        """Comprehensive system analysis including synaptic plasticity"""
        analysis = {
            'fragment_stats': self._analyze_fragments(),
            'network_stats': self._analyze_networks(),
            'community_stats': self._analyze_communities(),
            'learning_stats': self._analyze_learning_progress(),
            'plasticity_stats': self._analyze_plasticity(),
            'synaptic_stats': self._analyze_synaptic_plasticity()  # NOVO!
        }
        return analysis

    def _analyze_synaptic_plasticity(self):
        """An√°lise espec√≠fica da plasticidade sin√°ptica"""
        synaptic_network = self._get_synaptic_network_stats()

        # An√°lise de metaplasticidade
        plasticity_states = {'normal': 0, 'potentiated': 0, 'depressed': 0}
        adaptation_rates = []

        for frag in self.fragments.values():
            state = frag.get('plasticity_state', 'normal')
            plasticity_states[state] += 1

            if frag.get('recent_activations'):
                adaptation_rates.append(np.mean(list(frag['recent_activations'])))

        return {
            'total_synapses_created': self.plasticity_stats['synapses_created'],
            'synapses_consolidated': self.plasticity_stats['synapses_consolidated'],
            'synapses_pruned': self.plasticity_stats['synapses_pruned'],
            'volatile_synapses_decayed': self.plasticity_stats['volatile_synapses_decayed'],
            'ltd_applications': self.plasticity_stats['ltd_applications'],
            'consolidation_rate': synaptic_network.get('consolidation_rate', 0.0),
            'avg_synaptic_weight': synaptic_network.get('avg_weight', 0.0),
            'strong_synapses': synaptic_network.get('strong_synapses', 0),
            'metaplasticity_distribution': plasticity_states,
            'avg_adaptation_rate': np.mean(adaptation_rates) if adaptation_rates else 0.0,
            'synaptic_efficiency': (self.plasticity_stats['synapses_consolidated'] /
                                  max(1, self.plasticity_stats['synapses_created']))
        }

    def _analyze_fragments(self):
        """Analyze fragment distribution and usage"""
        if not self.fragments:
            return {}

        usages = [f['usage'] for f in self.fragments.values()]
        subspace_dist = {}
        for f in self.fragments.values():
            s = f['subspace']
            subspace_dist[s] = subspace_dist.get(s, 0) + 1

        # An√°lise de adapta√ß√£o e metaplasticidade
        adaptation_counts = [f.get('adaptation_count', 0) for f in self.fragments.values()]
        specialization_scores = [f.get('specialization_score', 0) for f in self.fragments.values()]

        # NOVO: An√°lise de estados de metaplasticidade
        plasticity_states = {}
        activation_frequencies = []

        for f in self.fragments.values():
            state = f.get('plasticity_state', 'normal')
            plasticity_states[state] = plasticity_states.get(state, 0) + 1

            freq = f.get('activation_frequency', 0.0)
            if freq > 0:
                activation_frequencies.append(freq)

        return {
            'total_fragments': len(self.fragments),
            'usage_stats': {
                'mean': np.mean(usages),
                'std': np.std(usages),
                'max': np.max(usages),
                'low_usage': sum(1 for u in usages if u < 1.0)
            },
            'subspace_distribution': subspace_dist,
            'dynamic_fragments': sum(1 for f in self.fragments.values()
                                   if 'dyn' in f.get('label', '')),
            'adaptation_stats': {
                'mean_adaptations': np.mean(adaptation_counts),
                'max_adaptations': np.max(adaptation_counts) if adaptation_counts else 0,
                'adapted_fragments': sum(1 for c in adaptation_counts if c > 0)
            },
            'specialization_stats': {
                'mean_specialization': np.mean(specialization_scores),
                'high_diversity': sum(1 for s in specialization_scores if s > 0.5)
            },
            'metaplasticity_stats': {
                'states_distribution': plasticity_states,
                'avg_activation_frequency': np.mean(activation_frequencies) if activation_frequencies else 0.0,
                'high_frequency_fragments': sum(1 for f in activation_frequencies if f > 0.5)
            }
        }

    def _analyze_networks(self):
        """Analyze network properties including synaptic details"""
        # An√°lise b√°sica da rede
        basic_stats = {
            'nodes': len(self.fragment_graph.nodes()),
            'edges': len(self.fragment_graph.edges()),
            'avg_clustering': nx.average_clustering(self.fragment_graph) if self.fragment_graph.nodes() else 0,
            'connected_components': nx.number_connected_components(self.fragment_graph)
        }

        # NOVO: An√°lise sin√°ptica detalhada
        if self.fragment_graph.edges():
            edge_data = list(self.fragment_graph.edges(data=True))
            weights = [d.get('w', 0.0) for u, v, d in edge_data]
            volatile_edges = sum(1 for u, v, d in edge_data if d.get('volatile', False))
            usage_counts = [d.get('usage_count', 0) for u, v, d in edge_data]

            synaptic_details = {
                'weight_distribution': {
                    'mean': np.mean(weights),
                    'std': np.std(weights),
                    'max': np.max(weights),
                    'saturated_synapses': sum(1 for w in weights if w >= self.max_synaptic_weight * 0.9)
                },
                'volatility': {
                    'volatile_count': volatile_edges,
                    'consolidated_count': len(edge_data) - volatile_edges,
                    'volatility_ratio': volatile_edges / len(edge_data)
                },
                'usage_patterns': {
                    'mean_usage': np.mean(usage_counts),
                    'max_usage': np.max(usage_counts),
                    'highly_used_synapses': sum(1 for u in usage_counts if u > 5)
                }
            }
            basic_stats.update(synaptic_details)

        return {
            'fragment_network': basic_stats,
            'concept_network': {
                'nodes': len(self.concept_graph.nodes()),
                'edges': len(self.concept_graph.edges())
            }
        }

    def _analyze_communities(self):
        """Analyze detected communities"""
        return {
            'fragment_communities': len([n for n, d in self.concept_graph.nodes(data=True)
                                         if 'members' in d]),
            'avg_community_size': np.mean([len(d['members']) for n, d in self.concept_graph.nodes(data=True)
                                           if 'members' in d]) if self.concept_graph.nodes() else 0
        }

    def _analyze_learning_progress(self):
        """Analyze learning progress over interactions"""
        return {
            'total_interactions': self.interaction_count,
            'queries_processed': len(self.query_history),
            'avg_activations_per_query': np.mean([len(act) for act in self.activation_history])
                                           if self.activation_history else 0
        }

    def _analyze_plasticity(self):
        """Analyze plasticity mechanisms performance"""
        return {
            'vectors_adapted_total': self.plasticity_stats['vectors_adapted'],
            'fragments_specialized': self.plasticity_stats['fragments_specialized'],
            'fragments_fused': self.plasticity_stats['fragments_fused'],
            'decay_cycles': self.plasticity_stats['decay_cycles'],
            'plasticity_rate': (self.plasticity_stats['vectors_adapted'] /
                               max(1, self.interaction_count)),
            'structural_changes': (self.plasticity_stats['fragments_specialized'] +
                                  self.plasticity_stats['fragments_fused']),
            'avg_fragment_age': np.mean([
                self.interaction_count - f.get('last_activation', self.interaction_count)
                for f in self.fragments.values()
            ]) if self.fragments else 0
        }


# Enhanced demo function with synaptic plasticity focus
def demo_enhanced_synaptic_system(num_interactions=None):
    """Demonstra√ß√£o do sistema com plasticidade sin√°ptica aprimorada"""

    # Corpus expandido para melhor demonstra√ß√£o da plasticidade sin√°ptica
    corpus = [
        # Tecnologia e IA
        "Algoritmos de machine learning revolucionam a an√°lise de dados complexos.",
        "Deep learning permite reconhecimento avan√ßado de padr√µes visuais.",
        "Redes neurais artificiais simulam o processamento cerebral humano.",
        "Intelig√™ncia artificial automatiza processos empresariais complexos.",
        "Big data requer ferramentas de processamento distribu√≠do eficiente.",
        "Cloud computing oferece escalabilidade infinita para aplica√ß√µes modernas.",
        "Processamento de linguagem natural facilita intera√ß√£o homem-m√°quina.",

        # Neuroci√™ncia e Plasticidade
        "Plasticidade sin√°ptica permite adapta√ß√£o cont√≠nua do sistema nervoso.",
        "Metaplasticidade regula a capacidade de mudan√ßa das conex√µes neurais.",
        "Potencia√ß√£o a longo prazo fortalece sinapses com uso repetido.",
        "Depress√£o a longo prazo enfraquece conex√µes pouco utilizadas.",
        "Homeostase sin√°ptica mant√©m equil√≠brio da atividade neural.",
        "Neurog√™nese adulta adiciona novos neur√¥nios ao sistema existente.",

        # Natureza e Sustentabilidade
        "Florestas tropicais s√£o pulm√µes vitais do planeta Terra.",
        "Biodiversidade garante equil√≠brio dos ecossistemas naturais complexos.",
        "Mudan√ßas clim√°ticas afetam habitats ao redor do mundo inteiro.",
        "Conserva√ß√£o ambiental protege esp√©cies em extin√ß√£o cr√≠tica.",
        "Energias renov√°veis reduzem pegada de carbono significativamente.",
        "Ecossistemas marinhos enfrentam polui√ß√£o pl√°stica crescente.",
        "Agricultura sustent√°vel preserva solos para futuras gera√ß√µes.",

        # Neg√≥cios e Gest√£o
        "Gest√£o eficaz coordena equipes multidisciplinares produtivas.",
        "Lideran√ßa inspiradora motiva colaboradores criativos e dedicados.",
        "Planejamento estrat√©gico define metas organizacionais claras e mensur√°veis.",
        "Inova√ß√£o empresarial gera vantagem competitiva sustent√°vel e duradoura.",
        "Cultura organizacional influencia performance dos times significativamente.",
        "Transforma√ß√£o digital acelera processos internos das empresas.",
        "Empreendedorismo social combina lucro com impacto positivo.",

        # Sa√∫de e Bem-estar
        "Exerc√≠cios regulares fortalecem sistema cardiovascular humano eficientemente.",
        "Alimenta√ß√£o balanceada fornece nutrientes essenciais ao organismo.",
        "Sono reparador restaura fun√ß√µes cognitivas cerebrais importantes.",
        "Medicina preventiva reduz riscos de doen√ßas cr√¥nicas graves.",
        "Bem-estar mental impacta qualidade de vida profundamente.",
        "Medita√ß√£o mindfulness diminui estresse e ansiedade di√°rios.",
        "Atividade f√≠sica regular previne doen√ßas metab√≥licas comuns.",

        # Educa√ß√£o e Desenvolvimento
        "Pedagogia moderna adapta-se √†s necessidades individuais dos estudantes.",
        "Tecnologia educacional personaliza experi√™ncias de aprendizado √∫nicas.",
        "Metodologias ativas engajam estudantes no processo educativo.",
        "Educa√ß√£o continuada desenvolve compet√™ncias profissionais essenciais.",
        "Interdisciplinaridade conecta diferentes √°reas do conhecimento humano.",
        "Ensino h√≠brido combina presencial e digital eficientemente.",
        "Gamifica√ß√£o torna aprendizagem mais envolvente e divertida.",

        # Frases de conex√£o cross-domain com foco em plasticidade
        "Intelig√™ncia artificial bio-inspirada imita plasticidade neural adaptativa.",
        "Aprendizado cont√≠nuo fortalece conex√µes entre conhecimentos diversos.",
        "Adaptabilidade organizacional espelha flexibilidade sin√°ptica cerebral.",
        "Redes colaborativas emergem atrav√©s de conex√µes repetidas e fortalecidas.",
        "Consolida√ß√£o de mem√≥rias requer repeti√ß√£o e refor√ßo contextual.",
        "Sistemas auto-organizados desenvolvem padr√µes atrav√©s de uso frequente."
    ]

    # Initialize enhanced synaptic system
    system = EnhancedSynapticCorticalSystem(n_subspaces=4, max_hierarchy_levels=3)
    items = system.process_corpus_optimized(corpus)

    # Extended query sequence to specifically test synaptic plasticity
    queries = [
        # Fase 1: Estabelecer conex√µes iniciais
        "Como redes neurais artificiais simulam plasticidade cerebral?",
        "Que papel tem a metaplasticidade no aprendizado adaptativo?",
        "Gest√£o de equipes requer adaptabilidade constante e flexibilidade.",

        # Fase 2: Refor√ßar conex√µes (teste de consolida√ß√£o sin√°ptica)
        "Plasticidade sin√°ptica permite adapta√ß√£o cont√≠nua do sistema nervoso.",
        "Como algoritmos de machine learning imitam processos neurais?",
        "Lideran√ßa adaptativa espelha flexibilidade das conex√µes cerebrais.",

        # Fase 3: Diversificar ativa√ß√£o (teste de volatilidade)
        "Sustentabilidade ambiental requer inova√ß√£o tecnol√≥gica constante.",
        "Educa√ß√£o personalizada utiliza intelig√™ncia artificial avan√ßada.",
        "Bem-estar mental impacta performance organizacional significativamente.",

        # Fase 4: Teste de satura√ß√£o e LTD
        "Redes neurais artificiais simulam processamento cerebral humano.",
        "Metaplasticidade regula capacidade de mudan√ßa das conex√µes neurais.",
        "Sistemas auto-organizados desenvolvem padr√µes atrav√©s de uso frequente.",

        # Fase 5: Consolida√ß√£o e especializa√ß√£o
        "Aprendizado cont√≠nuo fortalece conex√µes entre conhecimentos diversos.",
        "Intelig√™ncia artificial bio-inspirada imita plasticidade neural adaptativa.",
        "Adaptabilidade organizacional espelha flexibilidade sin√°ptica cerebral.",

        # Fase 6: Teste de conex√µes cross-domain complexas
        "Como metodologias √°geis aplicam princ√≠pios de plasticidade neural?",
        "Gamifica√ß√£o educacional utiliza refor√ßo similar √† potencia√ß√£o sin√°ptica.",
        "Transforma√ß√£o digital requer adaptabilidade inspirada em neuroplasticidade."
    ]

    # Ajuste do n√∫mero de intera√ß√µes, se solicitado
    if num_interactions is not None and isinstance(num_interactions, int) and num_interactions > 0:
        base = list(queries)
        if num_interactions <= len(base):
            queries = base[:num_interactions]
        else:
            reps = (num_interactions + len(base) - 1) // len(base)
            queries = (base * reps)[:num_interactions]

    print("=== DEMONSTRA√á√ÉO SISTEMA COM PLASTICIDADE SIN√ÅPTICA ===")
    results = []
    plasticity_timeline = []
    synaptic_timeline = []

    for i, query in enumerate(queries):
        print(f"\n[{i+1}/{len(queries)}]", "="*60)
        result = system.enhanced_query_processing(query)
        results.append(result)

        # Coletar estat√≠sticas de plasticidade sin√°ptica
        synaptic_stats = result['synaptic_network']
        plasticity_stats = result['system_changes']

        timeline_entry = {
            'interaction': i + 1,
            'query': query[:40] + "...",
            'fragments_active': result['active_fragments'],
            'plasticity_applied': result['plasticity_applied'],
            'vectors_adapted': plasticity_stats['vectors_adapted'],
            'fragments_specialized': plasticity_stats['fragments_specialized'],
            'fragments_fused': plasticity_stats['fragments_fused'],
            'total_fragments': len(system.fragments)
        }
        plasticity_timeline.append(timeline_entry)

        synaptic_entry = {
            'interaction': i + 1,
            'total_synapses': synaptic_stats.get('total_synapses', 0),
            'volatile_synapses': synaptic_stats.get('volatile_synapses', 0),
            'consolidated_synapses': synaptic_stats.get('consolidated_synapses', 0),
            'avg_weight': synaptic_stats.get('avg_weight', 0.0),
            'strong_synapses': synaptic_stats.get('strong_synapses', 0),
            'consolidation_rate': synaptic_stats.get('consolidation_rate', 0.0),
            'synapses_created': plasticity_stats.get('synapses_created', 0),
            'synapses_consolidated': plasticity_stats.get('synapses_consolidated', 0),
            'synapses_pruned': plasticity_stats.get('synapses_pruned', 0)
        }
        synaptic_timeline.append(synaptic_entry)

        print(f"Fragmentos ativados: {result['active_fragments']}")
        print(f"Plasticidade aplicada: {result['plasticity_applied']} fragmentos")
        print(f"Rede sin√°ptica: {synaptic_stats.get('total_synapses', 0)} sinapses "
              f"({synaptic_stats.get('consolidated_synapses', 0)} consolidadas)")
        print(f"Mudan√ßas estruturais: Esp={plasticity_stats['fragments_specialized']}, "
              f"Fus√£o={plasticity_stats['fragments_fused']}")
        print("Top resultados:")
        for j, (iid, text, score, explanation) in enumerate(result['top_items'][:3], 1):
            print(f"  {j}. [{score:.3f}] {text}")
            print(f"     Explica√ß√£o: {explanation}")

    # Final comprehensive analysis
    print("\n" + "="*80)
    print("AN√ÅLISE FINAL - SISTEMA COM PLASTICIDADE SIN√ÅPTICA")
    print("="*80)

    analysis = system.comprehensive_analysis()

    for category, stats in analysis.items():
        print(f"\n{category.upper().replace('_', ' ')}:")
        if isinstance(stats, dict):
            for key, value in stats.items():
                if isinstance(value, dict):
                    for subkey, subvalue in value.items():
                        if isinstance(subvalue, float):
                            print(f"    {subkey}: {subvalue:.3f}")
                        else:
                            print(f"    {subkey}: {subvalue}")
                else:
                    if isinstance(value, float):
                        print(f"    {key}: {value:.3f}")
                    else:
                        print(f"    {key}: {value}")

    # Enhanced visualization with synaptic plasticity focus
    print(f"\nGerando visualiza√ß√£o avan√ßada com foco em plasticidade sin√°ptica...")
    fig, axes = plt.subplots(3, 3, figsize=(20, 16))
    fig.suptitle("Sistema Cortical com Plasticidade Sin√°ptica - An√°lise Completa",
                 fontsize=16, fontweight='bold')

    # 1. Fragment network with synaptic properties
    _plot_synaptic_network(system.fragment_graph, axes[0, 0],
                          "Rede de Fragmentos (Propriedades Sin√°pticas)",
                          system.fragments, min_weight=0.1)

    # 2. Concept network
    _plot_network_enhanced(system.concept_graph, axes[0, 1], "Rede de Conceitos Emergentes")

    # 3. Synaptic evolution over time
    interactions = [st['interaction'] for st in synaptic_timeline]
    total_synapses = [st['total_synapses'] for st in synaptic_timeline]
    volatile_synapses = [st['volatile_synapses'] for st in synaptic_timeline]
    consolidated_synapses = [st['consolidated_synapses'] for st in synaptic_timeline]

    axes[0, 2].plot(interactions, total_synapses, 'b-o', linewidth=2, label='Total')
    axes[0, 2].plot(interactions, volatile_synapses, 'r--s', linewidth=2, label='Vol√°teis')
    axes[0, 2].plot(interactions, consolidated_synapses, 'g-^', linewidth=2, label='Consolidadas')
    axes[0, 2].set_title('Evolu√ß√£o das Sinapses')
    axes[0, 2].set_xlabel('Intera√ß√£o')
    axes[0, 2].set_ylabel('N√∫mero de Sinapses')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)

    # 4. Synaptic weight distribution with volatility
    if system.fragment_graph.edges():
        edge_data = list(system.fragment_graph.edges(data=True))
        weights = [d.get('w', 0.0) for u, v, d in edge_data]
        volatile_weights = [d.get('w', 0.0) for u, v, d in edge_data if d.get('volatile', False)]
        consolidated_weights = [d.get('w', 0.0) for u, v, d in edge_data if not d.get('volatile', False)]

        bins = np.linspace(0, 1, 15)
        axes[1, 0].hist([volatile_weights, consolidated_weights], bins=bins, alpha=0.7,
                       color=['red', 'green'], label=['Vol√°teis', 'Consolidadas'])
        axes[1, 0].axvline(system.max_synaptic_weight, color='black', linestyle='--',
                          label=f'Satura√ß√£o ({system.max_synaptic_weight})')
        axes[1, 0].set_title('Distribui√ß√£o de Pesos Sin√°pticos')
        axes[1, 0].set_xlabel('Peso Sin√°ptico')
        axes[1, 0].set_ylabel('Frequ√™ncia')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)

    # 5. Consolidation rate over time
    consolidation_rates = [st['consolidation_rate'] for st in synaptic_timeline]
    axes[1, 1].plot(interactions, consolidation_rates, 'purple', linewidth=2, marker='o')
    axes[1, 1].set_title('Taxa de Consolida√ß√£o Sin√°ptica')
    axes[1, 1].set_xlabel('Intera√ß√£o')
    axes[1, 1].set_ylabel('Taxa de Consolida√ß√£o')
    axes[1, 1].grid(True, alpha=0.3)
    axes[1, 1].set_ylim(0, 1)

    # 6. Metaplasticity states
    if 'metaplasticity_stats' in analysis['fragment_stats']:
        states = analysis['fragment_stats']['metaplasticity_stats']['states_distribution']
        state_names = list(states.keys())
        state_counts = list(states.values())

        colors = {'normal': 'skyblue', 'potentiated': 'green', 'depressed': 'red'}
        bar_colors = [colors.get(state, 'gray') for state in state_names]

        axes[1, 2].bar(state_names, state_counts, color=bar_colors, alpha=0.7)
        axes[1, 2].set_title('Estados de Metaplasticidade')
        axes[1, 2].set_ylabel('N√∫mero de Fragmentos')
        axes[1, 2].grid(True, alpha=0.3, axis='y')

    # 7. Usage vs Adaptation with metaplasticity
    if system.fragments:
        usages = []
        adaptations = []
        plasticity_colors = []

        color_map = {'normal': 'blue', 'potentiated': 'green', 'depressed': 'red'}

        for frag in system.fragments.values():
            usages.append(frag['usage'])
            adaptations.append(frag.get('adaptation_count', 0))
            state = frag.get('plasticity_state', 'normal')
            plasticity_colors.append(color_map.get(state, 'gray'))

        scatter = axes[2, 0].scatter(usages, adaptations, c=plasticity_colors, alpha=0.7, s=60)
        axes[2, 0].set_xlabel('Uso do Fragmento')
        axes[2, 0].set_ylabel('Contagem de Adapta√ß√µes')
        axes[2, 0].set_title('Uso vs Adapta√ß√£o (por Estado Metapl√°stico)')
        axes[2, 0].grid(True, alpha=0.3)

        # Legend manual para cores de estado
        from matplotlib.patches import Patch
        legend_elements = [Patch(facecolor=color, label=state.capitalize())
                          for state, color in color_map.items()]
        axes[2, 0].legend(handles=legend_elements, loc='upper right')

    # 8. Synaptic strength evolution
    avg_weights = [st['avg_weight'] for st in synaptic_timeline]
    strong_synapses = [st['strong_synapses'] for st in synaptic_timeline]

    ax1 = axes[2, 1]
    ax2 = ax1.twinx()

    line1 = ax1.plot(interactions, avg_weights, 'b-o', linewidth=2, label='Peso M√©dio')
    line2 = ax2.plot(interactions, strong_synapses, 'r-s', linewidth=2, label='Sinapses Fortes')

    ax1.set_xlabel('Intera√ß√£o')
    ax1.set_ylabel('Peso Sin√°ptico M√©dio', color='b')
    ax2.set_ylabel('Sinapses Fortes (>0.5)', color='r')
    ax1.set_title('Evolu√ß√£o da For√ßa Sin√°ptica')
    ax1.grid(True, alpha=0.3)

    # Combine legends
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax1.legend(lines, labels, loc='center left')

    # 9. Synaptic operations summary
    operations_data = {
        'Criadas': system.plasticity_stats['synapses_created'],
        'Consolidadas': system.plasticity_stats['synapses_consolidated'],
        'Podadas': system.plasticity_stats['synapses_pruned'],
        'Vol√°teis Deca√≠das': system.plasticity_stats['volatile_synapses_decayed'],
        'LTD Aplicada': system.plasticity_stats['ltd_applications']
    }

    axes[2, 2].bar(range(len(operations_data)), list(operations_data.values()),
                   color=['green', 'blue', 'red', 'orange', 'purple'], alpha=0.7)
    axes[2, 2].set_xticks(range(len(operations_data)))
    axes[2, 2].set_xticklabels(list(operations_data.keys()), rotation=45, ha='right')
    axes[2, 2].set_title('Opera√ß√µes Sin√°pticas Totais')
    axes[2, 2].set_ylabel('Quantidade')
    axes[2, 2].grid(True, alpha=0.3, axis='y')

    plt.tight_layout()

    # Detailed synaptic plasticity analysis
    print(f"\n" + "="*60)
    print("AN√ÅLISE DETALHADA DA PLASTICIDADE SIN√ÅPTICA")
    print("="*60)

    synaptic_stats = analysis['synaptic_stats']
    fragment_stats = analysis['fragment_stats']
    network_stats = analysis['network_stats']

    print(f"\nüîπ OPERA√á√ïES SIN√ÅPTICAS:")
    print(f"   Sinapses criadas: {synaptic_stats['total_synapses_created']}")
    print(f"   Sinapses consolidadas: {synaptic_stats['synapses_consolidated']}")
    print(f"   Sinapses podadas: {synaptic_stats['synapses_pruned']}")
    print(f"   Sinapses vol√°teis deca√≠das: {synaptic_stats['volatile_synapses_decayed']}")
    print(f"   Aplica√ß√µes de LTD: {synaptic_stats['ltd_applications']}")

    print(f"\nüîπ PROPRIEDADES DA REDE SIN√ÅPTICA:")
    print(f"   Taxa de consolida√ß√£o: {synaptic_stats['consolidation_rate']:.3f}")
    print(f"   Peso sin√°ptico m√©dio: {synaptic_stats['avg_synaptic_weight']:.3f}")
    print(f"   Sinapses fortes (>0.5): {synaptic_stats['strong_synapses']}")
    print(f"   Efici√™ncia sin√°ptica: {synaptic_stats['synaptic_efficiency']:.3f}")

    # Network analysis with synaptic details
    if 'weight_distribution' in network_stats['fragment_network']:
        weight_stats = network_stats['fragment_network']['weight_distribution']
        volatility_stats = network_stats['fragment_network']['volatility']

        print(f"\nüîπ DISTRIBUI√á√ÉO DE PESOS:")
        print(f"   Peso m√©dio: {weight_stats['mean']:.3f}")
        print(f"   Desvio padr√£o: {weight_stats['std']:.3f}")
        print(f"   Peso m√°ximo: {weight_stats['max']:.3f}")
        print(f"   Sinapses saturadas: {weight_stats['saturated_synapses']}")

        print(f"\nüîπ VOLATILIDADE SIN√ÅPTICA:")
        print(f"   Sinapses vol√°teis: {volatility_stats['volatile_count']}")
        print(f"   Sinapses consolidadas: {volatility_stats['consolidated_count']}")
        print(f"   Raz√£o de volatilidade: {volatility_stats['volatility_ratio']:.3f}")

    # Metaplasticity analysis
    if 'metaplasticity_stats' in fragment_stats:
        meta_stats = fragment_stats['metaplasticity_stats']

        print(f"\nüîπ METAPLASTICIDADE:")
        print(f"   Estados de plasticidade:")
        for state, count in meta_stats['states_distribution'].items():
            print(f"     {state.capitalize()}: {count}")
        print(f"   Frequ√™ncia m√©dia de ativa√ß√£o: {meta_stats['avg_activation_frequency']:.3f}")
        print(f"   Fragmentos alta frequ√™ncia: {meta_stats['high_frequency_fragments']}")

    # Synaptic efficiency analysis
    print(f"\nüîπ EFICI√äNCIA DO SISTEMA:")

    total_interactions = len(queries)
    synaptic_efficiency = (synaptic_stats['synapses_consolidated'] /
                          max(1, synaptic_stats['total_synapses_created']))
    plasticity_efficiency = (synaptic_stats['avg_adaptation_rate'])

    print(f"   Efici√™ncia de consolida√ß√£o: {synaptic_efficiency:.3f}")
    print(f"   Taxa m√©dia de adapta√ß√£o: {plasticity_efficiency:.3f}")
    print(f"   Opera√ß√µes sin√°pticas/intera√ß√£o: {synaptic_stats['total_synapses_created']/total_interactions:.1f}")

    # Learning progression analysis
    if len(synaptic_timeline) > 6:
        initial_consolidation = np.mean([st['consolidation_rate'] for st in synaptic_timeline[:3]])
        final_consolidation = np.mean([st['consolidation_rate'] for st in synaptic_timeline[-3:]])

        initial_avg_weight = np.mean([st['avg_weight'] for st in synaptic_timeline[:3]])
        final_avg_weight = np.mean([st['avg_weight'] for st in synaptic_timeline[-3:]])

        print(f"\nüîπ PROGRESS√ÉO DO APRENDIZADO:")
        print(f"   Consolida√ß√£o inicial: {initial_consolidation:.3f}")
        print(f"   Consolida√ß√£o final: {final_consolidation:.3f}")
        print(f"   Melhoria na consolida√ß√£o: {((final_consolidation - initial_consolidation) / max(initial_consolidation, 0.001) * 100):+.1f}%")
        print(f"   Peso inicial m√©dio: {initial_avg_weight:.3f}")
        print(f"   Peso final m√©dio: {final_avg_weight:.3f}")
        print(f"   Fortalecimento m√©dio: {((final_avg_weight - initial_avg_weight) / max(initial_avg_weight, 0.001) * 100):+.1f}%")

    print(f"\nüîπ CAPACIDADES DEMONSTRADAS:")
    print(f"   ‚úì Satura√ß√£o sin√°ptica biol√≥gica (max: {system.max_synaptic_weight})")
    print(f"   ‚úì Sinapses vol√°teis com consolida√ß√£o por uso")
    print(f"   ‚úì Depress√£o a longo prazo (LTD) para conex√µes fracas")
    print(f"   ‚úì Metaplasticidade com {len(meta_stats['states_distribution'])} estados")
    print(f"   ‚úì Homeostase atrav√©s de decay diferenciado")
    print(f"   ‚úì Auto-organiza√ß√£o com {synaptic_stats['total_synapses_created']} sinapses criadas")
    print(f"   ‚úì Efici√™ncia adaptativa ({synaptic_efficiency:.1%} consolida√ß√£o)")

    print(f"\nüéØ CONCLUS√ÉO:")
    print(f"O sistema demonstrou plasticidade sin√°ptica biol√≥gicamente inspirada,")
    print(f"incluindo satura√ß√£o de pesos, consolida√ß√£o dependente de uso,")
    print(f"depress√£o a longo prazo e metaplasticidade adaptativa.")
    print(f"Taxa de consolida√ß√£o de {synaptic_efficiency:.1%} indica aprendizado eficiente.")
    print(f"\nSISTEMA COM PLASTICIDADE SIN√ÅPTICA IMPLEMENTADO COM SUCESSO!")
    print(f"{'='*80}")

    return system, results, fig, analysis, plasticity_timeline, synaptic_timeline


def _plot_synaptic_network(graph, ax, title, fragments=None, min_weight=0.05):
    """Enhanced helper function to plot networks with synaptic properties"""
    if len(graph.nodes()) == 0:
        ax.text(0.5, 0.5, 'Grafo Vazio', ha='center', va='center', transform=ax.transAxes)
        ax.set_title(title)
        ax.axis('off')
        return

    # Filter edges by weight
    strong_edges = [(u, v) for u, v, d in graph.edges(data=True)
                    if d.get('w', 0) > min_weight]

    if not strong_edges:
        ax.text(0.5, 0.5, f'Sem Conex√µes > {min_weight}',
                ha='center', va='center', transform=ax.transAxes)
        ax.set_title(title)
        ax.axis('off')
        return

    # Create subgraph with strong edges
    subgraph = graph.edge_subgraph(strong_edges)

    # Layout
    if len(subgraph.nodes()) > 1:
        pos = nx.spring_layout(subgraph, seed=42, k=2, iterations=50)
    else:
        pos = {list(subgraph.nodes())[0]: (0, 0)}

    # Enhanced node properties with metaplasticity info
    if fragments:
        node_colors = []
        node_sizes = []

        for node in subgraph.nodes():
            if node in fragments:
                f = fragments[node]

                # MODIFICADO: Colorir por subespa√ßo para visualizar clusters conceituais
                node_colors.append(f.get('subspace', -1))

                # Size by usage with adaptation bonus
                base_size = 50 + f.get('usage', 1) * 15
                if f.get('adaptation_count', 0) > 0:
                    base_size += f['adaptation_count'] * 5
                node_sizes.append(base_size)

            else:
                node_colors.append(-1)  # Concept nodes
                node_sizes.append(200)
    else:
        node_colors = 'lightblue'
        node_sizes = [100] * len(subgraph.nodes())

    # Draw network with enhanced synaptic visualization
    nx.draw_networkx_nodes(subgraph, pos, node_color=node_colors,
                           node_size=node_sizes, alpha=0.8, ax=ax,
                           cmap=plt.cm.Set1, edgecolors='black', linewidths=0.5)

    # Enhanced edge visualization with synaptic properties
    volatile_edges = []
    consolidated_edges = []
    volatile_weights = []
    consolidated_weights = []

    for u, v, d in subgraph.edges(data=True):
        weight = d.get('w', 0)
        is_volatile = d.get('volatile', False)

        if is_volatile:
            volatile_edges.append((u, v))
            volatile_weights.append(weight * 8)
        else:
            consolidated_edges.append((u, v))
            consolidated_weights.append(weight * 8)

    # Draw volatile synapses (dashed, red)
    if volatile_edges:
        nx.draw_networkx_edges(subgraph, pos, edgelist=volatile_edges,
                              width=volatile_weights, edge_color='red',
                              alpha=0.6, style='dashed', ax=ax)

    # Draw consolidated synapses (solid, blue)
    if consolidated_edges:
        nx.draw_networkx_edges(subgraph, pos, edgelist=consolidated_edges,
                              width=consolidated_weights, edge_color='blue',
                              alpha=0.8, style='solid', ax=ax)

    # Enhanced labels for small networks
    if len(subgraph.nodes()) <= 12:
        labels = {}
        for node in subgraph.nodes():
            if fragments and node in fragments:
                f = fragments[node]
                subspace = f.get('subspace', '?')
                adaptations = f.get('adaptation_count', 0)
                state = f.get('plasticity_state', 'normal')

                state_symbol = {'normal': '', 'potentiated': '+', 'depressed': '-'}

                if adaptations > 0:
                    labels[node] = f"s{subspace}{state_symbol.get(state, '')}{adaptations}"
                else:
                    labels[node] = f"s{subspace}{state_symbol.get(state, '')}"
            else:
                labels[node] = str(node)[:6]

        nx.draw_networkx_labels(subgraph, pos, labels, font_size=6, ax=ax,
                               bbox=dict(boxstyle="round,pad=0.1", facecolor="white", alpha=0.8))

    # Enhanced title with synaptic statistics
    volatile_count = len(volatile_edges)
    consolidated_count = len(consolidated_edges)

    title_enhanced = (f"{title}\n"
                     f"({len(subgraph.nodes())} n√≥s, {len(subgraph.edges())} sinapses)\n"
                     f"Vol√°teis: {volatile_count}, Consolidadas: {consolidated_count}")
    ax.set_title(title_enhanced, fontsize=9)
    ax.axis('off')


def _plot_network_enhanced(graph, ax, title, fragments=None, min_weight=0.05):
    """Enhanced helper function to plot network graphs with plasticity info"""
    if len(graph.nodes()) == 0:
        ax.text(0.5, 0.5, 'Grafo Vazio', ha='center', va='center', transform=ax.transAxes)
        ax.set_title(title)
        ax.axis('off')
        return

    # Filter edges by weight
    strong_edges = [(u, v) for u, v, d in graph.edges(data=True)
                    if d.get('w', 0) > min_weight]

    if not strong_edges:
        ax.text(0.5, 0.5, f'Sem Conex√µes > {min_weight}',
                ha='center', va='center', transform=ax.transAxes)
        ax.set_title(title)
        ax.axis('off')
        return

    # Create subgraph with strong edges
    subgraph = graph.edge_subgraph(strong_edges)

    # Layout
    if len(subgraph.nodes()) > 1:
        pos = nx.spring_layout(subgraph, seed=42, k=2, iterations=50)
    else:
        pos = {list(subgraph.nodes())[0]: (0, 0)}

    # Enhanced node properties with plasticity info
    if fragments:
        node_colors = []
        node_sizes = []

        for node in subgraph.nodes():
            if node in fragments:
                f = fragments[node]

                # Color by subspace with plasticity modifier
                base_color = f.get('subspace', 0)
                if f.get('adaptation_count', 0) > 0:
                    base_color += 0.3  # Shift color for adapted fragments
                node_colors.append(base_color)

                # Size by usage with adaptation bonus
                base_size = 50 + f.get('usage', 1) * 15
                if f.get('adaptation_count', 0) > 0:
                    base_size += f['adaptation_count'] * 5
                node_sizes.append(base_size)

            else:
                node_colors.append(-1)  # Concept nodes
                node_sizes.append(200)
    else:
        node_colors = 'lightblue'
        node_sizes = [100] * len(subgraph.nodes())

    # Draw network with enhanced styling
    nx.draw_networkx_nodes(subgraph, pos, node_color=node_colors,
                           node_size=node_sizes, alpha=0.8, ax=ax,
                           cmap=plt.cm.Set1, edgecolors='black', linewidths=0.5)

    # Enhanced edge visualization
    edge_weights = []
    edge_colors = []
    for u, v, d in subgraph.edges(data=True):
        weight = d.get('w', 0)
        edge_weights.append(weight * 8)

        # Color edges by strength
        if weight > 0.5:
            edge_colors.append('red')
        elif weight > 0.3:
            edge_colors.append('orange')
        else:
            edge_colors.append('gray')

    nx.draw_networkx_edges(subgraph, pos, width=edge_weights,
                          edge_color=edge_colors, alpha=0.6, ax=ax)

    # Enhanced labels for small networks
    if len(subgraph.nodes()) <= 15:
        labels = {}
        for node in subgraph.nodes():
            if fragments and node in fragments:
                f = fragments[node]
                subspace = f.get('subspace', '?')
                adaptations = f.get('adaptation_count', 0)

                if adaptations > 0:
                    labels[node] = f"s{subspace}*{adaptations}"
                else:
                    labels[node] = f"s{subspace}"
            else:
                labels[node] = str(node)[:8]

        nx.draw_networkx_labels(subgraph, pos, labels, font_size=7, ax=ax,
                               bbox=dict(boxstyle="round,pad=0.1", facecolor="white", alpha=0.7))

    # Enhanced title with statistics
    adapted_nodes = 0
    if fragments:
        adapted_nodes = sum(1 for n in subgraph.nodes()
                          if n in fragments and fragments[n].get('adaptation_count', 0) > 0)

    title_enhanced = f"{title}\n({len(subgraph.nodes())} n√≥s, {len(subgraph.edges())} arestas, {adapted_nodes} adaptados)"
    ax.set_title(title_enhanced, fontsize=10)
    ax.axis('off')


class ChatInterface:
    """
    Uma interface de linha de comando simples para interagir com o GenerativeCorticalSystem.
    """
    def __init__(self, generative_system):
        self.generative_system = generative_system
        print("\n" + "="*80)
        print("Interface de Chat Interativo iniciada.")
        print("Digite sua pergunta e pressione Enter.")
        print("Para sair, digite 'sair', 'exit' ou 'quit'.")
        print("="*80)

    def start_chat(self):
        """Inicia o loop do chat."""
        while True:
            try:
                user_input = input("\nVoc√™: ")
                if user_input.lower() in ["sair", "exit", "quit"]:
                    print("\nEncerrando o chat. At√© logo!")
                    break

                if not user_input.strip():
                    continue

                print("Sistema: Pensando...")

                # Processa a consulta atrav√©s do sistema generativo
                answer, _ = self.generative_system.process_complex_query(user_input)

                print("\nSistema:")
                print(answer)

            except KeyboardInterrupt:
                print("\nEncerrando o chat. At√© logo!")
                break
            except Exception as e:
                print(f"\nOcorreu um erro inesperado: {e}")


if __name__ == "__main__":
    # Parse simples de argumento --interactions=N
    import sys
    interactions = None
    neo4j_mode = os.getenv("NEO4J_AUTO_PUSH", "on").lower()  # on|off|auto
    for arg in sys.argv[1:]:
        if arg.startswith("--interactions="):
            try:
                interactions = int(arg.split("=", 1)[1])
            except Exception:
                interactions = None
        elif arg.startswith("--neo4j="):
            try:
                neo4j_mode = arg.split("=", 1)[1].strip().lower()
            except Exception:
                pass

    # Suporte: carregar somente do Neo4j (sem processar corpus/queries)
    neo4j_load_only = os.getenv("NEO4J_LOAD_ONLY", "off").lower() in ("on", "true", "1")
    if neo4j_load_only:
        print("\nCarregando grafo diretamente do Neo4j (NEO4J_LOAD_ONLY=on)...")
        system = EnhancedSynapticCorticalSystem(n_subspaces=4, max_hierarchy_levels=3)
        ok = system.load_from_neo4j()
        if not ok:
            print("AVISO: Falha ao carregar do Neo4j; executando demonstra√ß√£o padr√£o.")
            system, results, visualization, analysis, plasticity_timeline, synaptic_timeline = demo_enhanced_synaptic_system(interactions)
        else:
            results = []
            analysis = system.comprehensive_analysis()
            plasticity_timeline = []
            synaptic_timeline = []
            # figura vazia apenas para manter compatibilidade com plt.show()
            visualization = plt.figure(figsize=(6, 4))
    else:
        # Executa a demonstra√ß√£o com n√∫mero de intera√ß√µes configur√°vel
        system, results, visualization, analysis, plasticity_timeline, synaptic_timeline = demo_enhanced_synaptic_system(interactions)

    # Sincroniza√ß√£o opcional com Neo4j (nativa)
    def _should_push_neo4j():
        if neo4j_mode == "off":
            return False
        if neo4j_mode == "on":
            return True
        # auto: s√≥ empurra se vari√°veis estiverem presentes
        return all(os.getenv(k) for k in ["NEO4J_URI", "NEO4J_USER", "NEO4J_PASSWORD"]) and GraphDatabase is not None

    if _should_push_neo4j():
        print("\nIniciando sincroniza√ß√£o com Neo4j (modo:", neo4j_mode, ")...")
        pushed = system.push_to_neo4j()
        print("Sincroniza√ß√£o Neo4j:", "SUCESSO" if pushed else "FALHA")
    plt.show()
    print(f"Peso m√©dio ap√≥s atualiza√ß√£o: {np.mean([d['w'] for u, v, d in system.fragment_graph.edges(data=True)]):.3f}")

    # Agora, inicializa o sistema generativo h√≠brido com o c√≥rtex j√° treinado
    print("\n\n" + "="*80)
    print("INICIALIZANDO O SISTEMA GENERATIVO H√çBRIDO (CORTICAL + GEMINI)")
    print("="*80)

    try:
        generative_system = GenerativeCorticalSystem(system)

        # Executa uma consulta complexa que requer racioc√≠nio e s√≠ntese
        complex_query = "Considerando todo o conhecimento processado, como a metaplasticidade em sistemas de IA pode ser diretamente comparada √† necessidade de inova√ß√£o cont√≠nua nas organiza√ß√µes empresariais?"

        final_answer, reflection_data = generative_system.process_complex_query(complex_query)

        print("\n\n" + "="*80)
        print("RESPOSTA FINAL DO SISTEMA H√çBRIDO")
        print("="*80)
        print(final_answer)

        print("\n" + "-"*80)
        print("DADOS DA REFLEX√ÉO (para escrita na mem√≥ria):")
        print(json.dumps(reflection_data, indent=2, ensure_ascii=False))
        print("-"*80)

        # Inicia a interface de chat interativa
        chat = ChatInterface(generative_system)
        chat.start_chat()

    except (ImportError, NameError) as e:
        print(f"\nO sistema generativo n√£o p√¥de ser executado. Erro: {e}")
        print("Certifique-se de que a biblioteca 'google-generativeai' est√° instalada e a sua API Key est√° configurada.")
    except Exception as e:
        print(f"\nOcorreu um erro durante a execu√ß√£o do sistema generativo: {e}")


    # Print comprehensive final summary focusing on synaptic improvements
    print(f"\n{'='*80}")
    print(f"RESUMO EXECUTIVO - SISTEMA COM PLASTICIDADE SIN√ÅPTICA")
    print(f"{'='*80}")

    frag_stats = analysis['fragment_stats']
    net_stats = analysis['network_stats']
    comm_stats = analysis['community_stats']
    learn_stats = analysis['learning_stats']
    plast_stats = analysis['plasticity_stats']
    synap_stats = analysis['synaptic_stats']

    print(f"\nüîπ ESTRUTURA DO SISTEMA:")
    print(f"   Fragmentos totais: {frag_stats['total_fragments']}")
    print(f"   Fragmentos din√¢micos: {frag_stats['dynamic_fragments']}")
    print(f"   Fragmentos adaptados: {frag_stats['adaptation_stats']['adapted_fragments']}")
    print(f"   Sinapses na rede: {net_stats['fragment_network']['edges']}")
    print(f"   Componentes conectados: {net_stats['fragment_network']['connected_components']}")
    print(f"   Comunidades emergentes: {comm_stats['fragment_communities']}")

    print(f"\nüîπ PLASTICIDADE VETORIAL:")
    print(f"   Adapta√ß√µes vetoriais: {plast_stats['vectors_adapted_total']}")
    print(f"   Taxa de plasticidade: {plast_stats['plasticity_rate']:.3f} adapt./consulta")
    print(f"   Especializa√ß√µes: {plast_stats['fragments_specialized']}")
    print(f"   Fus√µes realizadas: {plast_stats['fragments_fused']}")
    print(f"   Mudan√ßas estruturais: {plast_stats['structural_changes']}")

    print(f"\nüîπ PLASTICIDADE SIN√ÅPTICA (NOVA!):")
    print(f"   Sinapses criadas: {synap_stats['total_synapses_created']}")
    print(f"   Sinapses consolidadas: {synap_stats['synapses_consolidated']}")
    print(f"   Sinapses podadas: {synap_stats['synapses_pruned']}")
    print(f"   Taxa de consolida√ß√£o: {synap_stats['consolidation_rate']:.3f}")
    print(f"   Peso sin√°ptico m√©dio: {synap_stats['avg_synaptic_weight']:.3f}")
    print(f"   Efici√™ncia sin√°ptica: {synap_stats['synaptic_efficiency']:.3f}")
    print(f"   Aplica√ß√µes de LTD: {synap_stats['ltd_applications']}")

    print(f"\nüîπ METAPLASTICIDADE (NOVA!):")
    if 'metaplasticity_stats' in frag_stats:
        meta_stats = frag_stats['metaplasticity_stats']
        print(f"   Estados plasticity:")
        for state, count in meta_stats['states_distribution'].items():
            print(f"     - {state.capitalize()}: {count}")
        print(f"   Freq. ativa√ß√£o m√©dia: {meta_stats['avg_activation_frequency']:.3f}")
        print(f"   Fragmentos alta freq.: {meta_stats['high_frequency_fragments']}")

    print(f"\nüîπ PROPRIEDADES SIN√ÅPTICAS AVAN√áADAS:")
    if 'weight_distribution' in net_stats['fragment_network']:
        weight_stats = net_stats['fragment_network']['weight_distribution']
        volatility_stats = net_stats['fragment_network']['volatility']

        print(f"   Pesos saturados: {weight_stats['saturated_synapses']}")
        print(f"   Raz√£o volatilidade: {volatility_stats['volatility_ratio']:.3f}")
        print(f"   Sinapses fortes: {synap_stats['strong_synapses']}")

    print(f"\nüîπ DESEMPENHO DO APRENDIZADO:")
    print(f"   Consultas processadas: {learn_stats['queries_processed']}")
    print(f"   Ativa√ß√µes m√©dias/consulta: {learn_stats['avg_activations_per_query']:.1f}")
    print(f"   Uso m√©dio fragmentos: {frag_stats['usage_stats']['mean']:.2f}")
    print(f"   Taxa adapt. m√©dia: {synap_stats['avg_adaptation_rate']:.3f}")

    if comm_stats['fragment_communities'] > 0:
        print(f"   Tamanho m√©dio comunidades: {comm_stats['avg_community_size']:.1f}")

    print(f"\nüîπ CAPACIDADES BIOL√ìGICAMENTE INSPIRADAS:")
    print(f"   ‚úì Satura√ß√£o sin√°ptica (limite: {system.max_synaptic_weight})")
    print(f"   ‚úì Sinapses vol√°teis ‚Üí consolida√ß√£o por uso")
    print(f"   ‚úì Depress√£o a longo prazo (LTD)")
    print(f"   ‚úì Metaplasticidade com 3 estados")
    print(f"   ‚úì Homeostase sin√°ptica via decay diferenciado")
    print(f"   ‚úì Aprendizagem Hebbiana com modula√ß√£o")
    print(f"   ‚úì Plasticidade vetorial adaptativa")
    print(f"   ‚úì Auto-organiza√ß√£o emergente")

    # Advanced efficiency metrics
    total_interactions = learn_stats['total_interactions']
    synaptic_efficiency = synap_stats['synaptic_efficiency']
    consolidation_efficiency = synap_stats['consolidation_rate']

    print(f"\nüîπ M√âTRICAS DE EFICI√äNCIA SIN√ÅPTICA:")
    print(f"   Efici√™ncia consolida√ß√£o: {synaptic_efficiency:.1%}")
    print(f"   Taxa consolida√ß√£o atual: {consolidation_efficiency:.1%}")
    print(f"   Sinapses/intera√ß√£o: {synap_stats['total_synapses_created']/total_interactions:.1f}")
    print(f"   Reten√ß√£o conhecimento: {(1 - plast_stats['avg_fragment_age']/total_interactions)*100:.1f}%")

    # Comparative analysis
    print(f"\nüîπ COMPARA√á√ÉO COM SISTEMA ANTERIOR:")
    print(f"   + Satura√ß√£o sin√°ptica biol√≥gica implementada")
    print(f"   + Sinapses vol√°teis com consolida√ß√£o autom√°tica")
    print(f"   + Depress√£o a longo prazo para poda inteligente")
    print(f"   + Metaplasticidade com 3 estados adaptativos")
    print(f"   + Homeostase atrav√©s de decay diferenciado")
    print(f"   + Taxa consolida√ß√£o: {consolidation_efficiency:.1%}")
    print(f"   + Efici√™ncia sin√°ptica: {synaptic_efficiency:.1%}")

    print(f"\nüéØ CONCLUS√ÉO:")
    print(f"O sistema demonstrou plasticidade sin√°ptica biol√≥gicamente plaus√≠vel,")
    print(f"incorporando mecanismos fundamentais como satura√ß√£o de pesos,")
    print(f"consolida√ß√£o dependente de uso, LTD, e metaplasticidade.")
    print(f"Taxa de consolida√ß√£o de {consolidation_efficiency:.1%} e efici√™ncia de {synaptic_efficiency:.1%}")
    print(f"indicam aprendizado adaptativo eficiente e biologicamente inspirado.")
    print(f"\nSISTEMA CORTICAL COM PLASTICIDADE SIN√ÅPTICA AVAN√áADA COMPLETO!")
    print(f"{'='*80}")

